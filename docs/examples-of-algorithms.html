<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Rafael A. Irizarry">


<meta name="date" content="2019-02-13">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="smoothing.html">
<link rel="next" href="cross-validation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<link href="libs/str_view-0.1.0/str_view.css" rel="stylesheet" />
<script src="libs/str_view-binding-1.3.1/str_view.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#case-studies"><i class="fa fa-check"></i><b>1.1</b> Case studies</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#who-will-find-this-book-useful"><i class="fa fa-check"></i><b>1.2</b> Who will find this book useful?</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#what-does-this-book-cover"><i class="fa fa-check"></i><b>1.3</b> What does this book cover?</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#what-is-not-covered-by-this-book"><i class="fa fa-check"></i><b>1.4</b> What is not covered by this book?</a></li>
</ul></li>
<li class="part"><span><b>I R</b></span></li>
<li class="chapter" data-level="2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html"><i class="fa fa-check"></i><b>2</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="2.1" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-r"><i class="fa fa-check"></i><b>2.1</b> Installing R</a></li>
<li class="chapter" data-level="2.2" data-path="installing-r-rstudio.html"><a href="installing-r-rstudio.html#installing-rstudio"><i class="fa fa-check"></i><b>2.2</b> Installing RStudio</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>3</b> Getting Started with R and RStudio</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started.html"><a href="getting-started.html#why-r"><i class="fa fa-check"></i><b>3.1</b> Why R?</a></li>
<li class="chapter" data-level="3.2" data-path="getting-started.html"><a href="getting-started.html#the-r-console"><i class="fa fa-check"></i><b>3.2</b> The R console</a></li>
<li class="chapter" data-level="3.3" data-path="getting-started.html"><a href="getting-started.html#scripts"><i class="fa fa-check"></i><b>3.3</b> Scripts</a></li>
<li class="chapter" data-level="3.4" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>3.4</b> RStudio</a><ul>
<li class="chapter" data-level="3.4.1" data-path="getting-started.html"><a href="getting-started.html#the-panes"><i class="fa fa-check"></i><b>3.4.1</b> The panes</a></li>
<li class="chapter" data-level="3.4.2" data-path="getting-started.html"><a href="getting-started.html#key-bindings"><i class="fa fa-check"></i><b>3.4.2</b> Key bindings</a></li>
<li class="chapter" data-level="3.4.3" data-path="getting-started.html"><a href="getting-started.html#running-commands-while-editing-scripts"><i class="fa fa-check"></i><b>3.4.3</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="3.4.4" data-path="getting-started.html"><a href="getting-started.html#changing-global-options"><i class="fa fa-check"></i><b>3.4.4</b> Changing global options</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="getting-started.html"><a href="getting-started.html#installing-r-packages"><i class="fa fa-check"></i><b>3.5</b> Installing R packages</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>4</b> R Basics</a><ul>
<li class="chapter" data-level="4.1" data-path="r-basics.html"><a href="r-basics.html#case-study-us-gun-murders"><i class="fa fa-check"></i><b>4.1</b> Case study: US Gun Murders</a></li>
<li class="chapter" data-level="4.2" data-path="r-basics.html"><a href="r-basics.html#the-very-basics"><i class="fa fa-check"></i><b>4.2</b> The very basics</a><ul>
<li class="chapter" data-level="4.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>4.2.1</b> Objects</a></li>
<li class="chapter" data-level="4.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>4.2.2</b> The workspace</a></li>
<li class="chapter" data-level="4.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>4.2.3</b> Functions</a></li>
<li class="chapter" data-level="4.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>4.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="4.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>4.2.5</b> Variable names</a></li>
<li class="chapter" data-level="4.2.6" data-path="r-basics.html"><a href="r-basics.html#saving-your-workspace"><i class="fa fa-check"></i><b>4.2.6</b> Saving your workspace</a></li>
<li class="chapter" data-level="4.2.7" data-path="r-basics.html"><a href="r-basics.html#motivating-scripts"><i class="fa fa-check"></i><b>4.2.7</b> Motivating scripts</a></li>
<li class="chapter" data-level="4.2.8" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>4.2.8</b> Commenting your code</a></li>
<li class="chapter" data-level="4.2.9" data-path="r-basics.html"><a href="r-basics.html#exercises"><i class="fa fa-check"></i><b>4.2.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>4.3</b> Data types</a></li>
<li class="chapter" data-level="4.4" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>4.4</b> Data frames</a><ul>
<li class="chapter" data-level="4.4.1" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>4.4.1</b> Examining an object</a></li>
<li class="chapter" data-level="4.4.2" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>4.4.2</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="4.4.3" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>4.4.3</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="4.4.4" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>4.4.4</b> Factors</a></li>
<li class="chapter" data-level="4.4.5" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>4.4.5</b> Lists</a></li>
<li class="chapter" data-level="4.4.6" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>4.4.6</b> Matrices</a></li>
<li class="chapter" data-level="4.4.7" data-path="r-basics.html"><a href="r-basics.html#exercises-1"><i class="fa fa-check"></i><b>4.4.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>4.5</b> Vectors</a><ul>
<li class="chapter" data-level="4.5.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>4.5.1</b> Creating vectors</a></li>
<li class="chapter" data-level="4.5.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>4.5.2</b> Names</a></li>
<li class="chapter" data-level="4.5.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>4.5.3</b> Sequences</a></li>
<li class="chapter" data-level="4.5.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>4.5.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>4.6</b> Coercion</a><ul>
<li class="chapter" data-level="4.6.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>4.6.1</b> Not availables (NA)</a></li>
<li class="chapter" data-level="4.6.2" data-path="r-basics.html"><a href="r-basics.html#exercises-2"><i class="fa fa-check"></i><b>4.6.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>4.7</b> Sorting</a><ul>
<li class="chapter" data-level="4.7.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>4.7.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="4.7.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>4.7.2</b> <code>order</code></a></li>
<li class="chapter" data-level="4.7.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>4.7.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="4.7.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>4.7.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="4.7.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>4.7.5</b> Beware of recycling</a></li>
<li class="chapter" data-level="4.7.6" data-path="r-basics.html"><a href="r-basics.html#exercise"><i class="fa fa-check"></i><b>4.7.6</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>4.8</b> Vector arithmetics</a><ul>
<li class="chapter" data-level="4.8.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>4.8.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="4.8.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>4.8.2</b> Two vectors</a></li>
<li class="chapter" data-level="4.8.3" data-path="r-basics.html"><a href="r-basics.html#exercises-3"><i class="fa fa-check"></i><b>4.8.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>4.9</b> Indexing</a><ul>
<li class="chapter" data-level="4.9.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>4.9.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="4.9.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>4.9.2</b> Logical operators</a></li>
<li class="chapter" data-level="4.9.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>4.9.3</b> <code>which</code></a></li>
<li class="chapter" data-level="4.9.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>4.9.4</b> <code>match</code></a></li>
<li class="chapter" data-level="4.9.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>4.9.5</b> <code>%in%</code></a></li>
<li class="chapter" data-level="4.9.6" data-path="r-basics.html"><a href="r-basics.html#exercises-4"><i class="fa fa-check"></i><b>4.9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="r-basics.html"><a href="r-basics.html#basic-plots"><i class="fa fa-check"></i><b>4.10</b> Basic plots</a><ul>
<li class="chapter" data-level="4.10.1" data-path="r-basics.html"><a href="r-basics.html#plot"><i class="fa fa-check"></i><b>4.10.1</b> <code>plot</code></a></li>
<li class="chapter" data-level="4.10.2" data-path="r-basics.html"><a href="r-basics.html#hist"><i class="fa fa-check"></i><b>4.10.2</b> <code>hist</code></a></li>
<li class="chapter" data-level="4.10.3" data-path="r-basics.html"><a href="r-basics.html#boxplot"><i class="fa fa-check"></i><b>4.10.3</b> <code>boxplot</code></a></li>
<li class="chapter" data-level="4.10.4" data-path="r-basics.html"><a href="r-basics.html#image"><i class="fa fa-check"></i><b>4.10.4</b> <code>image</code></a></li>
<li class="chapter" data-level="4.10.5" data-path="r-basics.html"><a href="r-basics.html#exercises-5"><i class="fa fa-check"></i><b>4.10.5</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="programming-basics.html"><a href="programming-basics.html"><i class="fa fa-check"></i><b>5</b> Programming basics</a><ul>
<li class="chapter" data-level="5.1" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>5.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="5.2" data-path="programming-basics.html"><a href="programming-basics.html#defining-functions"><i class="fa fa-check"></i><b>5.2</b> Defining functions</a></li>
<li class="chapter" data-level="5.3" data-path="programming-basics.html"><a href="programming-basics.html#namespaces"><i class="fa fa-check"></i><b>5.3</b> Namespaces</a></li>
<li class="chapter" data-level="5.4" data-path="programming-basics.html"><a href="programming-basics.html#for-loops"><i class="fa fa-check"></i><b>5.4</b> For-loops</a></li>
<li class="chapter" data-level="5.5" data-path="programming-basics.html"><a href="programming-basics.html#vectorization"><i class="fa fa-check"></i><b>5.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="5.6" data-path="programming-basics.html"><a href="programming-basics.html#exercises-6"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html"><i class="fa fa-check"></i><b>6</b> Basic data wrangling with dplyr</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#manipulating-data-frames"><i class="fa fa-check"></i><b>6.1</b> Manipulating data frames</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#adding-a-column-with-mutate"><i class="fa fa-check"></i><b>6.1.1</b> Adding a column with <code>mutate</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#subsetting-with-filter"><i class="fa fa-check"></i><b>6.1.2</b> Subsetting with <code>filter</code></a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#selecting-columns-with-select"><i class="fa fa-check"></i><b>6.1.3</b> Selecting columns with <code>select</code></a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#exercises-7"><i class="fa fa-check"></i><b>6.1.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#the-pipe"><i class="fa fa-check"></i><b>6.2</b> The pipe: <code>%&gt;%</code></a><ul>
<li class="chapter" data-level="6.2.1" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#exercises-8"><i class="fa fa-check"></i><b>6.2.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#summarizing-data"><i class="fa fa-check"></i><b>6.3</b> Summarizing data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#summarize"><i class="fa fa-check"></i><b>6.3.1</b> <code>summarize</code></a></li>
<li class="chapter" data-level="6.3.2" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#pull"><i class="fa fa-check"></i><b>6.3.2</b> <code>pull</code></a></li>
<li class="chapter" data-level="6.3.3" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#group-by"><i class="fa fa-check"></i><b>6.3.3</b> Group then summarize</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#sorting-data-frames"><i class="fa fa-check"></i><b>6.4</b> Sorting data frames</a><ul>
<li class="chapter" data-level="6.4.1" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#nested-sorting"><i class="fa fa-check"></i><b>6.4.1</b> Nested sorting</a></li>
<li class="chapter" data-level="6.4.2" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#the-top-n"><i class="fa fa-check"></i><b>6.4.2</b> The top <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="6.4.3" data-path="basic-data-wrangling-with-dplyr.html"><a href="basic-data-wrangling-with-dplyr.html#exercises-9"><i class="fa fa-check"></i><b>6.4.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>7</b> Importing data</a><ul>
<li class="chapter" data-level="7.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>7.1</b> Paths and the working directory</a><ul>
<li class="chapter" data-level="7.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>7.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="7.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>7.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="7.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>7.1.3</b> The working directory</a></li>
<li class="chapter" data-level="7.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>7.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="7.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>7.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>7.2</b> The readr and readxl packages</a><ul>
<li class="chapter" data-level="7.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>7.2.1</b> readr</a></li>
<li class="chapter" data-level="7.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>7.2.2</b> readxl</a></li>
<li class="chapter" data-level="7.2.3" data-path="importing-data.html"><a href="importing-data.html#exercises-10"><i class="fa fa-check"></i><b>7.2.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>7.3</b> Downloading files</a></li>
<li class="chapter" data-level="7.4" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>7.4</b> R-base importing functions</a></li>
<li class="chapter" data-level="7.5" data-path="importing-data.html"><a href="importing-data.html#nuances"><i class="fa fa-check"></i><b>7.5</b> Nuances</a></li>
<li class="chapter" data-level="7.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>7.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="7.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>7.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="7.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>7.8</b> Organizing Data with Spreadsheets</a><ul>
<li class="chapter" data-level="7.8.1" data-path="importing-data.html"><a href="importing-data.html#exercises-11"><i class="fa fa-check"></i><b>7.8.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>8</b> ggplot2</a><ul>
<li class="chapter" data-level="8.1" data-path="ggplot2.html"><a href="ggplot2.html#the-components-of-a-graph"><i class="fa fa-check"></i><b>8.1</b> The components of a graph</a></li>
<li class="chapter" data-level="8.2" data-path="ggplot2.html"><a href="ggplot2.html#ggplot-objects-a-blank-slate"><i class="fa fa-check"></i><b>8.2</b> <code>ggplot</code> objects: a blank slate</a></li>
<li class="chapter" data-level="8.3" data-path="ggplot2.html"><a href="ggplot2.html#geometries"><i class="fa fa-check"></i><b>8.3</b> Geometries</a></li>
<li class="chapter" data-level="8.4" data-path="ggplot2.html"><a href="ggplot2.html#aesthetic-mappings"><i class="fa fa-check"></i><b>8.4</b> Aesthetic mappings</a></li>
<li class="chapter" data-level="8.5" data-path="ggplot2.html"><a href="ggplot2.html#layers"><i class="fa fa-check"></i><b>8.5</b> Layers</a></li>
<li class="chapter" data-level="8.6" data-path="ggplot2.html"><a href="ggplot2.html#tinkering-with-arguments"><i class="fa fa-check"></i><b>8.6</b> Tinkering with arguments</a></li>
<li class="chapter" data-level="8.7" data-path="ggplot2.html"><a href="ggplot2.html#global-versus-local-aesthetic-mappings"><i class="fa fa-check"></i><b>8.7</b> Global versus local aesthetic mappings</a></li>
<li class="chapter" data-level="8.8" data-path="ggplot2.html"><a href="ggplot2.html#scales"><i class="fa fa-check"></i><b>8.8</b> Scales</a></li>
<li class="chapter" data-level="8.9" data-path="ggplot2.html"><a href="ggplot2.html#labels-and-titles"><i class="fa fa-check"></i><b>8.9</b> Labels and titles</a></li>
<li class="chapter" data-level="8.10" data-path="ggplot2.html"><a href="ggplot2.html#categories-as-colors"><i class="fa fa-check"></i><b>8.10</b> Categories as colors</a></li>
<li class="chapter" data-level="8.11" data-path="ggplot2.html"><a href="ggplot2.html#annotation-shapes-and-adjustments"><i class="fa fa-check"></i><b>8.11</b> Annotation, shapes, and adjustments</a></li>
<li class="chapter" data-level="8.12" data-path="ggplot2.html"><a href="ggplot2.html#add-on-packages"><i class="fa fa-check"></i><b>8.12</b> Add-on packages</a></li>
<li class="chapter" data-level="8.13" data-path="ggplot2.html"><a href="ggplot2.html#putting-it-all-together"><i class="fa fa-check"></i><b>8.13</b> Putting it all together</a></li>
<li class="chapter" data-level="8.14" data-path="ggplot2.html"><a href="ggplot2.html#qplot"><i class="fa fa-check"></i><b>8.14</b> Quick plots with <code>qplot</code></a></li>
<li class="chapter" data-level="8.15" data-path="ggplot2.html"><a href="ggplot2.html#grids-of-plots"><i class="fa fa-check"></i><b>8.15</b> Grids of plots</a></li>
<li class="chapter" data-level="8.16" data-path="ggplot2.html"><a href="ggplot2.html#exercises-12"><i class="fa fa-check"></i><b>8.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tidyverse.html"><a href="tidyverse.html"><i class="fa fa-check"></i><b>9</b> The tidyverse</a><ul>
<li class="chapter" data-level="9.1" data-path="tidyverse.html"><a href="tidyverse.html#tidy-data"><i class="fa fa-check"></i><b>9.1</b> Tidy data</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tidyverse.html"><a href="tidyverse.html#exercises-13"><i class="fa fa-check"></i><b>9.1.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tidyverse.html"><a href="tidyverse.html#tibbles"><i class="fa fa-check"></i><b>9.2</b> Tibbles</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-display-better"><i class="fa fa-check"></i><b>9.2.1</b> Tibbles display better</a></li>
<li class="chapter" data-level="9.2.2" data-path="tidyverse.html"><a href="tidyverse.html#subsets-of-tibbles-are-tibbles"><i class="fa fa-check"></i><b>9.2.2</b> Subsets of tibbles are tibbles</a></li>
<li class="chapter" data-level="9.2.3" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-have-complex-entries"><i class="fa fa-check"></i><b>9.2.3</b> Tibbles can have complex entries</a></li>
<li class="chapter" data-level="9.2.4" data-path="tidyverse.html"><a href="tidyverse.html#tibbles-can-be-grouped"><i class="fa fa-check"></i><b>9.2.4</b> Tibbles can be grouped</a></li>
<li class="chapter" data-level="9.2.5" data-path="tidyverse.html"><a href="tidyverse.html#creating-a-tibble"><i class="fa fa-check"></i><b>9.2.5</b> Creating a tibble</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tidyverse.html"><a href="tidyverse.html#the-dot-operator"><i class="fa fa-check"></i><b>9.3</b> The dot operator</a></li>
<li class="chapter" data-level="9.4" data-path="tidyverse.html"><a href="tidyverse.html#do"><i class="fa fa-check"></i><b>9.4</b> <code>do</code></a></li>
<li class="chapter" data-level="9.5" data-path="tidyverse.html"><a href="tidyverse.html#the-purrr-package"><i class="fa fa-check"></i><b>9.5</b> The purrr package</a></li>
<li class="chapter" data-level="9.6" data-path="programming-basics.html"><a href="programming-basics.html#conditionals"><i class="fa fa-check"></i><b>9.6</b> Conditionals</a><ul>
<li class="chapter" data-level="9.6.1" data-path="tidyverse.html"><a href="tidyverse.html#case_when"><i class="fa fa-check"></i><b>9.6.1</b> <code>case_when</code></a></li>
<li class="chapter" data-level="9.6.2" data-path="tidyverse.html"><a href="tidyverse.html#between"><i class="fa fa-check"></i><b>9.6.2</b> <code>between</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Data Visualization</b></span></li>
<li class="chapter" data-level="10" data-path="introduction-to-data-visualization.html"><a href="introduction-to-data-visualization.html"><i class="fa fa-check"></i><b>10</b> Introduction to data visualization</a></li>
<li class="chapter" data-level="11" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>11</b> Distributions</a><ul>
<li class="chapter" data-level="11.1" data-path="distributions.html"><a href="distributions.html#variable-types"><i class="fa fa-check"></i><b>11.1</b> Variable types</a></li>
<li class="chapter" data-level="11.2" data-path="distributions.html"><a href="distributions.html#case-study-student-heights"><i class="fa fa-check"></i><b>11.2</b> Case study: Student heights</a></li>
<li class="chapter" data-level="11.3" data-path="distributions.html"><a href="distributions.html#distribution-function"><i class="fa fa-check"></i><b>11.3</b> Distribution function</a></li>
<li class="chapter" data-level="11.4" data-path="distributions.html"><a href="distributions.html#cumulative-distribution-functions"><i class="fa fa-check"></i><b>11.4</b> Cumulative distribution functions</a></li>
<li class="chapter" data-level="11.5" data-path="distributions.html"><a href="distributions.html#histograms"><i class="fa fa-check"></i><b>11.5</b> Histograms</a></li>
<li class="chapter" data-level="11.6" data-path="distributions.html"><a href="distributions.html#smoothed-density"><i class="fa fa-check"></i><b>11.6</b> Smoothed density</a><ul>
<li class="chapter" data-level="11.6.1" data-path="distributions.html"><a href="distributions.html#interpreting-the-y-axis"><i class="fa fa-check"></i><b>11.6.1</b> Interpreting the y-axis</a></li>
<li class="chapter" data-level="11.6.2" data-path="distributions.html"><a href="distributions.html#densities-permit-stratification"><i class="fa fa-check"></i><b>11.6.2</b> Densities permit stratification</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="distributions.html"><a href="distributions.html#exercises-14"><i class="fa fa-check"></i><b>11.7</b> Exercises</a></li>
<li class="chapter" data-level="11.8" data-path="distributions.html"><a href="distributions.html#the-normal-distribution"><i class="fa fa-check"></i><b>11.8</b> The normal distribution</a></li>
<li class="chapter" data-level="11.9" data-path="distributions.html"><a href="distributions.html#standardized-units"><i class="fa fa-check"></i><b>11.9</b> Standardized units</a></li>
<li class="chapter" data-level="11.10" data-path="distributions.html"><a href="distributions.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>11.10</b> Quantile-quantile plots</a></li>
<li class="chapter" data-level="11.11" data-path="distributions.html"><a href="distributions.html#percentiles"><i class="fa fa-check"></i><b>11.11</b> Percentiles</a></li>
<li class="chapter" data-level="11.12" data-path="distributions.html"><a href="distributions.html#case-study-student-heights-continued"><i class="fa fa-check"></i><b>11.12</b> Case study: Student heights (continued)</a></li>
<li class="chapter" data-level="11.13" data-path="distributions.html"><a href="distributions.html#boxplots"><i class="fa fa-check"></i><b>11.13</b> Boxplots</a></li>
<li class="chapter" data-level="11.14" data-path="distributions.html"><a href="distributions.html#student-height-cont"><i class="fa fa-check"></i><b>11.14</b> Case study: Student heights (continued)</a></li>
<li class="chapter" data-level="11.15" data-path="distributions.html"><a href="distributions.html#exercises-15"><i class="fa fa-check"></i><b>11.15</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="other-geometries.html"><a href="other-geometries.html"><i class="fa fa-check"></i><b>12</b> ggplot2 geometries</a><ul>
<li class="chapter" data-level="12.1" data-path="other-geometries.html"><a href="other-geometries.html#barplots"><i class="fa fa-check"></i><b>12.1</b> Barplots</a></li>
<li class="chapter" data-level="12.2" data-path="other-geometries.html"><a href="other-geometries.html#histograms-1"><i class="fa fa-check"></i><b>12.2</b> Histograms</a></li>
<li class="chapter" data-level="12.3" data-path="other-geometries.html"><a href="other-geometries.html#density-plots"><i class="fa fa-check"></i><b>12.3</b> Density plots</a></li>
<li class="chapter" data-level="12.4" data-path="other-geometries.html"><a href="other-geometries.html#boxplot-1"><i class="fa fa-check"></i><b>12.4</b> Boxplot</a></li>
<li class="chapter" data-level="12.5" data-path="other-geometries.html"><a href="other-geometries.html#qq-plots"><i class="fa fa-check"></i><b>12.5</b> QQ-plots</a></li>
<li class="chapter" data-level="12.6" data-path="other-geometries.html"><a href="other-geometries.html#images"><i class="fa fa-check"></i><b>12.6</b> Images</a></li>
<li class="chapter" data-level="12.7" data-path="other-geometries.html"><a href="other-geometries.html#quick-plots"><i class="fa fa-check"></i><b>12.7</b> Quick plots</a><ul>
<li class="chapter" data-level="12.7.1" data-path="other-geometries.html"><a href="other-geometries.html#exercises-16"><i class="fa fa-check"></i><b>12.7.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="gapminder.html"><a href="gapminder.html"><i class="fa fa-check"></i><b>13</b> Data visualization in practice</a><ul>
<li class="chapter" data-level="13.1" data-path="gapminder.html"><a href="gapminder.html#gapminder"><i class="fa fa-check"></i><b>13.1</b> Gapminder</a><ul>
<li class="chapter" data-level="13.1.1" data-path="gapminder.html"><a href="gapminder.html#hans-roslings-quiz"><i class="fa fa-check"></i><b>13.1.1</b> Hans Rosling’s quiz</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="gapminder.html"><a href="gapminder.html#scatterplots"><i class="fa fa-check"></i><b>13.2</b> Scatterplots</a></li>
<li class="chapter" data-level="13.3" data-path="gapminder.html"><a href="gapminder.html#faceting"><i class="fa fa-check"></i><b>13.3</b> Faceting</a><ul>
<li class="chapter" data-level="13.3.1" data-path="gapminder.html"><a href="gapminder.html#facet_wrap"><i class="fa fa-check"></i><b>13.3.1</b> <code>facet_wrap</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="gapminder.html"><a href="gapminder.html#fixed-scales-for-better-comparisons"><i class="fa fa-check"></i><b>13.3.2</b> Fixed scales for better comparisons</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="gapminder.html"><a href="gapminder.html#time-series-plots"><i class="fa fa-check"></i><b>13.4</b> Time series plots</a><ul>
<li class="chapter" data-level="13.4.1" data-path="gapminder.html"><a href="gapminder.html#labels-instead-of-legends"><i class="fa fa-check"></i><b>13.4.1</b> Labels instead of legends</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="gapminder.html"><a href="gapminder.html#transformations"><i class="fa fa-check"></i><b>13.5</b> Transformations</a><ul>
<li class="chapter" data-level="13.5.1" data-path="gapminder.html"><a href="gapminder.html#log-transformation"><i class="fa fa-check"></i><b>13.5.1</b> Log transformation</a></li>
<li class="chapter" data-level="13.5.2" data-path="gapminder.html"><a href="gapminder.html#which-base"><i class="fa fa-check"></i><b>13.5.2</b> Which base?</a></li>
<li class="chapter" data-level="13.5.3" data-path="gapminder.html"><a href="gapminder.html#transform-the-values-or-the-scale"><i class="fa fa-check"></i><b>13.5.3</b> Transform the values or the scale?</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="gapminder.html"><a href="gapminder.html#modes"><i class="fa fa-check"></i><b>13.6</b> Modes</a></li>
<li class="chapter" data-level="13.7" data-path="gapminder.html"><a href="gapminder.html#mulitple-distributions-boxplots-and-ridge-plots"><i class="fa fa-check"></i><b>13.7</b> Mulitple distributions: boxplots and ridge plots</a><ul>
<li class="chapter" data-level="13.7.1" data-path="gapminder.html"><a href="gapminder.html#boxplots-1"><i class="fa fa-check"></i><b>13.7.1</b> Boxplots</a></li>
<li class="chapter" data-level="13.7.2" data-path="gapminder.html"><a href="gapminder.html#ridge-plots"><i class="fa fa-check"></i><b>13.7.2</b> Ridge plots</a></li>
</ul></li>
<li class="chapter" data-level="13.8" data-path="gapminder.html"><a href="gapminder.html#example-1970-versus-2010-income-distributions"><i class="fa fa-check"></i><b>13.8</b> Example: 1970 versus 2010 income distributions</a><ul>
<li class="chapter" data-level="13.8.1" data-path="gapminder.html"><a href="gapminder.html#accessing-computed-variables"><i class="fa fa-check"></i><b>13.8.1</b> Accessing computed variables</a></li>
<li class="chapter" data-level="13.8.2" data-path="gapminder.html"><a href="gapminder.html#weighted-densities"><i class="fa fa-check"></i><b>13.8.2</b> Weighted densities</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="gapminder.html"><a href="gapminder.html#ecological-fallacy"><i class="fa fa-check"></i><b>13.9</b> Ecological fallacy</a><ul>
<li class="chapter" data-level="13.9.1" data-path="gapminder.html"><a href="gapminder.html#logit"><i class="fa fa-check"></i><b>13.9.1</b> Logistic transformation</a></li>
<li class="chapter" data-level="13.9.2" data-path="gapminder.html"><a href="gapminder.html#show-the-data"><i class="fa fa-check"></i><b>13.9.2</b> Show the data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html"><i class="fa fa-check"></i><b>14</b> Data visualization principles</a><ul>
<li class="chapter" data-level="14.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-data-using-visual-cues"><i class="fa fa-check"></i><b>14.1</b> Encoding data using visual cues</a></li>
<li class="chapter" data-level="14.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-when-to-include-0"><i class="fa fa-check"></i><b>14.2</b> Know when to include 0</a></li>
<li class="chapter" data-level="14.3" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#do-not-distort-quantities"><i class="fa fa-check"></i><b>14.3</b> Do not distort quantities</a></li>
<li class="chapter" data-level="14.4" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#order-by-a-meaningful-value"><i class="fa fa-check"></i><b>14.4</b> Order by a meaningful value</a></li>
<li class="chapter" data-level="14.5" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#show-the-data-1"><i class="fa fa-check"></i><b>14.5</b> Show the data</a></li>
<li class="chapter" data-level="14.6" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons-use-common-axes"><i class="fa fa-check"></i><b>14.6</b> Ease comparisons: use common axes</a></li>
<li class="chapter" data-level="14.7" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparisons-align-plots-vertically-to-see-horizontal-changes-and-horizontally-to-see-vertical-changes"><i class="fa fa-check"></i><b>14.7</b> Ease comparisons: align plots vertically to see horizontal changes and horizontally to see vertical changes</a></li>
<li class="chapter" data-level="14.8" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#consider-transformations"><i class="fa fa-check"></i><b>14.8</b> Consider transformations</a></li>
<li class="chapter" data-level="14.9" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#visual-cues-to-be-compared-should-be-adjacent"><i class="fa fa-check"></i><b>14.9</b> Visual cues to be compared should be adjacent</a></li>
<li class="chapter" data-level="14.10" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#ease-comparison-use-color"><i class="fa fa-check"></i><b>14.10</b> Ease comparison: use color</a></li>
<li class="chapter" data-level="14.11" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#think-of-the-color-blind"><i class="fa fa-check"></i><b>14.11</b> Think of the color blind</a></li>
<li class="chapter" data-level="14.12" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#two-variables"><i class="fa fa-check"></i><b>14.12</b> Two variables</a><ul>
<li class="chapter" data-level="14.12.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#slope-charts"><i class="fa fa-check"></i><b>14.12.1</b> Slope charts</a></li>
<li class="chapter" data-level="14.12.2" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#bland-altman-plot"><i class="fa fa-check"></i><b>14.12.2</b> Bland-Altman plot</a></li>
</ul></li>
<li class="chapter" data-level="14.13" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#encoding-a-third-variable"><i class="fa fa-check"></i><b>14.13</b> Encoding a third variable</a></li>
<li class="chapter" data-level="14.14" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-pseudo-three-dimensional-plots"><i class="fa fa-check"></i><b>14.14</b> Avoid pseudo three dimensional plots</a></li>
<li class="chapter" data-level="14.15" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-gratuitous-three-dimensional-plots"><i class="fa fa-check"></i><b>14.15</b> Avoid gratuitous three dimensional plots</a></li>
<li class="chapter" data-level="14.16" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#avoid-too-many-significant-digits"><i class="fa fa-check"></i><b>14.16</b> Avoid too many significant digits</a></li>
<li class="chapter" data-level="14.17" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#know-your-audience"><i class="fa fa-check"></i><b>14.17</b> Know your audience</a></li>
<li class="chapter" data-level="14.18" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-17"><i class="fa fa-check"></i><b>14.18</b> Exercises</a></li>
<li class="chapter" data-level="14.19" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#case-study-the-impact-of-vaccines-on-battling-infectious-diseases"><i class="fa fa-check"></i><b>14.19</b> Case study: The impact of vaccines on battling infectious diseases</a><ul>
<li class="chapter" data-level="14.19.1" data-path="data-visualization-principles.html"><a href="data-visualization-principles.html#exercises-18"><i class="fa fa-check"></i><b>14.19.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="robust-summaries.html"><a href="robust-summaries.html"><i class="fa fa-check"></i><b>15</b> Robust summaries</a><ul>
<li class="chapter" data-level="15.1" data-path="robust-summaries.html"><a href="robust-summaries.html#outliers"><i class="fa fa-check"></i><b>15.1</b> Outliers</a></li>
<li class="chapter" data-level="15.2" data-path="robust-summaries.html"><a href="robust-summaries.html#median"><i class="fa fa-check"></i><b>15.2</b> Median</a></li>
<li class="chapter" data-level="15.3" data-path="robust-summaries.html"><a href="robust-summaries.html#the-inter-quartile-range-iqr"><i class="fa fa-check"></i><b>15.3</b> The inter quartile range (IQR)</a></li>
<li class="chapter" data-level="15.4" data-path="robust-summaries.html"><a href="robust-summaries.html#tukeys-definition-of-an-outlier"><i class="fa fa-check"></i><b>15.4</b> Tukey’s definition of an outlier</a></li>
<li class="chapter" data-level="15.5" data-path="robust-summaries.html"><a href="robust-summaries.html#median-absolute-deviation"><i class="fa fa-check"></i><b>15.5</b> Median absolute deviation</a></li>
<li class="chapter" data-level="15.6" data-path="robust-summaries.html"><a href="robust-summaries.html#exercises-19"><i class="fa fa-check"></i><b>15.6</b> Exercises</a></li>
<li class="chapter" data-level="15.7" data-path="robust-summaries.html"><a href="robust-summaries.html#case-study-self-reported-student-heights"><i class="fa fa-check"></i><b>15.7</b> Case study: Self-reported student heights</a></li>
</ul></li>
<li class="part"><span><b>III Data Wrangling</b></span></li>
<li class="chapter" data-level="16" data-path="introduction-to-data-wrangling.html"><a href="introduction-to-data-wrangling.html"><i class="fa fa-check"></i><b>16</b> Introduction to Data Wrangling</a></li>
<li class="chapter" data-level="17" data-path="reshaping-data.html"><a href="reshaping-data.html"><i class="fa fa-check"></i><b>17</b> Reshaping data</a><ul>
<li class="chapter" data-level="17.1" data-path="reshaping-data.html"><a href="reshaping-data.html#gather"><i class="fa fa-check"></i><b>17.1</b> <code>gather</code></a></li>
<li class="chapter" data-level="17.2" data-path="reshaping-data.html"><a href="reshaping-data.html#spread"><i class="fa fa-check"></i><b>17.2</b> <code>spread</code></a></li>
<li class="chapter" data-level="17.3" data-path="reshaping-data.html"><a href="reshaping-data.html#separate"><i class="fa fa-check"></i><b>17.3</b> <code>separate</code></a></li>
<li class="chapter" data-level="17.4" data-path="reshaping-data.html"><a href="reshaping-data.html#unite"><i class="fa fa-check"></i><b>17.4</b> <code>unite</code></a></li>
<li class="chapter" data-level="17.5" data-path="reshaping-data.html"><a href="reshaping-data.html#exercises-20"><i class="fa fa-check"></i><b>17.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="joining-tables.html"><a href="joining-tables.html"><i class="fa fa-check"></i><b>18</b> Joining tables</a><ul>
<li class="chapter" data-level="18.1" data-path="joining-tables.html"><a href="joining-tables.html#joins"><i class="fa fa-check"></i><b>18.1</b> Joins</a><ul>
<li class="chapter" data-level="18.1.1" data-path="joining-tables.html"><a href="joining-tables.html#left-join"><i class="fa fa-check"></i><b>18.1.1</b> Left join</a></li>
<li class="chapter" data-level="18.1.2" data-path="joining-tables.html"><a href="joining-tables.html#right-join"><i class="fa fa-check"></i><b>18.1.2</b> Right join</a></li>
<li class="chapter" data-level="18.1.3" data-path="joining-tables.html"><a href="joining-tables.html#inner-join"><i class="fa fa-check"></i><b>18.1.3</b> Inner join</a></li>
<li class="chapter" data-level="18.1.4" data-path="joining-tables.html"><a href="joining-tables.html#full-join"><i class="fa fa-check"></i><b>18.1.4</b> Full join</a></li>
<li class="chapter" data-level="18.1.5" data-path="joining-tables.html"><a href="joining-tables.html#semi-join"><i class="fa fa-check"></i><b>18.1.5</b> Semi join</a></li>
<li class="chapter" data-level="18.1.6" data-path="joining-tables.html"><a href="joining-tables.html#anti-join"><i class="fa fa-check"></i><b>18.1.6</b> Anti join</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="joining-tables.html"><a href="joining-tables.html#binding"><i class="fa fa-check"></i><b>18.2</b> Binding</a><ul>
<li class="chapter" data-level="18.2.1" data-path="joining-tables.html"><a href="joining-tables.html#binding-columns"><i class="fa fa-check"></i><b>18.2.1</b> Binding columns</a></li>
<li class="chapter" data-level="18.2.2" data-path="joining-tables.html"><a href="joining-tables.html#binding-by-rows"><i class="fa fa-check"></i><b>18.2.2</b> Binding by rows</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="joining-tables.html"><a href="joining-tables.html#set-operators"><i class="fa fa-check"></i><b>18.3</b> Set operators</a><ul>
<li class="chapter" data-level="18.3.1" data-path="joining-tables.html"><a href="joining-tables.html#intersect"><i class="fa fa-check"></i><b>18.3.1</b> Intersect</a></li>
<li class="chapter" data-level="18.3.2" data-path="joining-tables.html"><a href="joining-tables.html#union"><i class="fa fa-check"></i><b>18.3.2</b> Union</a></li>
<li class="chapter" data-level="18.3.3" data-path="joining-tables.html"><a href="joining-tables.html#setdiff"><i class="fa fa-check"></i><b>18.3.3</b> <code>setdiff</code></a></li>
<li class="chapter" data-level="18.3.4" data-path="joining-tables.html"><a href="joining-tables.html#setequal"><i class="fa fa-check"></i><b>18.3.4</b> <code>setequal</code></a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="joining-tables.html"><a href="joining-tables.html#exercises-21"><i class="fa fa-check"></i><b>18.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="web-scraping.html"><a href="web-scraping.html"><i class="fa fa-check"></i><b>19</b> Web Scraping</a><ul>
<li class="chapter" data-level="19.1" data-path="web-scraping.html"><a href="web-scraping.html#html"><i class="fa fa-check"></i><b>19.1</b> HTML</a></li>
<li class="chapter" data-level="19.2" data-path="web-scraping.html"><a href="web-scraping.html#the-rvest-package"><i class="fa fa-check"></i><b>19.2</b> The rvest package</a></li>
<li class="chapter" data-level="19.3" data-path="web-scraping.html"><a href="web-scraping.html#css-selectors"><i class="fa fa-check"></i><b>19.3</b> CSS selectors</a></li>
<li class="chapter" data-level="19.4" data-path="web-scraping.html"><a href="web-scraping.html#json"><i class="fa fa-check"></i><b>19.4</b> JSON</a></li>
<li class="chapter" data-level="19.5" data-path="web-scraping.html"><a href="web-scraping.html#exercises-22"><i class="fa fa-check"></i><b>19.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="string-processing.html"><a href="string-processing.html"><i class="fa fa-check"></i><b>20</b> String Processing</a><ul>
<li class="chapter" data-level="20.1" data-path="string-processing.html"><a href="string-processing.html#stringr"><i class="fa fa-check"></i><b>20.1</b> The stringr package</a></li>
<li class="chapter" data-level="20.2" data-path="string-processing.html"><a href="string-processing.html#case-study-1-us-murders-data"><i class="fa fa-check"></i><b>20.2</b> Case study 1: US murders data</a></li>
<li class="chapter" data-level="20.3" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights"><i class="fa fa-check"></i><b>20.3</b> Case study 2: Self reported heights</a></li>
<li class="chapter" data-level="20.4" data-path="string-processing.html"><a href="string-processing.html#how-to-escape-when-defining-strings"><i class="fa fa-check"></i><b>20.4</b> How to <em>escape</em> when defining strings</a></li>
<li class="chapter" data-level="20.5" data-path="string-processing.html"><a href="string-processing.html#regular-expressions"><i class="fa fa-check"></i><b>20.5</b> Regular expressions</a><ul>
<li class="chapter" data-level="20.5.1" data-path="string-processing.html"><a href="string-processing.html#strings-are-a-regexp"><i class="fa fa-check"></i><b>20.5.1</b> Strings are a regexp</a></li>
<li class="chapter" data-level="20.5.2" data-path="string-processing.html"><a href="string-processing.html#special-characters"><i class="fa fa-check"></i><b>20.5.2</b> Special characters</a></li>
<li class="chapter" data-level="20.5.3" data-path="string-processing.html"><a href="string-processing.html#character-classes"><i class="fa fa-check"></i><b>20.5.3</b> Character classes</a></li>
<li class="chapter" data-level="20.5.4" data-path="string-processing.html"><a href="string-processing.html#anchors"><i class="fa fa-check"></i><b>20.5.4</b> Anchors</a></li>
<li class="chapter" data-level="20.5.5" data-path="string-processing.html"><a href="string-processing.html#quantifiers"><i class="fa fa-check"></i><b>20.5.5</b> Quantifiers</a></li>
<li class="chapter" data-level="20.5.6" data-path="string-processing.html"><a href="string-processing.html#white-space-s"><i class="fa fa-check"></i><b>20.5.6</b> White space <code>\s</code></a></li>
<li class="chapter" data-level="20.5.7" data-path="string-processing.html"><a href="string-processing.html#quantifiers-1"><i class="fa fa-check"></i><b>20.5.7</b> Quantifiers: <code>*</code>, <code>?</code>, <code>+</code></a></li>
<li class="chapter" data-level="20.5.8" data-path="string-processing.html"><a href="string-processing.html#not"><i class="fa fa-check"></i><b>20.5.8</b> Not</a></li>
<li class="chapter" data-level="20.5.9" data-path="string-processing.html"><a href="string-processing.html#groups"><i class="fa fa-check"></i><b>20.5.9</b> Groups</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-with-regex"><i class="fa fa-check"></i><b>20.6</b> Search and replace with regex</a><ul>
<li class="chapter" data-level="20.6.1" data-path="string-processing.html"><a href="string-processing.html#search-and-replace-using-groups"><i class="fa fa-check"></i><b>20.6.1</b> Search and replace using groups</a></li>
</ul></li>
<li class="chapter" data-level="20.7" data-path="string-processing.html"><a href="string-processing.html#testing-and-improving"><i class="fa fa-check"></i><b>20.7</b> Testing and improving</a></li>
<li class="chapter" data-level="20.8" data-path="string-processing.html"><a href="string-processing.html#trimming"><i class="fa fa-check"></i><b>20.8</b> Trimming</a></li>
<li class="chapter" data-level="20.9" data-path="string-processing.html"><a href="string-processing.html#changing-lettercase"><i class="fa fa-check"></i><b>20.9</b> Changing lettercase</a></li>
<li class="chapter" data-level="20.10" data-path="string-processing.html"><a href="string-processing.html#case-study-2-self-reported-heights-continued"><i class="fa fa-check"></i><b>20.10</b> Case study 2: Self reported heights (continued)</a><ul>
<li class="chapter" data-level="20.10.1" data-path="string-processing.html"><a href="string-processing.html#the-extract-function"><i class="fa fa-check"></i><b>20.10.1</b> The <code>extract</code> function</a></li>
<li class="chapter" data-level="20.10.2" data-path="string-processing.html"><a href="string-processing.html#putting-it-all-together-1"><i class="fa fa-check"></i><b>20.10.2</b> Putting it all together</a></li>
</ul></li>
<li class="chapter" data-level="20.11" data-path="string-processing.html"><a href="string-processing.html#string-splitting"><i class="fa fa-check"></i><b>20.11</b> String splitting</a></li>
<li class="chapter" data-level="20.12" data-path="string-processing.html"><a href="string-processing.html#case-study-3-extracting-tables-from-a-pdf"><i class="fa fa-check"></i><b>20.12</b> Case study 3: Extracting tables from a PDF</a></li>
<li class="chapter" data-level="20.13" data-path="string-processing.html"><a href="string-processing.html#recode"><i class="fa fa-check"></i><b>20.13</b> Re-coding</a></li>
<li class="chapter" data-level="20.14" data-path="string-processing.html"><a href="string-processing.html#exercises-23"><i class="fa fa-check"></i><b>20.14</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html"><i class="fa fa-check"></i><b>21</b> Parsing Dates and Times</a><ul>
<li class="chapter" data-level="21.1" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#the-date-data-type"><i class="fa fa-check"></i><b>21.1</b> The date data type</a></li>
<li class="chapter" data-level="21.2" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#lubridate"><i class="fa fa-check"></i><b>21.2</b> The lubridate package</a></li>
<li class="chapter" data-level="21.3" data-path="parsing-dates-and-times.html"><a href="parsing-dates-and-times.html#exercises-24"><i class="fa fa-check"></i><b>21.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="text-mining.html"><a href="text-mining.html"><i class="fa fa-check"></i><b>22</b> Text mining</a><ul>
<li class="chapter" data-level="22.1" data-path="text-mining.html"><a href="text-mining.html#case-study-trump-tweets"><i class="fa fa-check"></i><b>22.1</b> Case study: Trump tweets</a></li>
<li class="chapter" data-level="22.2" data-path="text-mining.html"><a href="text-mining.html#text-as-data"><i class="fa fa-check"></i><b>22.2</b> Text as data</a></li>
<li class="chapter" data-level="22.3" data-path="text-mining.html"><a href="text-mining.html#sentiment-analysis"><i class="fa fa-check"></i><b>22.3</b> Sentiment analysis</a></li>
<li class="chapter" data-level="22.4" data-path="text-mining.html"><a href="text-mining.html#exercises-25"><i class="fa fa-check"></i><b>22.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Probability, Inference, and Regression with R</b></span></li>
<li class="chapter" data-level="23" data-path="introduction-to-statistics-with-r.html"><a href="introduction-to-statistics-with-r.html"><i class="fa fa-check"></i><b>23</b> Introduction to Statistics with R</a></li>
<li class="chapter" data-level="24" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>24</b> Probability</a><ul>
<li class="chapter" data-level="24.1" data-path="probability.html"><a href="probability.html#discrete-probability"><i class="fa fa-check"></i><b>24.1</b> Discrete probability</a><ul>
<li class="chapter" data-level="24.1.1" data-path="probability.html"><a href="probability.html#relative-frequency"><i class="fa fa-check"></i><b>24.1.1</b> Relative frequency</a></li>
<li class="chapter" data-level="24.1.2" data-path="probability.html"><a href="probability.html#notation"><i class="fa fa-check"></i><b>24.1.2</b> Notation</a></li>
<li class="chapter" data-level="24.1.3" data-path="probability.html"><a href="probability.html#monte-carlo-simulations"><i class="fa fa-check"></i><b>24.1.3</b> Monte Carlo simulations</a></li>
<li class="chapter" data-level="24.1.4" data-path="probability.html"><a href="probability.html#setting-the-random-seed"><i class="fa fa-check"></i><b>24.1.4</b> Setting the random seed</a></li>
<li class="chapter" data-level="" data-path="probability.html"><a href="probability.html#with-and-without-replacement"><i class="fa fa-check"></i>With and without replacement</a></li>
<li class="chapter" data-level="24.1.5" data-path="probability.html"><a href="probability.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>24.1.5</b> Discrete probability distributions</a></li>
<li class="chapter" data-level="24.1.6" data-path="probability.html"><a href="probability.html#independence"><i class="fa fa-check"></i><b>24.1.6</b> Independence</a></li>
<li class="chapter" data-level="24.1.7" data-path="probability.html"><a href="probability.html#conditional-probabilities"><i class="fa fa-check"></i><b>24.1.7</b> Conditional probabilities</a></li>
<li class="chapter" data-level="24.1.8" data-path="probability.html"><a href="probability.html#multiplication-rule"><i class="fa fa-check"></i><b>24.1.8</b> Multiplication rule</a></li>
<li class="chapter" data-level="24.1.9" data-path="probability.html"><a href="probability.html#multiplication-rule-under-indepedence"><i class="fa fa-check"></i><b>24.1.9</b> Multiplication rule under indepedence</a></li>
<li class="chapter" data-level="24.1.10" data-path="probability.html"><a href="probability.html#addition-rule"><i class="fa fa-check"></i><b>24.1.10</b> Addition rule</a></li>
<li class="chapter" data-level="24.1.11" data-path="probability.html"><a href="probability.html#combinations-and-permutations"><i class="fa fa-check"></i><b>24.1.11</b> Combinations and permutations</a></li>
<li class="chapter" data-level="24.1.12" data-path="probability.html"><a href="probability.html#monte-carlo-example"><i class="fa fa-check"></i><b>24.1.12</b> Monte Carlo example</a></li>
<li class="chapter" data-level="24.1.13" data-path="probability.html"><a href="probability.html#birthday-problem-example"><i class="fa fa-check"></i><b>24.1.13</b> Birthday problem example</a></li>
<li class="chapter" data-level="24.1.14" data-path="probability.html"><a href="probability.html#how-many-monte-carlo-experiments-are-enough"><i class="fa fa-check"></i><b>24.1.14</b> How many Monte Carlo experiments are enough</a></li>
<li class="chapter" data-level="24.1.15" data-path="probability.html"><a href="probability.html#monty-hall-problem-example"><i class="fa fa-check"></i><b>24.1.15</b> Monty Hall problem example</a></li>
<li class="chapter" data-level="24.1.16" data-path="probability.html"><a href="probability.html#exercises-26"><i class="fa fa-check"></i><b>24.1.16</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="probability.html"><a href="probability.html#continuous-probability"><i class="fa fa-check"></i><b>24.2</b> Continuous probability</a><ul>
<li class="chapter" data-level="24.2.1" data-path="probability.html"><a href="probability.html#theoretical-distribution"><i class="fa fa-check"></i><b>24.2.1</b> Theoretical distribution</a></li>
<li class="chapter" data-level="24.2.2" data-path="probability.html"><a href="probability.html#theoretical-distributions-as-approximations"><i class="fa fa-check"></i><b>24.2.2</b> Theoretical distributions as approximations</a></li>
<li class="chapter" data-level="24.2.3" data-path="probability.html"><a href="probability.html#the-probability-density"><i class="fa fa-check"></i><b>24.2.3</b> The probability density</a></li>
<li class="chapter" data-level="24.2.4" data-path="probability.html"><a href="probability.html#monte-carlo-simulations-for-continuous-variables"><i class="fa fa-check"></i><b>24.2.4</b> Monte Carlo simulations for continuous variables</a></li>
<li class="chapter" data-level="24.2.5" data-path="probability.html"><a href="probability.html#other-continuous-distributions"><i class="fa fa-check"></i><b>24.2.5</b> Other continuous distributions</a></li>
<li class="chapter" data-level="24.2.6" data-path="probability.html"><a href="probability.html#exercises-27"><i class="fa fa-check"></i><b>24.2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>24.3</b> Random variables</a><ul>
<li class="chapter" data-level="24.3.1" data-path="probability.html"><a href="probability.html#sampling-models"><i class="fa fa-check"></i><b>24.3.1</b> Sampling models</a></li>
<li class="chapter" data-level="24.3.2" data-path="probability.html"><a href="probability.html#the-probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>24.3.2</b> The probability distribution of a random variable</a></li>
<li class="chapter" data-level="24.3.3" data-path="probability.html"><a href="probability.html#distributions-versus-probability-distributions"><i class="fa fa-check"></i><b>24.3.3</b> Distributions versus probability distributions</a></li>
<li class="chapter" data-level="24.3.4" data-path="probability.html"><a href="probability.html#notation-for-random-variables"><i class="fa fa-check"></i><b>24.3.4</b> Notation for random variables</a></li>
<li class="chapter" data-level="24.3.5" data-path="probability.html"><a href="probability.html#central-limit-theorem"><i class="fa fa-check"></i><b>24.3.5</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="24.3.6" data-path="probability.html"><a href="probability.html#the-expected-value-and-standard-error"><i class="fa fa-check"></i><b>24.3.6</b> The expected value and standard error</a></li>
<li class="chapter" data-level="24.3.7" data-path="probability.html"><a href="probability.html#central-limit-theorem-approximation"><i class="fa fa-check"></i><b>24.3.7</b> Central Limit Theorem approximation</a></li>
<li class="chapter" data-level="24.3.8" data-path="probability.html"><a href="probability.html#statistical-properties-of-averages"><i class="fa fa-check"></i><b>24.3.8</b> Statistical properties of averages</a></li>
<li class="chapter" data-level="24.3.9" data-path="probability.html"><a href="probability.html#law-of-large-numbers"><i class="fa fa-check"></i><b>24.3.9</b> Law of large numbers</a></li>
<li class="chapter" data-level="24.3.10" data-path="probability.html"><a href="probability.html#misinterpreting-law-of-averages"><i class="fa fa-check"></i><b>24.3.10</b> Misinterpreting law of averages</a></li>
<li class="chapter" data-level="24.3.11" data-path="probability.html"><a href="probability.html#how-large-is-large-in-clt"><i class="fa fa-check"></i><b>24.3.11</b> How large is large in CLT?</a></li>
<li class="chapter" data-level="24.3.12" data-path="probability.html"><a href="probability.html#population-sd-versus-the-sample-sd"><i class="fa fa-check"></i><b>24.3.12</b> Population SD versus the sample SD</a></li>
<li class="chapter" data-level="24.3.13" data-path="probability.html"><a href="probability.html#exercises-28"><i class="fa fa-check"></i><b>24.3.13</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="probability.html"><a href="probability.html#case-study-the-big-short"><i class="fa fa-check"></i><b>24.4</b> Case study: The Big Short</a><ul>
<li class="chapter" data-level="24.4.1" data-path="probability.html"><a href="probability.html#interest-rates-explained-with-chance-model"><i class="fa fa-check"></i><b>24.4.1</b> Interest rates explained with chance model</a></li>
<li class="chapter" data-level="24.4.2" data-path="probability.html"><a href="probability.html#the-big-short"><i class="fa fa-check"></i><b>24.4.2</b> The Big Short</a></li>
<li class="chapter" data-level="24.4.3" data-path="probability.html"><a href="probability.html#exercises-29"><i class="fa fa-check"></i><b>24.4.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>25</b> Statistical Inference</a><ul>
<li class="chapter" data-level="25.1" data-path="statistical-inference.html"><a href="statistical-inference.html#polls"><i class="fa fa-check"></i><b>25.1</b> Polls</a><ul>
<li class="chapter" data-level="25.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-sampling-model-for-polls"><i class="fa fa-check"></i><b>25.1.1</b> The sampling model for polls</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="statistical-inference.html"><a href="statistical-inference.html#populations-samples-parameters-and-estimates"><i class="fa fa-check"></i><b>25.2</b> Populations, samples, parameters and estimates</a><ul>
<li class="chapter" data-level="25.2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-sample-average"><i class="fa fa-check"></i><b>25.2.1</b> The sample average</a></li>
<li class="chapter" data-level="25.2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#parameters"><i class="fa fa-check"></i><b>25.2.2</b> Parameters</a></li>
<li class="chapter" data-level="25.2.3" data-path="statistical-inference.html"><a href="statistical-inference.html#polling-versus-forecasting"><i class="fa fa-check"></i><b>25.2.3</b> Polling versus forecasting</a></li>
<li class="chapter" data-level="25.2.4" data-path="statistical-inference.html"><a href="statistical-inference.html#properties-of-our-estimate-expected-value-and-standard-error"><i class="fa fa-check"></i><b>25.2.4</b> Properties of our estimate: expected value and standard error</a></li>
<li class="chapter" data-level="25.2.5" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-30"><i class="fa fa-check"></i><b>25.2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="statistical-inference.html"><a href="statistical-inference.html#clt"><i class="fa fa-check"></i><b>25.3</b> Central Limit Theorem in practice</a><ul>
<li class="chapter" data-level="25.3.1" data-path="statistical-inference.html"><a href="statistical-inference.html#a-monte-carlo-simulation"><i class="fa fa-check"></i><b>25.3.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="25.3.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-spread"><i class="fa fa-check"></i><b>25.3.2</b> The spread</a></li>
<li class="chapter" data-level="25.3.3" data-path="statistical-inference.html"><a href="statistical-inference.html#bias-why-not-run-a-very-large-poll"><i class="fa fa-check"></i><b>25.3.3</b> Bias: why not run a very large poll?</a></li>
<li class="chapter" data-level="25.3.4" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-31"><i class="fa fa-check"></i><b>25.3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>25.4</b> Confidence intervals</a><ul>
<li class="chapter" data-level="25.4.1" data-path="statistical-inference.html"><a href="statistical-inference.html#a-monte-carlo-simulation-1"><i class="fa fa-check"></i><b>25.4.1</b> A Monte Carlo simulation</a></li>
<li class="chapter" data-level="25.4.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-correct-language"><i class="fa fa-check"></i><b>25.4.2</b> The correct language</a></li>
<li class="chapter" data-level="25.4.3" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-32"><i class="fa fa-check"></i><b>25.4.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="statistical-inference.html"><a href="statistical-inference.html#power"><i class="fa fa-check"></i><b>25.5</b> Power</a></li>
<li class="chapter" data-level="25.6" data-path="statistical-inference.html"><a href="statistical-inference.html#p-values"><i class="fa fa-check"></i><b>25.6</b> p-values</a></li>
<li class="chapter" data-level="25.7" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-models"><i class="fa fa-check"></i><b>25.7</b> Statistical models</a><ul>
<li class="chapter" data-level="25.7.1" data-path="statistical-inference.html"><a href="statistical-inference.html#poll-aggregators"><i class="fa fa-check"></i><b>25.7.1</b> Poll aggregators</a></li>
<li class="chapter" data-level="25.7.2" data-path="statistical-inference.html"><a href="statistical-inference.html#poll-data"><i class="fa fa-check"></i><b>25.7.2</b> Poll data</a></li>
<li class="chapter" data-level="25.7.3" data-path="statistical-inference.html"><a href="statistical-inference.html#pollster-bias"><i class="fa fa-check"></i><b>25.7.3</b> Pollster bias</a></li>
<li class="chapter" data-level="25.7.4" data-path="statistical-inference.html"><a href="statistical-inference.html#data-driven-model"><i class="fa fa-check"></i><b>25.7.4</b> A data driven model</a></li>
<li class="chapter" data-level="25.7.5" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-33"><i class="fa fa-check"></i><b>25.7.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25.8" data-path="statistical-inference.html"><a href="statistical-inference.html#bayesian-statistics"><i class="fa fa-check"></i><b>25.8</b> Bayesian statistics</a><ul>
<li class="chapter" data-level="25.8.1" data-path="statistical-inference.html"><a href="statistical-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>25.8.1</b> Bayes theorem</a></li>
</ul></li>
<li class="chapter" data-level="25.9" data-path="statistical-inference.html"><a href="statistical-inference.html#bayes-theorem-simulation"><i class="fa fa-check"></i><b>25.9</b> Bayes Theorem simulation</a><ul>
<li class="chapter" data-level="25.9.1" data-path="statistical-inference.html"><a href="statistical-inference.html#bayes-in-practice"><i class="fa fa-check"></i><b>25.9.1</b> Bayes in practice</a></li>
<li class="chapter" data-level="25.9.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-hierarchical-model"><i class="fa fa-check"></i><b>25.9.2</b> The hierarchical model</a></li>
<li class="chapter" data-level="25.9.3" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-34"><i class="fa fa-check"></i><b>25.9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="25.10" data-path="statistical-inference.html"><a href="statistical-inference.html#election-forecasting"><i class="fa fa-check"></i><b>25.10</b> Case study: Election forecasting</a><ul>
<li class="chapter" data-level="25.10.1" data-path="statistical-inference.html"><a href="statistical-inference.html#bayesian-approach"><i class="fa fa-check"></i><b>25.10.1</b> Bayesian approach</a></li>
<li class="chapter" data-level="25.10.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-general-bias"><i class="fa fa-check"></i><b>25.10.2</b> The general bias</a></li>
<li class="chapter" data-level="25.10.3" data-path="statistical-inference.html"><a href="statistical-inference.html#mathematical-representations-of-models"><i class="fa fa-check"></i><b>25.10.3</b> Mathematical representations of models</a></li>
<li class="chapter" data-level="25.10.4" data-path="statistical-inference.html"><a href="statistical-inference.html#predicting-the-electoral-college"><i class="fa fa-check"></i><b>25.10.4</b> Predicting the electoral college</a></li>
<li class="chapter" data-level="25.10.5" data-path="statistical-inference.html"><a href="statistical-inference.html#forecasting"><i class="fa fa-check"></i><b>25.10.5</b> Forecasting</a></li>
<li class="chapter" data-level="25.10.6" data-path="statistical-inference.html"><a href="statistical-inference.html#exercise-1"><i class="fa fa-check"></i><b>25.10.6</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="25.11" data-path="statistical-inference.html"><a href="statistical-inference.html#t-dist"><i class="fa fa-check"></i><b>25.11</b> The t-distribution</a></li>
<li class="chapter" data-level="25.12" data-path="statistical-inference.html"><a href="statistical-inference.html#association-tests"><i class="fa fa-check"></i><b>25.12</b> Association Tests</a><ul>
<li class="chapter" data-level="25.12.1" data-path="statistical-inference.html"><a href="statistical-inference.html#lady-tasting-tea"><i class="fa fa-check"></i><b>25.12.1</b> Lady Tasting Tea</a></li>
<li class="chapter" data-level="25.12.2" data-path="statistical-inference.html"><a href="statistical-inference.html#two-by-two-tables"><i class="fa fa-check"></i><b>25.12.2</b> Two-by-two tables</a></li>
<li class="chapter" data-level="25.12.3" data-path="statistical-inference.html"><a href="statistical-inference.html#chi-square-test"><i class="fa fa-check"></i><b>25.12.3</b> Chi-square Test</a></li>
<li class="chapter" data-level="25.12.4" data-path="statistical-inference.html"><a href="statistical-inference.html#the-odds-ratio-odds-ratio"><i class="fa fa-check"></i><b>25.12.4</b> The Odds Ratio {odds-ratio}</a></li>
<li class="chapter" data-level="25.12.5" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-for-the-odds-ratio"><i class="fa fa-check"></i><b>25.12.5</b> Confidence intervals for the odds ratio</a></li>
<li class="chapter" data-level="25.12.6" data-path="statistical-inference.html"><a href="statistical-inference.html#small-count-correction"><i class="fa fa-check"></i><b>25.12.6</b> Small count correction</a></li>
<li class="chapter" data-level="25.12.7" data-path="statistical-inference.html"><a href="statistical-inference.html#large-samples-small-p-values"><i class="fa fa-check"></i><b>25.12.7</b> Large samples, small p-values</a></li>
<li class="chapter" data-level="25.12.8" data-path="statistical-inference.html"><a href="statistical-inference.html#exercises-35"><i class="fa fa-check"></i><b>25.12.8</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>26</b> Regression</a><ul>
<li class="chapter" data-level="26.1" data-path="regression.html"><a href="regression.html#case-study-is-height-hereditary"><i class="fa fa-check"></i><b>26.1</b> Case study: Is height hereditary?</a></li>
<li class="chapter" data-level="26.2" data-path="regression.html"><a href="regression.html#the-correlation-coefficient"><i class="fa fa-check"></i><b>26.2</b> The correlation coefficient</a><ul>
<li class="chapter" data-level="26.2.1" data-path="regression.html"><a href="regression.html#sample-correlation-is-a-random-variable"><i class="fa fa-check"></i><b>26.2.1</b> Sample correlation is a random variable</a></li>
<li class="chapter" data-level="26.2.2" data-path="regression.html"><a href="regression.html#correlation-is-not-always-a-useful-summary"><i class="fa fa-check"></i><b>26.2.2</b> Correlation is not always a useful summary</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="regression.html"><a href="regression.html#conditional-expectation"><i class="fa fa-check"></i><b>26.3</b> Conditional Expectations</a></li>
<li class="chapter" data-level="26.4" data-path="regression.html"><a href="regression.html#the-regression-line"><i class="fa fa-check"></i><b>26.4</b> The regression line</a><ul>
<li class="chapter" data-level="26.4.1" data-path="regression.html"><a href="regression.html#regression-improves-precision"><i class="fa fa-check"></i><b>26.4.1</b> Regression improves precision</a></li>
<li class="chapter" data-level="26.4.2" data-path="regression.html"><a href="regression.html#bivariate-normal-distribution-advanced"><i class="fa fa-check"></i><b>26.4.2</b> Bivariate normal distribution (advanced)</a></li>
<li class="chapter" data-level="26.4.3" data-path="regression.html"><a href="regression.html#variance-explained"><i class="fa fa-check"></i><b>26.4.3</b> Variance explained</a></li>
<li class="chapter" data-level="26.4.4" data-path="regression.html"><a href="regression.html#warning-there-are-two-regression-lines"><i class="fa fa-check"></i><b>26.4.4</b> Warning: there are two regression lines</a></li>
<li class="chapter" data-level="26.4.5" data-path="regression.html"><a href="regression.html#exercises-36"><i class="fa fa-check"></i><b>26.4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="regression.html"><a href="regression.html#case-study-moneyball"><i class="fa fa-check"></i><b>26.5</b> Case Study: Moneyball</a><ul>
<li class="chapter" data-level="26.5.1" data-path="regression.html"><a href="regression.html#sabermetics"><i class="fa fa-check"></i><b>26.5.1</b> Sabermetics</a></li>
<li class="chapter" data-level="26.5.2" data-path="regression.html"><a href="regression.html#baseball-basics"><i class="fa fa-check"></i><b>26.5.2</b> Baseball basics</a></li>
</ul></li>
<li class="chapter" data-level="26.6" data-path="regression.html"><a href="regression.html#no-awards-for-bb"><i class="fa fa-check"></i><b>26.6</b> No awards for BB</a><ul>
<li class="chapter" data-level="26.6.1" data-path="regression.html"><a href="regression.html#base-on-ball-or-stolen-bases"><i class="fa fa-check"></i><b>26.6.1</b> Base on Ball or Stolen Bases?</a></li>
<li class="chapter" data-level="26.6.2" data-path="regression.html"><a href="regression.html#regression-applied-to-baseball-statistics"><i class="fa fa-check"></i><b>26.6.2</b> Regression applied to baseball statistics</a></li>
</ul></li>
<li class="chapter" data-level="26.7" data-path="regression.html"><a href="regression.html#confounding"><i class="fa fa-check"></i><b>26.7</b> Confounding</a><ul>
<li class="chapter" data-level="26.7.1" data-path="regression.html"><a href="regression.html#understanding-confounding-through-stratification"><i class="fa fa-check"></i><b>26.7.1</b> Understanding confounding through stratification</a></li>
<li class="chapter" data-level="26.7.2" data-path="regression.html"><a href="regression.html#multivariate-regression"><i class="fa fa-check"></i><b>26.7.2</b> Multivariate regression</a></li>
</ul></li>
<li class="chapter" data-level="26.8" data-path="regression.html"><a href="regression.html#lse"><i class="fa fa-check"></i><b>26.8</b> Linear Models and Least Squared Estimates</a><ul>
<li class="chapter" data-level="26.8.1" data-path="regression.html"><a href="regression.html#interpreting-linear-models"><i class="fa fa-check"></i><b>26.8.1</b> Interpreting linear models</a></li>
<li class="chapter" data-level="26.8.2" data-path="regression.html"><a href="regression.html#least-squares-estimates-lse"><i class="fa fa-check"></i><b>26.8.2</b> Least Squares Estimates (LSE)</a></li>
<li class="chapter" data-level="26.8.3" data-path="regression.html"><a href="regression.html#the-lm-function"><i class="fa fa-check"></i><b>26.8.3</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="26.8.4" data-path="regression.html"><a href="regression.html#lse-are-random-variables"><i class="fa fa-check"></i><b>26.8.4</b> LSE are random variables</a></li>
<li class="chapter" data-level="26.8.5" data-path="regression.html"><a href="regression.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>26.8.5</b> Predicted values are random variables</a></li>
<li class="chapter" data-level="26.8.6" data-path="regression.html"><a href="regression.html#exercises-37"><i class="fa fa-check"></i><b>26.8.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26.9" data-path="regression.html"><a href="regression.html#linear-regression-in-the-tidyverse"><i class="fa fa-check"></i><b>26.9</b> Linear regression in the tidyverse</a><ul>
<li class="chapter" data-level="26.9.1" data-path="regression.html"><a href="regression.html#the-broom-package"><i class="fa fa-check"></i><b>26.9.1</b> The broom package</a></li>
<li class="chapter" data-level="26.9.2" data-path="regression.html"><a href="regression.html#exercises-38"><i class="fa fa-check"></i><b>26.9.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26.10" data-path="regression.html"><a href="regression.html#case-study-moneyball-continued"><i class="fa fa-check"></i><b>26.10</b> Case study: Moneyball continued</a><ul>
<li class="chapter" data-level="26.10.1" data-path="regression.html"><a href="regression.html#adding-salary-and-position-information"><i class="fa fa-check"></i><b>26.10.1</b> Adding salary and position information</a></li>
<li class="chapter" data-level="26.10.2" data-path="regression.html"><a href="regression.html#picking-9-players"><i class="fa fa-check"></i><b>26.10.2</b> Picking 9 players</a></li>
<li class="chapter" data-level="26.10.3" data-path="regression.html"><a href="regression.html#exercises-39"><i class="fa fa-check"></i><b>26.10.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="26.11" data-path="regression.html"><a href="regression.html#regression-fallacy"><i class="fa fa-check"></i><b>26.11</b> Regression fallacy</a></li>
<li class="chapter" data-level="26.12" data-path="regression.html"><a href="regression.html#measurement-error-models"><i class="fa fa-check"></i><b>26.12</b> Measurement error models</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html"><i class="fa fa-check"></i><b>27</b> Association is not causation</a><ul>
<li class="chapter" data-level="27.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#spurious-correlation"><i class="fa fa-check"></i><b>27.1</b> Spurious correlation</a></li>
<li class="chapter" data-level="27.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#outliers-1"><i class="fa fa-check"></i><b>27.2</b> Outliers</a></li>
<li class="chapter" data-level="27.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>27.3</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="27.4" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounders"><i class="fa fa-check"></i><b>27.4</b> Confounders</a><ul>
<li class="chapter" data-level="27.4.1" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#example-uc-berkeley-admissions"><i class="fa fa-check"></i><b>27.4.1</b> Example: UC Berkeley admissions</a></li>
<li class="chapter" data-level="27.4.2" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#confounding-explained-graphically"><i class="fa fa-check"></i><b>27.4.2</b> Confounding explained graphically</a></li>
<li class="chapter" data-level="27.4.3" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#average-after-stratifying"><i class="fa fa-check"></i><b>27.4.3</b> Average after stratifying</a></li>
</ul></li>
<li class="chapter" data-level="27.5" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#simpsons-paradox"><i class="fa fa-check"></i><b>27.5</b> Simpson’s Paradox</a></li>
<li class="chapter" data-level="27.6" data-path="association-is-not-causation.html"><a href="association-is-not-causation.html#exercises-40"><i class="fa fa-check"></i><b>27.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Machine Learning</b></span></li>
<li class="chapter" data-level="28" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>28</b> Introduction to Machine Learning</a><ul>
<li class="chapter" data-level="28.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#notation-1"><i class="fa fa-check"></i><b>28.1</b> Notation</a></li>
<li class="chapter" data-level="28.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#an-example"><i class="fa fa-check"></i><b>28.2</b> An example</a><ul>
<li class="chapter" data-level="28.2.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-41"><i class="fa fa-check"></i><b>28.2.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="28.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#evaluation-metrics"><i class="fa fa-check"></i><b>28.3</b> Evaluation Metrics</a><ul>
<li class="chapter" data-level="28.3.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#training-and-test-sets"><i class="fa fa-check"></i><b>28.3.1</b> Training and test sets</a></li>
<li class="chapter" data-level="28.3.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#overall-accuracy"><i class="fa fa-check"></i><b>28.3.2</b> Overall accuracy</a></li>
<li class="chapter" data-level="28.3.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-confusion-matrix"><i class="fa fa-check"></i><b>28.3.3</b> The confusion matrix</a></li>
<li class="chapter" data-level="28.3.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#sensitivity-and-specificity"><i class="fa fa-check"></i><b>28.3.4</b> Sensitivity and specificity</a></li>
<li class="chapter" data-level="28.3.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score"><i class="fa fa-check"></i><b>28.3.5</b> Balanced accuracy and <span class="math inline">\(F_1\)</span> score</a></li>
<li class="chapter" data-level="28.3.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#prevalence-matters-in-practice"><i class="fa fa-check"></i><b>28.3.6</b> Prevalence matters in practice</a></li>
<li class="chapter" data-level="28.3.7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#roc-and-precision-recall-curves"><i class="fa fa-check"></i><b>28.3.7</b> ROC and precision-recall curves</a></li>
<li class="chapter" data-level="28.3.8" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-42"><i class="fa fa-check"></i><b>28.3.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="28.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>28.4</b> Conditional probabilities and expectations</a><ul>
<li class="chapter" data-level="28.4.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-probabilities-1"><i class="fa fa-check"></i><b>28.4.1</b> Conditional probabilities</a></li>
<li class="chapter" data-level="28.4.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectations"><i class="fa fa-check"></i><b>28.4.2</b> Conditional expectations</a></li>
<li class="chapter" data-level="28.4.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#the-loss-function"><i class="fa fa-check"></i><b>28.4.3</b> The loss function</a></li>
<li class="chapter" data-level="28.4.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#conditional-expectation-minimizes-squared-loss-function"><i class="fa fa-check"></i><b>28.4.4</b> Conditional expectation minimizes squared loss function</a></li>
<li class="chapter" data-level="28.4.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#exercises-43"><i class="fa fa-check"></i><b>28.4.5</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="case-study-is-it-a-2-or-a-7.html"><a href="case-study-is-it-a-2-or-a-7.html"><i class="fa fa-check"></i><b>29</b> Case study: is it a 2 or a 7?</a></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a><ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>30.1</b> Bin smoothing</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernels"><i class="fa fa-check"></i><b>30.2</b> Kernels</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#local-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Local weighted regression (loess)</a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#fitting-parabolas"><i class="fa fa-check"></i><b>30.4</b> Fitting parabolas</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#beware-of-ggplot-defaults"><i class="fa fa-check"></i><b>30.5</b> Beware of <code>ggplot</code> defaults</a></li>
<li class="chapter" data-level="30.6" data-path="smoothing.html"><a href="smoothing.html#exercises-44"><i class="fa fa-check"></i><b>30.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html"><i class="fa fa-check"></i><b>31</b> Examples of algorithms</a><ul>
<li class="chapter" data-level="31.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-regression"><i class="fa fa-check"></i><b>31.1</b> Linear regression</a><ul>
<li class="chapter" data-level="31.1.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-predict-function"><i class="fa fa-check"></i><b>31.1.1</b> The <code>predict</code> function</a></li>
<li class="chapter" data-level="31.1.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-45"><i class="fa fa-check"></i><b>31.1.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#logistic-regression"><i class="fa fa-check"></i><b>31.2</b> Logistic regression</a><ul>
<li class="chapter" data-level="31.2.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generalized-linear-models"><i class="fa fa-check"></i><b>31.2.1</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="31.2.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-46"><i class="fa fa-check"></i><b>31.2.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>31.3</b> K-nearest neighbors</a><ul>
<li class="chapter" data-level="31.3.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#over-training"><i class="fa fa-check"></i><b>31.3.1</b> Over training</a></li>
<li class="chapter" data-level="31.3.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#over-smoothing"><i class="fa fa-check"></i><b>31.3.2</b> Over-smoothing</a></li>
<li class="chapter" data-level="31.3.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#picking-the-k-in-knn"><i class="fa fa-check"></i><b>31.3.3</b> Picking the <span class="math inline">\(k\)</span> in kNN</a></li>
<li class="chapter" data-level="31.3.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-47"><i class="fa fa-check"></i><b>31.3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#generative-models"><i class="fa fa-check"></i><b>31.4</b> Generative models</a><ul>
<li class="chapter" data-level="31.4.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#naive-bayes"><i class="fa fa-check"></i><b>31.4.1</b> Naive Bayes</a></li>
<li class="chapter" data-level="31.4.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#controlling-prevalence"><i class="fa fa-check"></i><b>31.4.2</b> Controlling prevalence</a></li>
<li class="chapter" data-level="31.4.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>31.4.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="31.4.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>31.4.4</b> Linear discriminant analysis</a></li>
<li class="chapter" data-level="31.4.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#connection-to-distance"><i class="fa fa-check"></i><b>31.4.5</b> Connection to distance</a></li>
<li class="chapter" data-level="31.4.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#case-study-more-than-three-classes"><i class="fa fa-check"></i><b>31.4.6</b> Case study: more than three classes</a></li>
<li class="chapter" data-level="31.4.7" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-48"><i class="fa fa-check"></i><b>31.4.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="31.5" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-and-regression-trees-cart"><i class="fa fa-check"></i><b>31.5</b> Classification and Regression Trees (CART)</a><ul>
<li class="chapter" data-level="31.5.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>31.5.1</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="31.5.2" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#cart-motivation"><i class="fa fa-check"></i><b>31.5.2</b> CART motivation</a></li>
<li class="chapter" data-level="31.5.3" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#regression-trees"><i class="fa fa-check"></i><b>31.5.3</b> Regression trees</a></li>
<li class="chapter" data-level="31.5.4" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#classification-decision-trees"><i class="fa fa-check"></i><b>31.5.4</b> Classification (decision) trees</a></li>
</ul></li>
<li class="chapter" data-level="31.6" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#random-forests"><i class="fa fa-check"></i><b>31.6</b> Random Forests</a><ul>
<li class="chapter" data-level="31.6.1" data-path="examples-of-algorithms.html"><a href="examples-of-algorithms.html#exercises-49"><i class="fa fa-check"></i><b>31.6.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="32" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>32</b> Cross validation</a><ul>
<li class="chapter" data-level="32.1" data-path="cross-validation.html"><a href="cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>32.1</b> K-fold cross validation</a></li>
<li class="chapter" data-level="32.2" data-path="cross-validation.html"><a href="cross-validation.html#exercises-50"><i class="fa fa-check"></i><b>32.2</b> Exercises</a></li>
<li class="chapter" data-level="32.3" data-path="cross-validation.html"><a href="cross-validation.html#bootstrap"><i class="fa fa-check"></i><b>32.3</b> Bootstrap</a></li>
<li class="chapter" data-level="32.4" data-path="cross-validation.html"><a href="cross-validation.html#exercises-51"><i class="fa fa-check"></i><b>32.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>33</b> The caret package</a><ul>
<li class="chapter" data-level="33.1" data-path="caret.html"><a href="caret.html#the-caret-train-functon"><i class="fa fa-check"></i><b>33.1</b> The caret <code>train</code> functon</a></li>
<li class="chapter" data-level="33.2" data-path="cross-validation.html"><a href="cross-validation.html#cross-validation"><i class="fa fa-check"></i><b>33.2</b> Cross validation</a></li>
<li class="chapter" data-level="33.3" data-path="caret.html"><a href="caret.html#example-fitting-with-loess"><i class="fa fa-check"></i><b>33.3</b> Example: fitting with loess</a></li>
</ul></li>
<li class="chapter" data-level="34" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html"><i class="fa fa-check"></i><b>34</b> Machine Learning in practice</a><ul>
<li class="chapter" data-level="34.1" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#preprocessing"><i class="fa fa-check"></i><b>34.1</b> Preprocessing</a></li>
<li class="chapter" data-level="34.2" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#k-nearest-neighbor"><i class="fa fa-check"></i><b>34.2</b> k-Nearest Neighbor</a></li>
<li class="chapter" data-level="34.3" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#variable-importance"><i class="fa fa-check"></i><b>34.3</b> Variable importance</a></li>
<li class="chapter" data-level="34.4" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#visual-assessments"><i class="fa fa-check"></i><b>34.4</b> Visual assessments</a></li>
<li class="chapter" data-level="34.5" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#ensembles"><i class="fa fa-check"></i><b>34.5</b> Ensembles</a></li>
<li class="chapter" data-level="34.6" data-path="machine-learning-in-practice.html"><a href="machine-learning-in-practice.html#exercises-52"><i class="fa fa-check"></i><b>34.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="large-datasets.html"><a href="large-datasets.html"><i class="fa fa-check"></i><b>35</b> Large datasets</a><ul>
<li class="chapter" data-level="35.1" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra"><i class="fa fa-check"></i><b>35.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="35.1.1" data-path="large-datasets.html"><a href="large-datasets.html#notation-2"><i class="fa fa-check"></i><b>35.1.1</b> Notation</a></li>
<li class="chapter" data-level="35.1.2" data-path="large-datasets.html"><a href="large-datasets.html#converting-a-vector-to-a-matrix"><i class="fa fa-check"></i><b>35.1.2</b> Converting a vector to a matrix</a></li>
<li class="chapter" data-level="35.1.3" data-path="large-datasets.html"><a href="large-datasets.html#row-and-column-summaries"><i class="fa fa-check"></i><b>35.1.3</b> Row and column summaries</a></li>
<li class="chapter" data-level="35.1.4" data-path="large-datasets.html"><a href="large-datasets.html#apply"><i class="fa fa-check"></i><b>35.1.4</b> <code>apply</code></a></li>
<li class="chapter" data-level="35.1.5" data-path="large-datasets.html"><a href="large-datasets.html#filtering-columns-based-on-summaries"><i class="fa fa-check"></i><b>35.1.5</b> Filtering columns based on summaries</a></li>
<li class="chapter" data-level="35.1.6" data-path="large-datasets.html"><a href="large-datasets.html#indexing-with-matrices"><i class="fa fa-check"></i><b>35.1.6</b> Indexing with matrices</a></li>
<li class="chapter" data-level="35.1.7" data-path="large-datasets.html"><a href="large-datasets.html#binarizing-the-data"><i class="fa fa-check"></i><b>35.1.7</b> Binarizing the data</a></li>
<li class="chapter" data-level="35.1.8" data-path="large-datasets.html"><a href="large-datasets.html#vectorization-for-matrices"><i class="fa fa-check"></i><b>35.1.8</b> Vectorization for matrices</a></li>
<li class="chapter" data-level="35.1.9" data-path="large-datasets.html"><a href="large-datasets.html#matrix-algebra-operations"><i class="fa fa-check"></i><b>35.1.9</b> Matrix algebra operations</a></li>
<li class="chapter" data-level="35.1.10" data-path="large-datasets.html"><a href="large-datasets.html#exercises-53"><i class="fa fa-check"></i><b>35.1.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="large-datasets.html"><a href="large-datasets.html#distance"><i class="fa fa-check"></i><b>35.2</b> Distance</a><ul>
<li class="chapter" data-level="35.2.1" data-path="large-datasets.html"><a href="large-datasets.html#euclidean-distance"><i class="fa fa-check"></i><b>35.2.1</b> Euclidean distance</a></li>
<li class="chapter" data-level="35.2.2" data-path="large-datasets.html"><a href="large-datasets.html#distance-in-higher-dimensions"><i class="fa fa-check"></i><b>35.2.2</b> Distance in higher dimensions</a></li>
<li class="chapter" data-level="35.2.3" data-path="large-datasets.html"><a href="large-datasets.html#example"><i class="fa fa-check"></i><b>35.2.3</b> Example</a></li>
<li class="chapter" data-level="35.2.4" data-path="large-datasets.html"><a href="large-datasets.html#predictor-space"><i class="fa fa-check"></i><b>35.2.4</b> Predictor Space</a></li>
<li class="chapter" data-level="35.2.5" data-path="large-datasets.html"><a href="large-datasets.html#distance-between-predictors"><i class="fa fa-check"></i><b>35.2.5</b> Distance between predictors</a></li>
<li class="chapter" data-level="35.2.6" data-path="large-datasets.html"><a href="large-datasets.html#exercises-54"><i class="fa fa-check"></i><b>35.2.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35.3" data-path="large-datasets.html"><a href="large-datasets.html#dimension-reduction"><i class="fa fa-check"></i><b>35.3</b> Dimension Reduction</a><ul>
<li class="chapter" data-level="35.3.1" data-path="large-datasets.html"><a href="large-datasets.html#preserving-distance"><i class="fa fa-check"></i><b>35.3.1</b> Preserving distance</a></li>
<li class="chapter" data-level="35.3.2" data-path="large-datasets.html"><a href="large-datasets.html#linear-transformations-advnced"><i class="fa fa-check"></i><b>35.3.2</b> Linear transformations (advnced)</a></li>
<li class="chapter" data-level="35.3.3" data-path="large-datasets.html"><a href="large-datasets.html#orthogogal-transformations-advaced"><i class="fa fa-check"></i><b>35.3.3</b> Orthogogal transformations (advaced)</a></li>
<li class="chapter" data-level="35.3.4" data-path="large-datasets.html"><a href="large-datasets.html#principal-component-analysis"><i class="fa fa-check"></i><b>35.3.4</b> Principal Component Analysis</a></li>
<li class="chapter" data-level="35.3.5" data-path="large-datasets.html"><a href="large-datasets.html#iris-example"><i class="fa fa-check"></i><b>35.3.5</b> Iris Example</a></li>
<li class="chapter" data-level="35.3.6" data-path="large-datasets.html"><a href="large-datasets.html#mnist-example"><i class="fa fa-check"></i><b>35.3.6</b> MNIST Example</a></li>
<li class="chapter" data-level="35.3.7" data-path="large-datasets.html"><a href="large-datasets.html#exercises-55"><i class="fa fa-check"></i><b>35.3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35.4" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems"><i class="fa fa-check"></i><b>35.4</b> Recommendation systems</a><ul>
<li class="chapter" data-level="35.4.1" data-path="large-datasets.html"><a href="large-datasets.html#movielens-data"><i class="fa fa-check"></i><b>35.4.1</b> Movielens data</a></li>
<li class="chapter" data-level="35.4.2" data-path="large-datasets.html"><a href="large-datasets.html#recommendation-systems-as-a-machine-learning-challenge"><i class="fa fa-check"></i><b>35.4.2</b> Recommendation systems as a machine learning challenge</a></li>
<li class="chapter" data-level="35.4.3" data-path="large-datasets.html"><a href="large-datasets.html#loss-function"><i class="fa fa-check"></i><b>35.4.3</b> Loss function</a></li>
<li class="chapter" data-level="35.4.4" data-path="large-datasets.html"><a href="large-datasets.html#a-first-model"><i class="fa fa-check"></i><b>35.4.4</b> A first model</a></li>
<li class="chapter" data-level="35.4.5" data-path="large-datasets.html"><a href="large-datasets.html#modeling-movie-effects"><i class="fa fa-check"></i><b>35.4.5</b> Modeling movie effects</a></li>
<li class="chapter" data-level="35.4.6" data-path="large-datasets.html"><a href="large-datasets.html#user-effects"><i class="fa fa-check"></i><b>35.4.6</b> User effects</a></li>
<li class="chapter" data-level="35.4.7" data-path="large-datasets.html"><a href="large-datasets.html#exercises-56"><i class="fa fa-check"></i><b>35.4.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35.5" data-path="large-datasets.html"><a href="large-datasets.html#regularization"><i class="fa fa-check"></i><b>35.5</b> Regularization</a><ul>
<li class="chapter" data-level="35.5.1" data-path="large-datasets.html"><a href="large-datasets.html#motivation"><i class="fa fa-check"></i><b>35.5.1</b> Motivation</a></li>
<li class="chapter" data-level="35.5.2" data-path="large-datasets.html"><a href="large-datasets.html#penalized-least-squares"><i class="fa fa-check"></i><b>35.5.2</b> Penalized Least Squares</a></li>
<li class="chapter" data-level="35.5.3" data-path="large-datasets.html"><a href="large-datasets.html#choosing-the-penalty-terms"><i class="fa fa-check"></i><b>35.5.3</b> Choosing the penalty terms</a></li>
<li class="chapter" data-level="35.5.4" data-path="large-datasets.html"><a href="large-datasets.html#exercises-57"><i class="fa fa-check"></i><b>35.5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="35.6" data-path="large-datasets.html"><a href="large-datasets.html#matrix-factorization"><i class="fa fa-check"></i><b>35.6</b> Matrix factorization</a><ul>
<li class="chapter" data-level="35.6.1" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>35.6.1</b> Factors</a></li>
<li class="chapter" data-level="35.6.2" data-path="large-datasets.html"><a href="large-datasets.html#connection-to-svd-and-pca"><i class="fa fa-check"></i><b>35.6.2</b> Connection to SVD and PCA</a></li>
<li class="chapter" data-level="35.6.3" data-path="large-datasets.html"><a href="large-datasets.html#exercises-58"><i class="fa fa-check"></i><b>35.6.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>36</b> Clustering</a><ul>
<li class="chapter" data-level="36.1" data-path="clustering.html"><a href="clustering.html#hierarchical-clustering"><i class="fa fa-check"></i><b>36.1</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="36.2" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>36.2</b> k-means</a></li>
<li class="chapter" data-level="36.3" data-path="clustering.html"><a href="clustering.html#heatmaps"><i class="fa fa-check"></i><b>36.3</b> Heatmaps</a></li>
<li class="chapter" data-level="36.4" data-path="clustering.html"><a href="clustering.html#filtering-features"><i class="fa fa-check"></i><b>36.4</b> Filtering features</a></li>
<li class="chapter" data-level="36.5" data-path="clustering.html"><a href="clustering.html#exercises-59"><i class="fa fa-check"></i><b>36.5</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Productivity tools</b></span></li>
<li class="chapter" data-level="37" data-path="introduction-to-productivity-tools.html"><a href="introduction-to-productivity-tools.html"><i class="fa fa-check"></i><b>37</b> Introduction to productivity tools</a></li>
<li class="chapter" data-level="38" data-path="github.html"><a href="github.html"><i class="fa fa-check"></i><b>38</b> GitHub</a><ul>
<li class="chapter" data-level="38.1" data-path="github.html"><a href="github.html#installing-git"><i class="fa fa-check"></i><b>38.1</b> Installing Git</a><ul>
<li class="chapter" data-level="38.1.1" data-path="github.html"><a href="github.html#installing-git-and-git-bash-on-windows"><i class="fa fa-check"></i><b>38.1.1</b> Installing Git and Git Bash on Windows</a></li>
<li class="chapter" data-level="38.1.2" data-path="github.html"><a href="github.html#installing-git-on-the-mac"><i class="fa fa-check"></i><b>38.1.2</b> Installing Git on the Mac</a></li>
</ul></li>
<li class="chapter" data-level="38.2" data-path="github.html"><a href="github.html#github-accounts"><i class="fa fa-check"></i><b>38.2</b> GitHub Accounts</a></li>
<li class="chapter" data-level="38.3" data-path="github.html"><a href="github.html#github-repositories"><i class="fa fa-check"></i><b>38.3</b> GitHub repositories</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="getting-started.html"><a href="getting-started.html#rstudio"><i class="fa fa-check"></i><b>39</b> RStudio</a><ul>
<li class="chapter" data-level="39.1" data-path="rstudio.html"><a href="rstudio.html"><i class="fa fa-check"></i><b>39.1</b> The panes</a></li>
<li class="chapter" data-level="39.2" data-path="rstudio.html"><a href="rstudio.html#key-bindings-1"><i class="fa fa-check"></i><b>39.2</b> Key bindings</a></li>
<li class="chapter" data-level="39.3" data-path="rstudio.html"><a href="rstudio.html#installing-r-packages-1"><i class="fa fa-check"></i><b>39.3</b> Installing R packages</a></li>
<li class="chapter" data-level="39.4" data-path="rstudio.html"><a href="rstudio.html#running-commands-while-editing-scripts-1"><i class="fa fa-check"></i><b>39.4</b> Running commands while editing scripts</a></li>
<li class="chapter" data-level="39.5" data-path="rstudio.html"><a href="rstudio.html#global-options"><i class="fa fa-check"></i><b>39.5</b> Global options</a></li>
<li class="chapter" data-level="39.6" data-path="rstudio.html"><a href="rstudio.html#keeping-organized-with-rstudio-projects"><i class="fa fa-check"></i><b>39.6</b> Keeping organized with RStudio projects</a></li>
<li class="chapter" data-level="39.7" data-path="rstudio.html"><a href="rstudio.html#using-git-and-github-in-rstudio"><i class="fa fa-check"></i><b>39.7</b> Using Git and GitHub in RStudio</a></li>
<li class="chapter" data-level="39.8" data-path="rstudio.html"><a href="rstudio.html#running-r-without-rstudio"><i class="fa fa-check"></i><b>39.8</b> Running R without RStudio</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="unix.html"><a href="unix.html"><i class="fa fa-check"></i><b>40</b> Organizing with Unix</a><ul>
<li class="chapter" data-level="40.1" data-path="unix.html"><a href="unix.html#the-terminal"><i class="fa fa-check"></i><b>40.1</b> The terminal</a></li>
<li class="chapter" data-level="40.2" data-path="unix.html"><a href="unix.html#filesystem"><i class="fa fa-check"></i><b>40.2</b> The filesystem</a><ul>
<li class="chapter" data-level="40.2.1" data-path="unix.html"><a href="unix.html#directories-and-subdirectories"><i class="fa fa-check"></i><b>40.2.1</b> Directories and subdirectories</a></li>
<li class="chapter" data-level="40.2.2" data-path="unix.html"><a href="unix.html#the-home-directory"><i class="fa fa-check"></i><b>40.2.2</b> The home directory</a></li>
<li class="chapter" data-level="40.2.3" data-path="unix.html"><a href="unix.html#working-directory"><i class="fa fa-check"></i><b>40.2.3</b> Working directory</a></li>
<li class="chapter" data-level="40.2.4" data-path="unix.html"><a href="unix.html#paths"><i class="fa fa-check"></i><b>40.2.4</b> Paths</a></li>
</ul></li>
<li class="chapter" data-level="40.3" data-path="unix.html"><a href="unix.html#unix-commands"><i class="fa fa-check"></i><b>40.3</b> Unix commands</a><ul>
<li class="chapter" data-level="40.3.1" data-path="unix.html"><a href="unix.html#ls-listing-directory-content"><i class="fa fa-check"></i><b>40.3.1</b> <code>ls</code>: Listing directory content</a></li>
<li class="chapter" data-level="40.3.2" data-path="unix.html"><a href="unix.html#mkdir-and-rmdir-make-and-remove-a-directory"><i class="fa fa-check"></i><b>40.3.2</b> <code>mkdir</code> and <code>rmdir</code>: make and remove a directory</a></li>
<li class="chapter" data-level="40.3.3" data-path="unix.html"><a href="unix.html#cd-navigating-the-filesystem-by-changing-directories"><i class="fa fa-check"></i><b>40.3.3</b> <code>cd</code>: Navigating the filesystem by changing directories</a></li>
</ul></li>
<li class="chapter" data-level="40.4" data-path="unix.html"><a href="unix.html#some-examples"><i class="fa fa-check"></i><b>40.4</b> Some examples</a></li>
<li class="chapter" data-level="40.5" data-path="unix.html"><a href="unix.html#more-unix-commands"><i class="fa fa-check"></i><b>40.5</b> More Unix commands</a><ul>
<li class="chapter" data-level="40.5.1" data-path="unix.html"><a href="unix.html#mv-moving-files"><i class="fa fa-check"></i><b>40.5.1</b> <code>mv</code>: moving files</a></li>
<li class="chapter" data-level="40.5.2" data-path="unix.html"><a href="unix.html#cp-copying-files"><i class="fa fa-check"></i><b>40.5.2</b> <code>cp</code>: copying files</a></li>
<li class="chapter" data-level="40.5.3" data-path="unix.html"><a href="unix.html#rm-removing-files"><i class="fa fa-check"></i><b>40.5.3</b> <code>rm</code>: removing files</a></li>
<li class="chapter" data-level="40.5.4" data-path="unix.html"><a href="unix.html#less-looking-at-a-file"><i class="fa fa-check"></i><b>40.5.4</b> <code>less</code>: looking at a file</a></li>
</ul></li>
<li class="chapter" data-level="40.6" data-path="unix.html"><a href="unix.html#preparing-for-a-data-science-project"><i class="fa fa-check"></i><b>40.6</b> Preparing for a data science project</a></li>
<li class="chapter" data-level="40.7" data-path="unix.html"><a href="unix.html#advanced-unix"><i class="fa fa-check"></i><b>40.7</b> Advanced Unix</a><ul>
<li class="chapter" data-level="40.7.1" data-path="unix.html"><a href="unix.html#arguments"><i class="fa fa-check"></i><b>40.7.1</b> Arguments</a></li>
<li class="chapter" data-level="40.7.2" data-path="unix.html"><a href="unix.html#getting-help"><i class="fa fa-check"></i><b>40.7.2</b> Getting help</a></li>
<li class="chapter" data-level="40.7.3" data-path="unix.html"><a href="unix.html#pipes"><i class="fa fa-check"></i><b>40.7.3</b> Pipes</a></li>
<li class="chapter" data-level="40.7.4" data-path="unix.html"><a href="unix.html#wild-cards"><i class="fa fa-check"></i><b>40.7.4</b> Wild cards</a></li>
<li class="chapter" data-level="40.7.5" data-path="unix.html"><a href="unix.html#environment-variables"><i class="fa fa-check"></i><b>40.7.5</b> Environment variables</a></li>
<li class="chapter" data-level="40.7.6" data-path="unix.html"><a href="unix.html#shells"><i class="fa fa-check"></i><b>40.7.6</b> Shells</a></li>
<li class="chapter" data-level="40.7.7" data-path="unix.html"><a href="unix.html#executables"><i class="fa fa-check"></i><b>40.7.7</b> Executables</a></li>
<li class="chapter" data-level="40.7.8" data-path="unix.html"><a href="unix.html#permissions-and-file-types"><i class="fa fa-check"></i><b>40.7.8</b> Permissions and file types</a></li>
<li class="chapter" data-level="40.7.9" data-path="unix.html"><a href="unix.html#commands-you-should-learn"><i class="fa fa-check"></i><b>40.7.9</b> Commands you should learn</a></li>
<li class="chapter" data-level="40.7.10" data-path="unix.html"><a href="unix.html#file-manipulation-in-r"><i class="fa fa-check"></i><b>40.7.10</b> File manipulation in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="41" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>41</b> Git</a><ul>
<li class="chapter" data-level="41.1" data-path="git.html"><a href="git.html#why-use-git-and-github"><i class="fa fa-check"></i><b>41.1</b> Why use Git and GitHub?</a></li>
<li class="chapter" data-level="41.2" data-path="git.html"><a href="git.html#overview-of-git"><i class="fa fa-check"></i><b>41.2</b> Overview of Git</a><ul>
<li class="chapter" data-level="41.2.1" data-path="git.html"><a href="git.html#clone"><i class="fa fa-check"></i><b>41.2.1</b> Clone</a></li>
</ul></li>
<li class="chapter" data-level="41.3" data-path="git.html"><a href="git.html#initilazing-a-git-directory"><i class="fa fa-check"></i><b>41.3</b> Initilazing a Git directory</a></li>
</ul></li>
<li class="chapter" data-level="42" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html"><i class="fa fa-check"></i><b>42</b> Reproducible reports with R Markdown</a><ul>
<li class="chapter" data-level="42.1" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#r-markdown"><i class="fa fa-check"></i><b>42.1</b> R Markdown</a><ul>
<li class="chapter" data-level="42.1.1" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#the-header"><i class="fa fa-check"></i><b>42.1.1</b> The header</a></li>
<li class="chapter" data-level="42.1.2" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#r-code-chunks"><i class="fa fa-check"></i><b>42.1.2</b> R code chunks</a></li>
<li class="chapter" data-level="42.1.3" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#global-options-1"><i class="fa fa-check"></i><b>42.1.3</b> Global options</a></li>
</ul></li>
<li class="chapter" data-level="42.2" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#knitr"><i class="fa fa-check"></i><b>42.2</b> knitR</a></li>
<li class="chapter" data-level="42.3" data-path="reproducible-reports-with-r-markdown.html"><a href="reproducible-reports-with-r-markdown.html#more-on-r-markdown"><i class="fa fa-check"></i><b>42.3</b> More on R markdown</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="examples-of-algorithms" class="section level1">
<h1><span class="header-section-number">Chapter 31</span> Examples of algorithms</h1>
<p>There are dozens of machine learning algorithms. Here we were provide a few examples spannig rather different approaches.</p>
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">31.1</span> Linear regression</h2>
<p>Linear regression can be considered a machine learning algorithm. As we will see, it is too rigid to be useful in general, but for some challenges it works rather well. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. To quickly make the connection between regression and machine learning, we will reformulate Galton’s study with heights: a continuous outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(HistData)

<span class="kw">set.seed</span>(<span class="dv">1983</span>)
galton_heights &lt;-<span class="st"> </span>GaltonFamilies <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(gender <span class="op">==</span><span class="st"> &quot;male&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(family) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(father, childHeight) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">son =</span> childHeight)</code></pre></div>
<p>Suppose you are tasked with building a machine learning algorithm that predicts the son’s height <span class="math inline">\(Y\)</span> using the father’s height <span class="math inline">\(X\)</span>. Let’s generate testing and training sets:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
y &lt;-<span class="st"> </span>galton_heights<span class="op">$</span>son
test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)

train_set &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)
test_set &lt;-<span class="st"> </span>galton_heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)</code></pre></div>
<p>In this case, if we were just ignoring the father’s height and guessing the son’s height, we would guess the average height of sons.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">mean</span>(train_set<span class="op">$</span>son)
m
<span class="co">#&gt; [1] 69.3</span></code></pre></div>
<p>Our squared loss is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>((m <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)
<span class="co">#&gt; [1] 7.36</span></code></pre></div>
<p>Can we do better? In the regression chapter, we learned that if the pair <span class="math inline">\((X,Y)\)</span> follow a bivariate normal distribution, the conditional expectation (what we want to estimate) is equivalent to the regression line:</p>
<p><span class="math display">\[
f(x) = \mbox{E}( Y  \mid  X= x ) = \beta_0 + \beta_1 x
\]</span></p>
<p>In Section <a href="regression.html#lse">26.8</a> we introduced least squares as a method for estimating the slope <span class="math inline">\(\beta_0\)</span> and intercept <span class="math inline">\(\beta_1\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(son <span class="op">~</span><span class="st"> </span>father, <span class="dt">data =</span> train_set)
fit<span class="op">$</span>coef
<span class="co">#&gt; (Intercept)      father </span>
<span class="co">#&gt;       51.99        0.25</span></code></pre></div>
<p>This gives us an estimate of the conditional expectation:</p>
<p><span class="math display">\[ \hat{f}(x) = 52 + 0.25 x \]</span></p>
<p>We can see that this does indeed provide an improvement over our guessing approach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>test_set<span class="op">$</span>father
<span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)
<span class="co">#&gt; [1] 5.68</span></code></pre></div>
<div id="the-predict-function" class="section level3">
<h3><span class="header-section-number">31.1.1</span> The <code>predict</code> function</h3>
<p>The <code>predict</code> function is very useful for machine learning applications. This function takes a fitted object from functions such as <code>lm</code> or <code>glm</code> (we learn about <code>glm</code> soon) and a data frame with the new predictors for which to predict. So in our current example, we would use predict like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, test_set)</code></pre></div>
<p>Using <code>predict</code>, we can get the same results as we did previously:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, test_set)
<span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>son)<span class="op">^</span><span class="dv">2</span>)
<span class="co">#&gt; [1] 5.68</span></code></pre></div>
<p>Predict does not always return objects of the same types; it depends on what type of object is sent to it. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used. The <code>predict</code> is actually a special type of function in R (called a <em>generic function</em>) that calls other functions depending on what kind of object it receives. So if <code>predict</code> receives an object coming out of the <code>lm</code> function, it will call <code>predict.lm</code>. If it receives an object coming out of <code>glm</code>, it calls <code>predict.glm</code>. These two functions are similar but different. You can learn more about the differences by reading the help files:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?predict.lm</code></pre></div>
<p>and</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?predict.glm</code></pre></div>
<p>There are many other versions of <code>predict</code> and many machine learning algorithms have one.</p>
</div>
<div id="exercises-45" class="section level3">
<h3><span class="header-section-number">31.1.2</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li><p>Create a dataset using the following code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">100</span>
Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>), <span class="dv">2</span>, <span class="dv">2</span>)
dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</code></pre></div>
<p>Use the <strong>caret</strong> package to partition into a test and training set of equal size. Train a linear model and report the RMSE. Repeat this exercise 100 times and make a histogram of the RMSEs and report the average and standard deviation. Hint: adapt the code shown earlier like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>dat<span class="op">$</span>y
test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)
train_set &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)
test_set &lt;-<span class="st"> </span>dat <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> train_set)
y_hat &lt;-<span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>test_set<span class="op">$</span>x
<span class="kw">mean</span>((y_hat <span class="op">-</span><span class="st"> </span>test_set<span class="op">$</span>y)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<p>and put inside a call to replicate.</p></li>
<li><p>Now we will repeat the above, but using a larger datasets. Repeat exercise 1 but for datasets with <code>n &lt;- c(100, 500, 1000, 5000, 10000)</code>. Save the average and standard deviation of RMSE from the 100 repetitions. Hint: use the <code>sapply</code> or <code>map</code> functions.</p></li>
<li><p>Describe what you observe with the RMSE as the size of the dataset becomes larger.</p>
<p>A. On average, the RMSE does not change much as <code>n</code> gets larger, while the variability of RMSE does decrease.</p>
<p>B. Because of the law of large numbers, the RMSE decreases: more data, more precise estimates.</p>
<p>C. <code>n = 10000</code> is not sufficiently large. To see a decrease in RMSE, we need to make it larger.</p>
<p>D. The RMSE is not a random variable.</p></li>
<li><p>Now repeat exercise 1, but this time make the correlation between <code>x</code> and <code>y</code> larger by changing <code>Sigma</code> like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">100</span>
Sigma &lt;-<span class="st"> </span><span class="dv">9</span><span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.95</span>, <span class="fl">0.95</span>, <span class="dv">1</span>), <span class="dv">2</span>, <span class="dv">2</span>)
dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">69</span>, <span class="dv">69</span>), Sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>))</code></pre></div>
<p>Repeat the exercise and note what happens to the RMSE now.</p></li>
<li><p>Which of the following best explains why the RMSE in exercise 4 is so much lower than exercise 1.</p>
<p>A. It is just luck. If we do it again, it will be larger.</p>
<p>B. The central limit theorem tell us the RMSE is normal.</p>
<p>C. When we increase the correlation between <code>x</code> and <code>y</code>, <code>x</code> has more predictive power and thus provides a better estimate of <code>y</code>. This correlation has a much bigger effect on RMSE than <code>n</code>. Large <code>n</code> simply provide us more precise estimates of the linear model coefficients.</p>
<p>D. These are both examples of regression so the RMSE has to be the same.</p></li>
<li><p>Create a dataset using the following code.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>, <span class="dv">0</span>, <span class="fl">0.75</span>, <span class="dv">0</span>, <span class="fl">1.0</span>), <span class="dv">3</span>, <span class="dv">3</span>)
dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x_1&quot;</span>, <span class="st">&quot;x_2&quot;</span>))</code></pre></div>
<p>Note that <code>y</code> is correlated with both <code>x_1</code> and <code>x_2</code>, but the two predictors are independent of each other.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(dat)</code></pre></div>
<p>Use the <strong>caret</strong> package to partition into a test and training set of equal size. Compare the RMSE when using just <code>x_1</code>, just <code>x_2</code> and both <code>x_1</code> and <code>x_2</code> Train a linear model and report the RMSE.</p></li>
<li><p>Repeat exercise 6 but now create an example in which <code>x_1</code> and <code>x_2</code> are highly correlated</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
Sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">0.75</span>, <span class="fl">1.0</span>, <span class="fl">0.95</span>, <span class="fl">0.75</span>, <span class="fl">0.95</span>, <span class="fl">1.0</span>), <span class="dv">3</span>, <span class="dv">3</span>)
dat &lt;-<span class="st"> </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">setNames</span>(<span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x_1&quot;</span>, <span class="st">&quot;x_2&quot;</span>))</code></pre></div>
<p>Use the <strong>caret</strong> package to partition into a test and training set of equal size. Compare the RMSE when using just <code>x_1</code>, just <code>x_2</code> and both <code>x_1</code> and <code>x_2</code> Train a linear model and report the RMSE.</p></li>
<li><p>Compare the results in 6 and 7 and choose the statement you agree with:</p>
<p>A. Adding extra predictors can improve RMSE substantially, but not when they are highly correlated with another predictor. B. Adding extra predictors improves predictions equally in both exercises. C. Adding extra predictors results in over fitting. D. Unless we include all predictors, we have no predicting power.</p></li>
</ol>
</div>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">31.2</span> Logistic regression</h2>
<p>The regression approach can be extended to categorical data. In this chapter we first illustrate how, for binary data, one can simply assign numeric values of 0 and 1 to the outcomes <span class="math inline">\(y\)</span>, and apply regression as if the data were continuous. We will then point out a limitation with this approach and introduce <em>logistic regression</em> as a solution. Logistic regression is a specific case of a set of <em>generalized linear models</em>. To illustrate this, we will apply it to our previous predicting sex example:</p>
<p>If we define the outcome <span class="math inline">\(Y\)</span> as 1 for females and 0 for males, and <span class="math inline">\(X\)</span> as the height, we are interested in the conditional probability:</p>
<p><span class="math display">\[
\mbox{Pr}( Y = 1 \mid X = x)
\]</span></p>
<p>As an example, let’s provide a prediction for a student that is 66 inches tall. What is the conditional probability of being female if you are 66 inches tall? In our dataset, we can estimate this by rounding to the nearest inch and computing:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">round</span>(height)<span class="op">==</span><span class="dv">66</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">y_hat =</span> <span class="kw">mean</span>(sex<span class="op">==</span><span class="st">&quot;Female&quot;</span>))
<span class="co">#&gt;   y_hat</span>
<span class="co">#&gt; 1 0.242</span></code></pre></div>
<p>To construct a prediction algorithm, we want to estimate the proportion of the population that is female for any given height <span class="math inline">\(X=x\)</span>, which we write as the conditional probability described above: <span class="math inline">\(\mbox{Pr}( Y = 1 | X=x)\)</span>. Let’s see what this looks like for several values of <span class="math inline">\(x\)</span> (we will remove strata of <span class="math inline">\(x\)</span> with few data points):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heights <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">round</span>(height)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop =</span> <span class="kw">mean</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, prop)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="book_files/figure-html/height-and-sex-conditional-probabilities-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Since the results from the plot above look close to linear, and it is the only approach we currently know, we will try regression. We assume that:</p>
<p><span class="math display">\[p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x\]</span></p>
<p>Note: because <span class="math inline">\(p_0(x) = 1 - p_1(x)\)</span>, we will only estimate <span class="math inline">\(p_1(x)\)</span> and drop the index.</p>
<p>If we convert the factors to 0s and 1s, we can we can estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> with least squares.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lm_fit &lt;-<span class="st"> </span><span class="kw">mutate</span>(train_set, <span class="dt">y =</span> <span class="kw">as.numeric</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>height, <span class="dt">data =</span> .)</code></pre></div>
<p>Once we have estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, we can obtain an actual prediction. Our estimate of the conditional probability <span class="math inline">\(p(x)\)</span> is:</p>
<p><span class="math display">\[
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
\]</span></p>
<p>To form a prediction, we define a <em>decision rule</em>: predict female if <span class="math inline">\(\hat{p}(x) &gt; 0.5\)</span>. We can compare our predictions to the outcomes using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(lm_fit, test_set)
y_hat &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">factor</span>()
<span class="kw">confusionMatrix</span>(y_hat, test_set<span class="op">$</span>sex)[[<span class="st">&quot;Accuracy&quot;</span>]]
<span class="co">#&gt; NULL</span></code></pre></div>
<p>We see this method does substantially better than guessing.</p>
<div id="generalized-linear-models" class="section level3">
<h3><span class="header-section-number">31.2.1</span> Generalized Linear Models</h3>
<p>The function <span class="math inline">\(\beta_0 + \beta_1 x\)</span> can take any value including negatives and values larger than 1. In fact, the estimate <span class="math inline">\(\hat{p}(x)\)</span> computed in the linear regression section does indeed become negative at around 76 inches.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">heights <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">round</span>(height)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">n</span>() <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">prop =</span> <span class="kw">mean</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, prop)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> lm_fit<span class="op">$</span>coef[<span class="dv">1</span>], <span class="dt">slope =</span> lm_fit<span class="op">$</span>coef[<span class="dv">2</span>])</code></pre></div>
<p><img src="book_files/figure-html/regression-prediction-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The range is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">range</span>(p_hat)
<span class="co">#&gt; [1] -0.398  1.123</span></code></pre></div>
<p>But we are estimating a probability: <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> which is constrained between 0 and 1.</p>
<p>The idea of Generalized Linear Models (GLM) is to 1) define a distribution of <span class="math inline">\(Y\)</span> that is consistent with it’s possible outcomes and 2) find a function <span class="math inline">\(g\)</span> so that <span class="math inline">\(g(\mbox{Pr}( Y = 1 \mid X = x))\)</span> can be modeled as a linear combination of predictors. Logistic regression is the most commonly used GLM. It is an extension of linear regression that assures that the estimate of <span class="math inline">\(\mbox{Pr}( Y = 1 \mid X = x)\)</span> is between 0 and 1. This approach makes use of the <em>logistics</em> transformation introduced in Section <a href="gapminder.html#logit">13.9.1</a>.</p>
<p><span class="math display">\[ g(p) = \log \frac{p}{1-p}\]</span></p>
<p>This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely something will happen compared to not happening. So <span class="math inline">\(p=0.5\)</span> means the odds are 1 to 1, thus the odds are 1. If <span class="math inline">\(p=0.75\)</span>, the odds are 3 to 1. A nice characteristic of this transformation is that it converts probabilities to be symmetric around 0. Here is a plot of <span class="math inline">\(g(p)\)</span> versus <span class="math inline">\(p\)</span>:</p>
<p><img src="book_files/figure-html/p-versus-logistic-of-p-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>With <em>logistic regression</em>, we model the conditional probability directly with:</p>
<p><span class="math display">\[ 
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
\]</span></p>
<p>With this model, we can no longer use least squares. Instead we compute the <em>maximum likelihood estimate</em> (MLE). You can learn more about this concept in a <a href="http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428">statistical theory text</a>.</p>
<p>In R, we can fit the logistic regression model with the function <code>glm</code>: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the <code>family</code> parameter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_fit &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">as.numeric</span>(sex <span class="op">==</span><span class="st"> &quot;Female&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>height, <span class="dt">data=</span>., <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>We can obtain prediction using the predict function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_hat_logit &lt;-<span class="st"> </span><span class="kw">predict</span>(glm_fit, <span class="dt">newdata =</span> test_set, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)</code></pre></div>
<p>When using <code>predict</code> with a <code>glm</code> object, we have to specify that we want <code>type=&quot;response&quot;</code> if we want the conditional probabilities since the default is to return the logistic transformed values.</p>
<p>This model fits the data slightly better than the line:</p>
<p><img src="book_files/figure-html/conditional-prob-glm-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Because we have an estimate <span class="math inline">\(\hat{p}(x)\)</span>, we can obtain predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat_logit &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_logit <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>factor
<span class="kw">confusionMatrix</span>(y_hat_logit, test_set<span class="op">$</span>sex)[[<span class="st">&quot;Accuracy&quot;</span>]]
<span class="co">#&gt; NULL</span></code></pre></div>
<p>The resulting predictions are similar. This is because the two estimates of <span class="math inline">\(p(x)\)</span> are larger than 1/2 in about the same region of x:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">seq</span>(<span class="kw">min</span>(tmp<span class="op">$</span>x), <span class="kw">max</span>(tmp<span class="op">$</span>x))) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">logistic =</span> <span class="kw">plogis</span>(glm_fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>glm_fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>x),
         <span class="dt">regression =</span> lm_fit<span class="op">$</span>coef[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>lm_fit<span class="op">$</span>coef[<span class="dv">2</span>]<span class="op">*</span>x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">gather</span>(method, p_x, <span class="op">-</span>x) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, p_x, <span class="dt">color =</span> method)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="fl">0.5</span>, <span class="dt">lty =</span> <span class="dv">5</span>)</code></pre></div>
<p><img src="book_files/figure-html/glm-prediction-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Both linear and logistic regressions provide an estimate for the conditional expectation:</p>
<p><span class="math display">\[
\mbox{E}(Y \mid X=x)
\]</span> which in the case of binary data is equivalent to the conditional probability:</p>
<p><span class="math display">\[
\mbox{Pr}(Y = 1 \mid X = x)
\]</span></p>
<p>Once we move on to more complex examples, we will see that linear regression and generalized linear regression are limited and not flexible enough to be useful for most machine learning challenges. The techniques we learn are essentially approaches to estimating the conditional probability in a way that is more flexible.</p>
</div>
<div id="exercises-46" class="section level3">
<h3><span class="header-section-number">31.2.2</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li><p>Define the following dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">make_data &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">n =</span> <span class="dv">1000</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, 
                      <span class="dt">mu_0 =</span> <span class="dv">0</span>, <span class="dt">mu_1 =</span> <span class="dv">2</span>, 
                      <span class="dt">sigma_0 =</span> <span class="dv">1</span>,  <span class="dt">sigma_1 =</span> <span class="dv">1</span>){

  y &lt;-<span class="st"> </span><span class="kw">rbinom</span>(n, <span class="dv">1</span>, p)
  f_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, mu_<span class="dv">0</span>, sigma_<span class="dv">0</span>)
  f_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, mu_<span class="dv">1</span>, sigma_<span class="dv">1</span>)
  x &lt;-<span class="st"> </span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, f_<span class="dv">1</span>, f_<span class="dv">0</span>)

  test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)

  <span class="kw">list</span>(<span class="dt">train =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">as.factor</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index),
       <span class="dt">test =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> <span class="kw">as.factor</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index))
}
dat &lt;-<span class="st"> </span><span class="kw">make_data</span>()</code></pre></div>
<p>Note that we have defined a variable <code>x</code> that is predictive of a binary outcome <code>y</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat<span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_density</span>()</code></pre></div>
<p>Compare the accuracy of linear regression and logistic regression.</p></li>
<li><p>Repeat the simulation from exercise 1 100 times and compare the average accuracy for each method and notice they give practically the same answer.</p></li>
<li><p>Generate 25 different datasets changing the difference between the two class: <code>delta &lt;- seq(0, 3, len = 25)</code>. Plot accuracy versus <code>delta</code>.</p></li>
</ol>

</div>
</div>
<div id="k-nearest-neighbors" class="section level2">
<h2><span class="header-section-number">31.3</span> K-nearest neighbors</h2>
<p>Let’s get back to our digits data with the two predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)
mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color =</span> y)) <span class="op">+</span><span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="book_files/figure-html/mnist-27-data-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>To see how this relates to smoothing, we can think of how we would estimate the conditional probability:</p>
<p><span class="math display">\[
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).
\]</span></p>
<p>The 0s and 1s we observe are “noisy” because for some regions the probabilities <span class="math inline">\(p(x_1, x_2)\)</span> are not that close to 0 or 1. So we need to estimate <span class="math inline">\(p(x_1, x_2)\)</span>. How do we do this? We can try smoothing.</p>
<p>K-nearest neighbors (kNN) is similar to bin smoothing, but it is easier to adapt to multiple dimensions. We first define the distance between all observations based on the features. Basically, for any point <span class="math inline">\((x_1,x_2)\)</span> for which we want an estimate of <span class="math inline">\(p(x_1, x_2)\)</span>, we look for the <span class="math inline">\(k\)</span> nearest points and then take an average of these 0s and 1s associated with these points. We refer to the set of points used to compute the average and the <em>neighborhood</em>. Due to the connection we described earlier between conditional expectations and conditional probabilities, this gives us an <span class="math inline">\(\hat{p}(x_1,x_2)\)</span>, just like the bin smoother gave us an estimate of a trend.</p>
<p>We can now control the flexibility of our estimate through <span class="math inline">\(k\)</span>: larger <span class="math inline">\(k\)</span>s result in smoother estimates, while smaller <span class="math inline">\(k\)</span>s result in more flexible and more wiggly estimates.</p>
<p>As an example, let’s use logistic regression as the standard we need to beat.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
fit_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x_<span class="dv">2</span>, <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
p_hat_logistic &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_glm, mnist_<span class="dv">27</span><span class="op">$</span>test)
y_hat_logistic &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">ifelse</span>(p_hat_logistic <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">2</span>))
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat_logistic, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.76</span></code></pre></div>
<p>Now compare to kNN. We will use the <code>knn3</code> function from the <strong>caret</strong> package. Looking at the help file of this package, we can see that we can call it in one of two ways. In the first, we specify a <em>formula</em> and a data frame. The data frame contains all the data to be used. The formula has the from <code>outcome ~ predictor_1 + predictor_2 + predictor_3</code> and so on. Therefore, we would type <code>y ~ x_1 + x_2</code>. If we are going to use all the predictors, we can use the <code>.</code> like this <code>y ~ .</code>. The final call looks like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</code></pre></div>
<p>The second way to call this function is with the first argument being the matrix of predictors and the second a vector of outcomes. So the code would look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(mnist_<span class="dv">27</span><span class="op">$</span>train[,<span class="dv">2</span><span class="op">:</span><span class="dv">3</span>])
y &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train<span class="op">$</span>y
knn_fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(x, y)</code></pre></div>
<p>For this function, we also need to pick a parameter: the number of neighbors to include. Let’s start with the default <span class="math inline">\(k=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">k =</span> <span class="dv">5</span>)</code></pre></div>
<p>In this case, since our dataset is balanced and we care just as much about sensitivity as we do about specificity, we will use accuracy to quantify performance.</p>
<p>The <code>predict</code> function for <code>knn</code> produces a probability for each class. So we keep the probability of being a 7 as the estimate <span class="math inline">\(\hat{p}(x_1, x_2)\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat_knn &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat_knn, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.815</span></code></pre></div>
<p>This already beats the logistics model. To see why this is case, we will plot <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> and compare it to the the true conditional probability <span class="math inline">\(p(x_1, x_2)\)</span>:</p>
<p><img src="book_files/figure-html/knn-fit-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In the estimate, we see some islands of blue in the red area. Intuitively, this does not make much sense. This is due to what we call <em>over training</em>. We describe over-training in detail below, but note that we have higher accuracy in the train set compared to the test set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat_knn &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit, mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat_knn, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>train<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.882</span>

y_hat_knn &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat_knn, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.815</span></code></pre></div>
<div id="over-training" class="section level3">
<h3><span class="header-section-number">31.3.1</span> Over training</h3>
<p>Over-training is at its worst when we set <span class="math inline">\(k=1\)</span>. With <span class="math inline">\(k=1\)</span> the estimate for each <span class="math inline">\((x_1, x_2)\)</span> in the training set is obtained with just the <span class="math inline">\(y\)</span> corresponding to that point. So, in this case, if the <span class="math inline">\((x_1, x_2)\)</span> are unique, we will obtain perfect accuracy in the training set because each point is used to predict itself. Remember that if the predictors are not unique and have different outcomes for at least one set of predictors then it is impossible to predict perfectly.</p>
<p>Here we fit a kNN model with <span class="math inline">\(k=1\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_fit_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">knn3</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">k =</span> <span class="dv">1</span>)
y_hat_knn_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit_<span class="dv">1</span>, mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data=</span>y_hat_knn_<span class="dv">1</span>, <span class="dt">reference=</span>mnist_<span class="dv">27</span><span class="op">$</span>train<span class="op">$</span>y)<span class="op">$</span>overall[[<span class="st">&quot;Accuracy&quot;</span>]]
<span class="co">#&gt; [1] 0.995</span></code></pre></div>
<p>However, the test set accuracy is actually worse than logistics regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat_knn_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit_<span class="dv">1</span>, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data=</span>y_hat_knn_<span class="dv">1</span>, <span class="dt">reference=</span>mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.745</span></code></pre></div>
<p>We can see the over-fitting problem in this figure. <img src="book_files/figure-html/knn-1-overfit-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The black curves denote the decision rule boundaries.</p>
<p>The estimate <span class="math inline">\(\hat{p}(x_1, x_2)\)</span> follows the training data too closely (left). You can see that in the training set, boundaries have been drawn to perfectly surround a single red point in a sea of blue. Because most points <span class="math inline">\((x_1, x_2)\)</span> are unique, the prediction is either 1 or 0 and the prediction for that point is the associated label. However, once we introduce the training set (right), we see that many of these small islands now have the opposite color and we end up making several incorrect predictions.</p>
</div>
<div id="over-smoothing" class="section level3">
<h3><span class="header-section-number">31.3.2</span> Over-smoothing</h3>
<p>Although not as badly as with the previous examples, we saw that with <span class="math inline">\(k=5\)</span> we also over-trained. Hence, we should consider a larger <span class="math inline">\(k\)</span>. Let’s try, as an example, a much larger number: <span class="math inline">\(k=401\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn_fit_<span class="dv">401</span> &lt;-<span class="st"> </span><span class="kw">knn3</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">k =</span> <span class="dv">401</span>)
y_hat_knn_<span class="dv">401</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_fit_<span class="dv">401</span>, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">confusionMatrix</span>(<span class="dt">data=</span>y_hat_knn_<span class="dv">401</span>, <span class="dt">reference=</span>mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.79</span></code></pre></div>
<p>This turns out to be similar to logistic regression: <img src="book_files/figure-html/mnist-27-glm-est-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>This size of <span class="math inline">\(k\)</span> is so large that it does not permit enough flexibility. We call this <em>over smoothing</em>.</p>
</div>
<div id="picking-the-k-in-knn" class="section level3">
<h3><span class="header-section-number">31.3.3</span> Picking the <span class="math inline">\(k\)</span> in kNN</h3>
<p>So how do we pick <span class="math inline">\(k\)</span>?</p>
<p>Let’s repeat what we did above but for different values of <span class="math inline">\(k\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ks &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">3</span>, <span class="dv">251</span>, <span class="dv">2</span>)</code></pre></div>
<p>Now we use the <code>map_df</code> function to repeat the above for each one. For comparative purposes, we will compute the accuracy by using both the training set (incorrect) and the test set (correct):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(purrr)
accuracy &lt;-<span class="st"> </span><span class="kw">map_df</span>(ks, <span class="cf">function</span>(k){
  fit &lt;-<span class="st"> </span><span class="kw">knn3</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">k =</span> k)
  
  y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, mnist_<span class="dv">27</span><span class="op">$</span>train, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
  cm_train &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>train<span class="op">$</span>y)
  train_error &lt;-<span class="st"> </span>cm_train<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
  
  y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, mnist_<span class="dv">27</span><span class="op">$</span>test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)
  cm_test &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)
  test_error &lt;-<span class="st"> </span>cm_test<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
  
  <span class="kw">tibble</span>(<span class="dt">train =</span> train_error, <span class="dt">test =</span> test_error)
})</code></pre></div>
<p>We can now plot the accuracy against the value of <span class="math inline">\(k\)</span>:</p>
<p><img src="book_files/figure-html/accuracy-vs-k-knn-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>First, note that the accuracy versus <span class="math inline">\(k\)</span> plot is quite jagged. We do not expect this because small changes in <span class="math inline">\(k\)</span> should not affect the algorithm’s performance too much. The jaggedness is explained by the fact that the accuracy is computed on a sample and therefore is a random variable. This demonstrates why we prefer to minimize the expectation loss rather than the loss we observe with one dataset. We will soon learn a better way of estimating this expected loss. Despite the noise present in the plot above, we still see a general pattern. Low values of <span class="math inline">\(k\)</span> give low test set accuracy but high train set accuracy, which is evidence of over-training. Large values of <span class="math inline">\(k\)</span> result in low accuracy, which is evidence of over-smoothing. The maximum is achieved somewhere between 25 and 41 with a maximum accuracy of 0.85. In fact, the resulting estimate with <span class="math inline">\(k=41\)</span> looks quite similar to the true conditional probability:</p>
<p><img src="book_files/figure-html/knn-41-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The final accuracy for this value of <span class="math inline">\(k\)</span> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">max</span>(accuracy<span class="op">$</span>test)
<span class="co">#&gt; [1] 0.86</span></code></pre></div>
<p><strong>So is this what we should expect if we apply this algorithm in the real world? The answer is “no” because we broke a golden rule of machine learning: we selected the <span class="math inline">\(k\)</span> using the test set. </strong> So how do we select the <span class="math inline">\(k\)</span> in the real world? In the next section, we introduce the important concept of cross validation which provides a way to estimate the expected loss for any given method using only the training set.</p>
</div>
<div id="exercises-47" class="section level3">
<h3><span class="header-section-number">31.3.4</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li><p>Earlier, we used logistic regression to predict sex from height. Use kNN to do the same. Use the code described in this chapter to select the <span class="math inline">\(F_1\)</span> measure and plot it against <span class="math inline">\(k\)</span>. Compare to the <span class="math inline">\(F_1\)</span> of about 0.6 we obtained with regression.</p></li>
<li><p>Here we will use the same gene expression example used in the Distance Chapter exercises. You can load it like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)</code></pre></div>
<p>Split the data in training and test sets, then see what accuracy you obtain. Try it for <span class="math inline">\(k = 1, 3, \dots, 11\)</span>.</p></li>
</ol>

</div>
</div>
<div id="generative-models" class="section level2">
<h2><span class="header-section-number">31.4</span> Generative models</h2>
<p>We have described how, when using squared loss, the conditional expectation/probabilities provide the best approach to developing a decision rule. In a binary case, the smallest true error we can achieve is determined by Bayes’ rule, which is a decision rule based on the true conditional probability:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1 \mid \mathbf{X}=\mathbf{x}) 
\]</span></p>
<p>We have described several approaches to estimating <span class="math inline">\(p(\mathbf{x})\)</span>. In all these approaches, we estimate the conditional probability directly and do not consider the distribution of the predictors. In machine learning, these are referred to as <em>discriminative</em> approaches.</p>
<p>However, Bayes’ theorem tells us that knowing the distribution of the predictors <span class="math inline">\(\mathbf{X}\)</span> may be useful. Methods that model the joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\mathbf{X}\)</span> are referred to as <em>generative models</em> (we model how the entire data, <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span>, are generated). We start by describing the most general generative model, Naive Bayes, and then proceed to describe two specific cases, quadratic discriminant analysis (QDA) and linear discriminant analysis (LDA).</p>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">31.4.1</span> Naive Bayes</h3>
<p>Recall that Bayes rule tells use that we can rewrite <span class="math inline">\(p(\mathbf{x})\)</span> like this:</p>
<p><span class="math display">\[
p(\mathbf{x}) = \mbox{Pr}(Y=1|\mathbf{X}=\mathbf{x}) = \frac{f_{\mathbf{X}|Y=1}(\mathbf{x}) \mbox{Pr}(Y=1)}
{ f_{\mathbf{X}|Y=0}(\mathbf{x})\mbox{Pr}(Y=0)  + f_{\mathbf{X}|Y=1}(\mathbf{x})\mbox{Pr}(Y=1) }
\]</span></p>
<p>with <span class="math inline">\(f_{\mathbf{X}|Y=1}\)</span> and <span class="math inline">\(f_{\mathbf{X}|Y=0}\)</span> representing the distribution functions of the predictor <span class="math inline">\(\mathbf{X}\)</span> for the two classes <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. The formula implies that if we can estimate these conditional distributions of the predictors, we can develop a powerful decision rule. However, this is a big <em>if</em>. As we go forward, we will encounter examples in which <span class="math inline">\(\mathbf{X}\)</span> has many dimensions and we do not have much information about the distribution. In these cases, Naive Bayes will be practically impossible to implement. However, there are instances in which we have a small number of predictors (not much more than 2) and many categories in which generative models can be quite powerful. We describe two specific examples and use our previously described case studies to illustrate them.</p>
<p>Let’s start with a very simple and uninteresting, yet illustrative, case: the example related to predicting sex from height.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(caret)

<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;heights&quot;</span>)

y &lt;-<span class="st"> </span>heights<span class="op">$</span>height
<span class="kw">set.seed</span>(<span class="dv">1995</span>)
test_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">times =</span> <span class="dv">1</span>, <span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)
train_set &lt;-<span class="st"> </span>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="op">-</span>test_index)
test_set &lt;-<span class="st"> </span>heights <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(test_index)</code></pre></div>
<p>In this case, the Naive Bayes approach is particularly appropriate because we know that the normal distribution is a good approximation for the conditional distributions of height given sex for both classes <span class="math inline">\(Y=1\)</span> (female) and <span class="math inline">\(Y=0\)</span> (Male). This implies that we can approximate the conditional distributions <span class="math inline">\(f_{X|Y=1}\)</span> and <span class="math inline">\(f_{X|Y=0}\)</span> by simply estimating averages and standard deviations from the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(sex) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg =</span> <span class="kw">mean</span>(height), <span class="dt">sd =</span> <span class="kw">sd</span>(height))
params
<span class="co">#&gt; # A tibble: 2 x 3</span>
<span class="co">#&gt;   sex      avg    sd</span>
<span class="co">#&gt;   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1 Female  65.1  3.43</span>
<span class="co">#&gt; 2 Male    69.2  3.55</span></code></pre></div>
<p>The prevalence, which we will denote with <span class="math inline">\(\pi = \mbox{Pr}(Y=1)\)</span>, can be estimated from the data with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pi &lt;-<span class="st"> </span>train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize</span>(<span class="dt">pi=</span><span class="kw">mean</span>(sex<span class="op">==</span><span class="st">&quot;Female&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(pi)
pi
<span class="co">#&gt; [1] 0.214</span></code></pre></div>
<p>Now we can use our estimates of average and standard deviation to get an actual rule:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>test_set<span class="op">$</span>height

f0 &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, params<span class="op">$</span>avg[<span class="dv">2</span>], params<span class="op">$</span>sd[<span class="dv">2</span>])
f1 &lt;-<span class="st"> </span><span class="kw">dnorm</span>(x, params<span class="op">$</span>avg[<span class="dv">1</span>], params<span class="op">$</span>sd[<span class="dv">1</span>])

p_hat_bayes &lt;-<span class="st"> </span>f1<span class="op">*</span>pi <span class="op">/</span><span class="st"> </span>(f1<span class="op">*</span>pi <span class="op">+</span><span class="st"> </span>f0<span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>pi))</code></pre></div>
<p>Our Naive Bayes estimate <span class="math inline">\(\hat{p}(x)\)</span> looks a lot like our logistic regression estimate:</p>
<p><img src="book_files/figure-html/conditional-prob-glm-fit-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In fact, we can show that the Naive Bayes approach is similar to the logistic regression prediction mathematically. However, we leave the demonstration to a more advanced text, such as <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">this one</a>. We can see that they are similar empirically by comparing the two resulting curves.</p>
</div>
<div id="controlling-prevalence" class="section level3">
<h3><span class="header-section-number">31.4.2</span> Controlling prevalence</h3>
<p>One useful feature of the Naive Bayes approach is that it includes a parameter to account for differences in prevalence. Using our sample, we estimated <span class="math inline">\(f_{X|Y=1}\)</span>, <span class="math inline">\(f_{X|Y=0}\)</span> and <span class="math inline">\(\pi\)</span>. If we use hats to denote the estimates, we can write <span class="math inline">\(\hat{p}(x)\)</span> as:</p>
<p><span class="math display">\[
\hat{p}(x)= \frac{\hat{f}_{X|Y=1}(x) \hat{\pi}}
{ \hat{f}_{X|Y=0}(x)(1-\hat{\pi}) + \hat{f}_{X|Y=1}(x)\hat{\pi} }
\]</span></p>
<p>As we discussed earlier, our sample has a much lower prevalence, 0.21, than the general population. So if we use the rule <span class="math inline">\(\hat{p}(x)&gt;0.5\)</span> to predict females, our accuracy will be affected due to the low sensitivity:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat_bayes &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_bayes <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>)
<span class="kw">sensitivity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))
<span class="co">#&gt; [1] 0.349</span></code></pre></div>
<p>Again, this is because the algorithm gives more weight to specificity to account for the low prevalence:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">specificity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))
<span class="co">#&gt; [1] 0.945</span></code></pre></div>
<p>This is due mainly to the fact that <span class="math inline">\(\hat{\pi}\)</span> is substantially less than 0.5, so we tend to predict <code>Male</code> more often. It makes sense for a machine learning algorithm to do this in our sample because we do have a higher percentage of males. But if we were to extrapolate this to a general population, our overall accuracy would be affected by the low sensitivity.</p>
<p>The Naive Bayes approach gives us a direct way to correct this since we can simply force <span class="math inline">\(\hat{\pi}\)</span> to be whatever value we want it to be. So to balance specificity and sensitivity, instead of changing the cutoff in the decision rule, we could simply change <span class="math inline">\(\hat{\pi}\)</span> to 0.5 like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_hat_bayes_unbiased &lt;-<span class="st"> </span>f1 <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">/</span><span class="st"> </span>(f1 <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">+</span><span class="st"> </span>f0 <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span>)) 
y_hat_bayes_unbiased &lt;-<span class="st"> </span><span class="kw">ifelse</span>(p_hat_bayes_unbiased<span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="st">&quot;Female&quot;</span>, <span class="st">&quot;Male&quot;</span>)</code></pre></div>
<p>Note the difference in sensitivity with a better balance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sensitivity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes_unbiased), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))
<span class="co">#&gt; [1] 0.778</span>
<span class="kw">specificity</span>(<span class="dt">data =</span> <span class="kw">factor</span>(y_hat_bayes_unbiased), <span class="dt">reference =</span> <span class="kw">factor</span>(test_set<span class="op">$</span>sex))
<span class="co">#&gt; [1] 0.743</span></code></pre></div>
<p>The new rule also gives us a very intuitive cutoff between 66-67, which is about the middle of the female and male average heights:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, p_hat_bayes_unbiased, <span class="dt">geom =</span> <span class="st">&quot;line&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="fl">0.5</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="dv">67</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="book_files/figure-html/naive-with-good-prevalence-1.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="quadratic-discriminant-analysis" class="section level3">
<h3><span class="header-section-number">31.4.3</span> Quadratic Discriminant Analysis</h3>
<p>Quadratic Discriminant Analysis (QDA) is a version of <em>Naive Bayes</em> in which we assume that the distributions <span class="math inline">\(p_{\mathbf{X}|Y=1}(x)\)</span> and <span class="math inline">\(p_{\mathbf{X}|Y=0}(\mathbf{x})\)</span> are multivariate normal. The simple example we described in the previous section is actually QDA. Let’s now look at a slightly more complicated case: the 2 or 7 example.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</code></pre></div>
<p>In this case, we have two predictors so we assume each one is bivariate normal. This implies that we need to estimate two averages, two standard deviations, and a correlation for each case <span class="math inline">\(Y=1\)</span> and <span class="math inline">\(Y=0\)</span>. Once we have these, we can approximate the distributions <span class="math inline">\(f_{X_1,X_2|Y=1}\)</span> and <span class="math inline">\(f_{X_1, X_2|Y=0}\)</span>. We can easily estimate parameters from the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg_1 =</span> <span class="kw">mean</span>(x_<span class="dv">1</span>), <span class="dt">avg_2 =</span> <span class="kw">mean</span>(x_<span class="dv">2</span>), 
            <span class="dt">sd_1=</span> <span class="kw">sd</span>(x_<span class="dv">1</span>), <span class="dt">sd_2 =</span> <span class="kw">sd</span>(x_<span class="dv">2</span>), 
            <span class="dt">r =</span> <span class="kw">cor</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>))
params
<span class="co">#&gt; # A tibble: 2 x 6</span>
<span class="co">#&gt;   y     avg_1 avg_2   sd_1   sd_2     r</span>
<span class="co">#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1 2     0.129 0.283 0.0702 0.0578 0.401</span>
<span class="co">#&gt; 2 7     0.234 0.288 0.0719 0.105  0.455</span></code></pre></div>
<p>Here we provide a visual way of showing the approach. We plot the data and use contour plots to give an idea of what the two estimated normal densities look like (we show the curve representing a region that includes 95% of the points):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>, <span class="dt">lwd =</span> <span class="fl">1.5</span>)</code></pre></div>
<p><img src="book_files/figure-html/qda-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This defines the following estimate of <span class="math inline">\(f(x_1, x_2)\)</span>.</p>
<p>We can use the <code>train</code> function from the <strong>caret</strong> package, described in detail in Section <a href="caret.html#caret">33</a>, to fit the model and obtain predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
train_qda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>, <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)</code></pre></div>
<p>We see that we obtain relatively good accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_qda, mnist_<span class="dv">27</span><span class="op">$</span>test)
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.82</span></code></pre></div>
<p>The estimated conditional probability looks relatively good, although it does not fit as well as the kernel smoothers:</p>
<p><img src="book_files/figure-html/qda-estimate-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>One reason QDA does not work as well as the kernel methods is perhaps because the assumption of normality does not quite hold. Although for the 2s it seems reasonable, for the 7s it does seem to be off. Notice the slight curvature in the points for the 7s:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>y)</code></pre></div>
<p><img src="book_files/figure-html/qda-does-not-fit-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>QDA can work well here, but it becomes harder to use as the number of predictors increases. Here we have 2 predictors and had to compute 4 means, 4 SDs and 2 correlations. How many parameters would we have if instead of 2 predictors, we had 10? The main problem comes from estimating correlations for 10 of predictors. With 10, we have 45 correlations for each class. In general, the formula is <span class="math inline">\(K\times p(p-1)/2\)</span> which gets big fast. Once the number of parameters approaches the size of our data, the method becomes unpractical due to overfitting.</p>
</div>
<div id="linear-discriminant-analysis" class="section level3">
<h3><span class="header-section-number">31.4.4</span> Linear discriminant analysis</h3>
<p>A relatively simple solution to the problem of having too many parameters is to assume that the correlation structure is the same for all classes, which reduces the number of parameters we need to estimate.</p>
<p>In this case, we would compute just one pair of standard deviations and one correlation, so the parameters would look something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params &lt;-<span class="st"> </span>mnist_<span class="dv">27</span><span class="op">$</span>train <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">avg_1 =</span> <span class="kw">mean</span>(x_<span class="dv">1</span>), <span class="dt">avg_2 =</span> <span class="kw">mean</span>(x_<span class="dv">2</span>), <span class="dt">sd_1=</span> <span class="kw">sd</span>(x_<span class="dv">1</span>), <span class="dt">sd_2 =</span> <span class="kw">sd</span>(x_<span class="dv">2</span>), <span class="dt">r =</span> <span class="kw">cor</span>(x_<span class="dv">1</span>,x_<span class="dv">2</span>))

params &lt;-params <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">sd_1 =</span> <span class="kw">mean</span>(sd_<span class="dv">1</span>), <span class="dt">sd_2=</span><span class="kw">mean</span>(sd_<span class="dv">2</span>), <span class="dt">r=</span><span class="kw">mean</span>(r))
params 
<span class="co">#&gt; # A tibble: 2 x 6</span>
<span class="co">#&gt;   y     avg_1 avg_2   sd_1   sd_2     r</span>
<span class="co">#&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;</span>
<span class="co">#&gt; 1 2     0.129 0.283 0.0710 0.0813 0.428</span>
<span class="co">#&gt; 2 7     0.234 0.288 0.0710 0.0813 0.428</span></code></pre></div>
<p>and the distributions like this:</p>
<p><img src="book_files/figure-html/lda-explained-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Now the size of the ellipses as well as the angle are the same. This is because they have the same standard deviations and correlations. When we force this assumption, we can show mathematically that the boundary is a line, just as with logistic regression. For this reason, we call the method <em>linear</em> discriminant analysis (LDA). Similarly, for QDA, we can show that the boundary must be a quadratic function.</p>
<p><img src="book_files/figure-html/lda-estimate-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>In the case of LDA, the lack of flexibility does not permit us to capture the non-linearity in the true conditional probability function.</p>
<p>Note we can fit the LDA model using caret:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_lda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,
                   <span class="dt">method =</span> <span class="st">&quot;lda&quot;</span>,
                   <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)
y_hat &lt;-<span class="st"> </span><span class="kw">predict</span>(train_lda, mnist_<span class="dv">27</span><span class="op">$</span>test)
<span class="kw">confusionMatrix</span>(<span class="dt">data =</span> y_hat, <span class="dt">reference =</span> mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.75</span></code></pre></div>
</div>
<div id="connection-to-distance" class="section level3">
<h3><span class="header-section-number">31.4.5</span> Connection to distance</h3>
<p>The normal density is:</p>
<p><span class="math display">\[
p(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp\left\{ - \frac{(x-\mu)^2}{\sigma^2}\right\}
\]</span></p>
<p>If we remove the constant <span class="math inline">\(1/(\sqrt{2\pi} \sigma)\)</span> and then take the log, we get:</p>
<p><span class="math display">\[
- \frac{(x-\mu)^2}{\sigma^2}
\]</span></p>
<p>which is the negative of a distance squared scaled by the standard deviation. For higher dimensions, the same is true except the scaling is more complex and involves correlations.</p>
</div>
<div id="case-study-more-than-three-classes" class="section level3">
<h3><span class="header-section-number">31.4.6</span> Case study: more than three classes</h3>
<p>We will briefly give a slightly more complex example: one with 3 classes instead of 2. We first create a dataset similar to the 2 or 7 dataset, except now we have 1s, 2s and 7s.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="op">!</span><span class="kw">exists</span>(<span class="st">&quot;mnist&quot;</span>)) mnist &lt;-<span class="st"> </span><span class="kw">read_mnist</span>()

<span class="kw">set.seed</span>(<span class="dv">3456</span>)
index_<span class="dv">127</span> &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">which</span>(mnist<span class="op">$</span>train<span class="op">$</span>labels <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">7</span>)), <span class="dv">2000</span>)
y &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>labels[index_<span class="dv">127</span>] 
x &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span>images[index_<span class="dv">127</span>,]
index_train &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(y, <span class="dt">p=</span><span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)

<span class="co"># get the quandrants</span>
<span class="co">#temporary object to help figure out the quandrants</span>
row_column &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">row=</span><span class="dv">1</span><span class="op">:</span><span class="dv">28</span>, <span class="dt">col=</span><span class="dv">1</span><span class="op">:</span><span class="dv">28</span>) 
upper_left_ind &lt;-<span class="st"> </span><span class="kw">which</span>(row_column<span class="op">$</span>col <span class="op">&lt;=</span><span class="st"> </span><span class="dv">14</span> <span class="op">&amp;</span><span class="st"> </span>row_column<span class="op">$</span>row <span class="op">&lt;=</span><span class="st"> </span><span class="dv">14</span>)
lower_right_ind &lt;-<span class="st"> </span><span class="kw">which</span>(row_column<span class="op">$</span>col <span class="op">&gt;</span><span class="st"> </span><span class="dv">14</span> <span class="op">&amp;</span><span class="st"> </span>row_column<span class="op">$</span>row <span class="op">&gt;</span><span class="st"> </span><span class="dv">14</span>)

<span class="co">#binarize the values. Above 200 is ink, below is no ink</span>
x &lt;-<span class="st"> </span>x <span class="op">&gt;</span><span class="st"> </span><span class="dv">200</span> 

<span class="co">#cbind proportion of pixels in upper right quandrant and</span>
##proportion of pixes in lower rigth quandrant
x &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rowSums</span>(x[ ,upper_left_ind])<span class="op">/</span><span class="kw">rowSums</span>(x), 
           <span class="kw">rowSums</span>(x[ ,lower_right_ind])<span class="op">/</span><span class="kw">rowSums</span>(x)) 

train_set &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y[index_train]),
                        <span class="dt">x_1 =</span> x[index_train,<span class="dv">1</span>],
                        <span class="dt">x_2 =</span> x[index_train,<span class="dv">2</span>])
test_set &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y[<span class="op">-</span>index_train]),
                       <span class="dt">x_1 =</span> x[<span class="op">-</span>index_train,<span class="dv">1</span>],
                       <span class="dt">x_2 =</span> x[<span class="op">-</span>index_train,<span class="dv">2</span>])</code></pre></div>
<p>Here is the training data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color=</span>y)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="book_files/figure-html/mnist-27-trainig-data-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We use the <strong>caret</strong> package to train the QDA model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_qda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>, <span class="dt">data =</span> train_set)</code></pre></div>
<p>Now we estimate three conditional probabilities (although they have to add to 1):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(train_qda, test_set, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()
<span class="co">#&gt;        1     2      7</span>
<span class="co">#&gt; 1 0.2223 0.660 0.1180</span>
<span class="co">#&gt; 2 0.1926 0.454 0.3539</span>
<span class="co">#&gt; 3 0.6275 0.322 0.0505</span>
<span class="co">#&gt; 4 0.0462 0.101 0.8529</span>
<span class="co">#&gt; 5 0.2167 0.623 0.1604</span>
<span class="co">#&gt; 6 0.1267 0.335 0.5383</span></code></pre></div>
<p>And our predictions are one of the three classes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(train_qda, test_set) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>()
<span class="co">#&gt; [1] 2 2 1 7 2 7</span>
<span class="co">#&gt; Levels: 1 2 7</span></code></pre></div>
<p>So the confusion matrix has a 3 by 3 table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_qda, test_set), test_set<span class="op">$</span>y)<span class="op">$</span>table
<span class="co">#&gt;           Reference</span>
<span class="co">#&gt; Prediction   1   2   7</span>
<span class="co">#&gt;          1 111  17   7</span>
<span class="co">#&gt;          2  14  80  17</span>
<span class="co">#&gt;          7  19  25 109</span></code></pre></div>
<p>The actuary is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_qda, test_set), test_set<span class="op">$</span>y)<span class="op">$</span>overal[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.752</span></code></pre></div>
<p>For sensitivity and specificity, we have a pair of values for <strong>each</strong> class. To define these terms, we need a binary outcome. We therefore have three columns: one for each class as the positives and the other two as the negatives.</p>
<p>We can visualize what parts of the region are called 1, 2 and 7:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">GS &lt;-<span class="st"> </span><span class="dv">150</span>
new_x &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">x_1 =</span> <span class="kw">seq</span>(<span class="kw">min</span>(train_set<span class="op">$</span>x_<span class="dv">1</span>), <span class="kw">max</span>(train_set<span class="op">$</span>x_<span class="dv">1</span>), <span class="dt">len=</span>GS),
                     <span class="dt">x_2 =</span> <span class="kw">seq</span>(<span class="kw">min</span>(train_set<span class="op">$</span>x_<span class="dv">2</span>), <span class="kw">max</span>(train_set<span class="op">$</span>x_<span class="dv">2</span>), <span class="dt">len=</span>GS))
new_x <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(train_qda, new_x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">color =</span> y_hat, <span class="dt">z =</span> <span class="kw">as.numeric</span>(y_hat))) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="fl">0.5</span>, <span class="dt">pch =</span> <span class="dv">16</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_contour</span>(<span class="dt">breaks=</span><span class="kw">c</span>(<span class="fl">1.5</span>, <span class="fl">2.5</span>),<span class="dt">color=</span><span class="st">&quot;black&quot;</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">colour =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">2</span>)))</code></pre></div>
<p><img src="book_files/figure-html/three-classes-plot-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Here is what it looks like for LDA:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_lda &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;lda&quot;</span>, <span class="dt">data =</span> train_set)
<span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_lda, test_set), test_set<span class="op">$</span>y)<span class="op">$</span>overal[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.664</span></code></pre></div>
<p>The accuracy is much worse because the model is more rigid. This is what the decision rule looks like:</p>
<p><img src="book_files/figure-html/lda-too-rigid-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The results for kNN are much better:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_knn &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">15</span>, <span class="dv">51</span>, <span class="dv">2</span>)), 
                   <span class="dt">data =</span> train_set)

<span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_knn, test_set), test_set<span class="op">$</span>y)<span class="op">$</span>overal[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.769</span></code></pre></div>
<p>with much better accuracy now. The decision rule looks like this:</p>
<p><img src="book_files/figure-html/three-classes-knn-better-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note that the limitations of LDA are to the lack of fit of the normal assumption, in particular for class 1.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_set <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">factor</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(x_<span class="dv">1</span>, x_<span class="dv">2</span>, <span class="dt">fill =</span> y, <span class="dt">color=</span>y)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">show.legend =</span> <span class="ot">FALSE</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">stat_ellipse</span>(<span class="dt">type=</span><span class="st">&quot;norm&quot;</span>) </code></pre></div>
<p><img src="book_files/figure-html/three-classes-lack-of-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Generative models can be very powerful, but only when we are able to successfully approximate the joint distribution of predictors conditioned on each class.</p>
</div>
<div id="exercises-48" class="section level3">
<h3><span class="header-section-number">31.4.7</span> Exercises</h3>
<p>We are going to apply LDA and QDA to the <code>tissue_gene_expression</code> dataset. We will start with simple examples based on this dataset and then develop a realistic example.</p>
<ol style="list-style-type: decimal">
<li><p>Create a dataset with just the cerebella and hippocampi, two parts of the brain, and a predictor matrix with 10 randomly selected columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1993</span>)
<span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)
ind &lt;-<span class="st"> </span><span class="kw">which</span>(tissue_gene_expression<span class="op">$</span>y <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;cerebellum&quot;</span>, <span class="st">&quot;hippocampus&quot;</span>))
y &lt;-<span class="st"> </span><span class="kw">droplevels</span>(tissue_gene_expression<span class="op">$</span>y[ind])
x &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>x[ind, ]
x &lt;-<span class="st"> </span>x[, <span class="kw">sample</span>(<span class="kw">ncol</span>(x), <span class="dv">10</span>)]</code></pre></div>
<p>Use the <code>train</code> function to estimate the accuracy of LDA.</p></li>
<li><p>In this case, LDA fits two 10-dimensional normal distributions. Look at the fitted model by looking at the <code>finalModel</code> component of the result of train. Notice there is a component called <code>means</code> that includes the estimate <code>means</code> of both distribution. Plot the mean vectors against each other and determine which predictors (genes) appear to be driving the algorithm.</p></li>
<li><p>Repeat exercises 1 with QDA. Does it have a higher accuracy than LDA?</p></li>
<li><p>Are the same predictors (genes) driving the algorithm? Make a plot as in exercise 2.</p></li>
<li><p>One thing we see in the previous plot is that the value of predictors correlate in both groups: some predictors are low in both groups while others high in both groups. The mean value of each predictor or <code>colMeans(x)</code> is not informative or useful for prediction and often for interpretation purposes it is useful to center or scale each column. This can be achieved with the <code>preProcessing</code> argument in train. Re-run LDA with <code>preProcessing = &quot;scale&quot;</code>. Note that accuracy does not change but see how it is easier to identify the predictors that differ more between groups in the plot made in exercise 4.</p></li>
<li><p>In the previous exercises we saw that both approaches worked well. Plot the predictor values for the two genes with the largest differences between the two groups in a scatter plot to see how they appear to follow a bivariate distribution as assumed by the LDA and QDA approaches. Color the points by the outcome.</p></li>
<li><p>Now we are going to increase the complexity of the challenge slightly: we will consider all the tissue types.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1993</span>)
<span class="kw">data</span>(<span class="st">&quot;tissue_gene_expression&quot;</span>)
y &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>y
x &lt;-<span class="st"> </span>tissue_gene_expression<span class="op">$</span>x
x &lt;-<span class="st"> </span>x[, <span class="kw">sample</span>(<span class="kw">ncol</span>(x), <span class="dv">10</span>)]</code></pre></div>
<p>What accuracy do you get with LDA?</p></li>
<li><p>We see that the results are slightly worse. Use the <code>confusionMatrix</code> function to learn what type of errors we are making.</p></li>
<li><p>Plot an image of the centers of the seven 10-dimensional normal distributions</p></li>
</ol>

</div>
</div>
<div id="classification-and-regression-trees-cart" class="section level2">
<h2><span class="header-section-number">31.5</span> Classification and Regression Trees (CART)</h2>
<div id="the-curse-of-dimensionality" class="section level3">
<h3><span class="header-section-number">31.5.1</span> The curse of dimensionality</h3>
<p>We described how methods such as LDA and QDA are not meant to be used with many predictors <span class="math inline">\(p\)</span> because the number of parameters that we need to estimate becomes too large. For example, with the digits example <span class="math inline">\(p=784\)</span>, so we would have over 600,000 parameters with LDA and we would multiply that by the number of classes for QDA. Kernel methods such as kNN or local regression do not have model parameters to estimate. However, they also face a challenge when multiple predictors are used due to what is referred to the <em>curse of dimensionality</em>. The <em>dimension</em> here refers to the fact that when we have <span class="math inline">\(p\)</span> predictors, the distance between two observations is computed in <span class="math inline">\(p\)</span>-dimensional space.</p>
<p>A useful way of understanding the curse of dimensionality is by considering how large we have to make a span/neighborhood/window to include a given percentage of the data. Remember that with larger neighborhoods, our methods lose flexibility.</p>
<p>For example, suppose we have one continuous predictor with equally spaced points in the [0,1] interval and we want to create windows that include 1/10-th of data. Then it’s easy to see that our windows have to be of size 0.1:</p>
<p><img src="book_files/figure-html/curse-of-dim-1-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Now, for two predictors, if we decide to keep the neighborhood just as small, 10% for each dimension, we include only 1 point:</p>
<p><img src="book_files/figure-html/curse-of-dim-2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>If we want to include 10% of the data, then we need to increase the size of each side of the square to <span class="math inline">\(\sqrt{.10} \approx .316\)</span>:</p>
<p><img src="book_files/figure-html/curse-of-dim-3-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Using the same logic, if we want to include 10% of the data in a three dimensional space, then the side of each cube is <span class="math inline">\(\sqrt[3]{.10} \approx 0.464\)</span>. In general, to include 10% of the data in a case with <span class="math inline">\(p\)</span> dimensions, we need an interval with each side of size <span class="math inline">\(\sqrt[p]{.10}\)</span> of the total. This proportion gets close to 1 quickly and if the proportion is 1, it means we include all the data and are no longer smoothing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
p &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span>
<span class="kw">qplot</span>(p, .<span class="dv">1</span><span class="op">^</span>(<span class="dv">1</span><span class="op">/</span>p), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="book_files/figure-html/curse-of-dim-4-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>So by the time we reach 100 predictors, the neighborhood is no longer very local, as each side covers almost the entire dataset.</p>
<p>Here we look at a set of elegant and versatile methods that adapt to higher dimensions and also allow these regions to take more complex shapes while still producing models that are interpretable. These are very popular, well-known and studied methods. We will concentrate on Regression and Decision Trees and their extension to Random Forests.</p>
</div>
<div id="cart-motivation" class="section level3">
<h3><span class="header-section-number">31.5.2</span> CART motivation</h3>
<p>To motivate this section, we will use a new dataset that includes the breakdown of the composition of olive oil into 8 fatty acids:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(dslabs)
<span class="kw">data</span>(<span class="st">&quot;olive&quot;</span>)
olive <span class="op">%&gt;%</span><span class="st"> </span>tbl_df
<span class="co">#&gt; # A tibble: 572 x 10</span>
<span class="co">#&gt;   region area  palmitic palmitoleic stearic oleic linoleic linolenic</span>
<span class="co">#&gt; * &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;</span>
<span class="co">#&gt; 1 South… Nort…    10.8        0.75     2.26  78.2     6.72      0.36</span>
<span class="co">#&gt; 2 South… Nort…    10.9        0.73     2.24  77.1     7.81      0.31</span>
<span class="co">#&gt; 3 South… Nort…     9.11       0.54     2.46  81.1     5.49      0.31</span>
<span class="co">#&gt; 4 South… Nort…     9.66       0.570    2.4   79.5     6.19      0.5 </span>
<span class="co">#&gt; 5 South… Nort…    10.5        0.67     2.59  77.7     6.72      0.5 </span>
<span class="co">#&gt; 6 South… Nort…     9.11       0.49     2.68  79.2     6.78      0.51</span>
<span class="co">#&gt; # ... with 566 more rows, and 2 more variables: arachidic &lt;dbl&gt;,</span>
<span class="co">#&gt; #   eicosenoic &lt;dbl&gt;</span></code></pre></div>
<p>For illustrative purposes, we will try to predict the region using the fatty acid composition values as predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(olive<span class="op">$</span>region)
<span class="co">#&gt; </span>
<span class="co">#&gt; Northern Italy       Sardinia Southern Italy </span>
<span class="co">#&gt;            151             98            323</span></code></pre></div>
<p>We remove the <code>area</code> column because we won’t use it as a predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">olive &lt;-<span class="st"> </span><span class="kw">select</span>(olive, <span class="op">-</span>area)</code></pre></div>
<p>Let’s very quickly try to predict the region using kNN:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
fit &lt;-<span class="st"> </span><span class="kw">train</span>(region <span class="op">~</span><span class="st"> </span>.,  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, 
             <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">15</span>, <span class="dv">2</span>)), 
             <span class="dt">data =</span> olive)
<span class="kw">ggplot</span>(fit)</code></pre></div>
<p><img src="book_files/figure-html/olive-knn-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We see that using just one neighbor, we can predict relatively well. However, a bit of data exploration reveals that we should be able to do even better. For example, if we look at the distribution of each predictor stratified by region we see that eicosenoic is only present in Southern Italy and that linolenic separates Northern Italy from Sardinia.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">olive <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">gather</span>(fatty_acid, percentage, <span class="op">-</span>region) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(region, percentage, <span class="dt">fill =</span> region)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>fatty_acid, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/olive-eda-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>This implies that we should be able to build an algorithm that predicts perfectly! We can see this clearly by plotting the values for eicosenoic and linoleic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span>olive <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(eicosenoic, linoleic, <span class="dt">color =</span> region)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()
p</code></pre></div>
<p><img src="book_files/figure-html/olive-two-predictors-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In Section <a href="large-datasets.html#predictor-space">35.2.4</a> we defined predictor spaces. The predictor space are eight-dimensional points with values between 0 and 100. In the plot above, we show the space defined by the two predictors eicosenoic and linoleic, and, by eye, we can construct a prediction rule that partitions the predictor space so that each partition contains only outcomes of a one category. This in turn can be used to define an algorithm with perfect accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p <span class="op">+</span><span class="st"> </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.065</span>, <span class="dt">lty =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="op">-</span><span class="fl">0.2</span>, <span class="dt">y =</span> <span class="fl">10.54</span>, <span class="dt">xend =</span> <span class="fl">0.065</span>, <span class="dt">yend =</span> <span class="fl">10.54</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="book_files/figure-html/olive-separated-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Specifically, we define the following decision rule. If eicosenoic is larger than 0.065, predict Southern Italy. If not, then if linolenic is larger than <span class="math inline">\(10.535\)</span>, predict Sardinia and if lower, predict Northern Italy. We can draw this decision tree like this:</p>
<p><img src="book_files/figure-html/olive-tree-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Decision trees like this are often used in practice. For example, to decide if a person is at risk of having a heart attack, doctors use the following:</p>
<p><img src="ml/img/Decision-Tree-for-Heart-Attack-Victim-adapted-from-Gigerenzer-et-al-1999-4.png" width="50%" style="display: block; margin: auto;" /></p>
<p>(Source: <a href="https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1759289_code1486039.pdf?abstractid=1759289&amp;mirid=1&amp;type=2">Walton 2010 Informal Logic, Vol. 30, No. 2, pp. 159-184</a>)</p>
<p>A tree is basically a flow chart of yes or no questions. The general idea of the methods we are describing is to define an algorithm that uses data to create these trees with predictions at the ends, referred to as <em>nodes</em>. Regression and decision trees operate by predicting an outcome variable <span class="math inline">\(Y\)</span> by partitioning the predictors.</p>
</div>
<div id="regression-trees" class="section level3">
<h3><span class="header-section-number">31.5.3</span> Regression trees</h3>
<p>When the outcome is continuous, we call the method a <em>regression</em> tree. To introduce regression trees, we will use the 2008 poll data used in previous sections to describe the basic idea of how we build these algorithms. As with other machine learning algorithms, we will try to estimate the conditional expectation <span class="math inline">\(f(x) = \mbox{E}(Y | X = x)\)</span> with <span class="math inline">\(Y\)</span> the poll margin and <span class="math inline">\(x\)</span> the day.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;polls_2008&quot;</span>)
<span class="kw">qplot</span>(day, margin, <span class="dt">data =</span> polls_<span class="dv">2008</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-again-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The general idea here is to build a decision tree and, at end of each <em>node</em>, obtain a predictor <span class="math inline">\(\hat{y}\)</span>. A mathematical way to describe this is to say that we are<br />
partitioning the predictor space into <span class="math inline">\(J\)</span> non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and then for any predictor <span class="math inline">\(x\)</span> that falls within region <span class="math inline">\(R_j\)</span>, estimate <span class="math inline">\(f(x)\)</span> with the average of the training observations <span class="math inline">\(y_i\)</span> for which the associated predictor <span class="math inline">\(x_i\)</span> in also in <span class="math inline">\(R_j\)</span>.</p>
<p>But how do we decide on the partition <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span> and how do we choose <span class="math inline">\(J\)</span>? Here is where the algorithm gets a bit complicated.</p>
<p>Regression trees create partitions recursively. We start the algorithm with one partition, the entire predictor space. In our simple first example this space is the interval [-155, 1]. But after the first step we will have two partitions. After the second step we will split one of these partitions into two and will have three partitions. Then four, then five, and so one. We describe how we pick the partition to further partition, and when to stop, later.</p>
<p>Once we select a partition to split, to create the new partitions, we find a predictor <span class="math inline">\(j\)</span> and value <span class="math inline">\(s\)</span> that define two new partitions, we will call them <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(R_2(j,s)\)</span>, that split our observations in the current partition, referred to as <span class="math inline">\(\mathbf{x}\)</span>, by asking if <span class="math inline">\(x_j\)</span> is bigger than <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[
R_1(j,s) = \{\mathbf{x} \mid x_j &lt; s\} \mbox{  and  } R_2(j,s) = \{\mathbf{x} \mid x_j \geq s\}
\]</span></p>
<p>In our example we only have on predictor, so we will always chose <span class="math inline">\(j=1\)</span>, but in general this will not be the case. Now, after we define the new partitions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>, and we decide to stop the partitioning, we compute predictors by taking the average of all the observations <span class="math inline">\(y\)</span> for which the associated <span class="math inline">\(\mathbf{x}\)</span> is in <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We refer to these two as <span class="math inline">\(\hat{y}_{R_1}\)</span> and <span class="math inline">\(\hat{y}_{R_2}\)</span> respectively.</p>
<p>But how do we pick <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>? Basically we find the pair that minimizes the residual sum of square (RSS): <span class="math display">\[
\sum_{i:\, x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2
\]</span></p>
<p>This is then applied recursively to the new regions <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span>. We described how we stop later, but once we are done partitioning the predictor space into regions, in each region a prediction is made using the observations in that region.</p>
<p>Let’s take a look at what this algorithm does on the 2008 presidential election poll data. We will use the <code>rpart</code> function in the <strong>rpart</strong> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>)</code></pre></div>
<p>Here, there is only one predictor. So we do not have to decide which predictor <span class="math inline">\(j\)</span> to split by, we simply have to decide what value <span class="math inline">\(s\)</span> we use to split. We can visually see where the splits were made:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">margin =</span> <span class="fl">0.1</span>)
<span class="kw">text</span>(fit, <span class="dt">cex =</span> <span class="fl">0.75</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-tree-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The first split is made on day 39.5. One of those regions is then split at day 86.5. The two resulting new partitions are split on days 49.5 and 117.5 respectively, and so on. We end up with 8 partitions. The final estimate <span class="math inline">\(\hat{f}(x)\)</span> looks like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-tree-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note that the algorithm stopped partitioning at 8. Now we explain how this decision is made.</p>
<p>First we need to define the term <em>complexity parameter</em> (cp). Every time we split and define two new partitions, our training set RSS decreases. This is because with more partitions, our model has more flexibility to adapt to the training data. In fact, if you split until every point is its own partition, then RSS goes all the way down to 0 since the average of one value is that same value. To avoid this, the algorithm sets a minimum for how much the RSS must improve for another partition to be added. This parameter is referred to as the <em>complexity parameter</em> (cp). The RSS must improve by a factor of cp for the new partition to be added. Large value of cp will therefore force the algorithm to stop earlier which result in less nodes.</p>
<p>However, cp is not the only parameter used to decide if we should partition a current partition or not. Another common parameter is the minimum number of observations required in a partition before partitioning it further. The argument used in the <code>rpart</code> function is <code>minsplit</code> and the default is 20. The <code>rpart</code> implementation of regression trees also permits users to determine a minimum number observations in each node. The argument is <code>minbucket</code> and defaults to <code>round(minsplit/3)</code>.</p>
<p>As expected, if we set <code>cp = 0</code> and <code>minsplit=2</code>, then our prediction is as flexible as possible and our predictor is our original data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(margin <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>, <span class="dt">control =</span> <span class="kw">rpart.control</span>(<span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">minsplit =</span> <span class="dv">2</span>))
polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-tree-over-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Intuitively we know that this is not a good approach as it will generally result in over-training. These <code>cp</code>, <code>minsplit</code> and <code>minbucket</code> three parameters can be used to control the variability of the final predictors. The larger these values are the more data is averaged to compute a predictor and thus reduce variability. The drawback is that it restrict flexibility.</p>
<p>So how do we pick these parameters? We can use cross validation, described in Section <a href="cross-validation.html#cross-validation">32</a>, just like with any tuning parameter. Here is an example of using cross validation to chose cp.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(margin <span class="op">~</span><span class="st"> </span>., 
                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="fl">0.05</span>, <span class="dt">len =</span> <span class="dv">25</span>)),
                     <span class="dt">data =</span> polls_<span class="dv">2008</span>)
<span class="kw">ggplot</span>(train_rpart)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-tree-train-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>To see the resulting tree, we access the <code>finalModel</code> and plot it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">margin =</span> <span class="fl">0.1</span>)
<span class="kw">text</span>(train_rpart<span class="op">$</span>finalModel, <span class="dt">cex =</span> <span class="fl">0.75</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-final-tree-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>And because we only have one predictor, we can actually plot <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(train_rpart)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-final-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Note that if we already have a tree and want apply a higher cp value we can use the <code>prune</code> function. We call this <em>pruning</em> tree because we are <em>snipping of</em> partitions that do not meet a <code>cp</code> criterion. We previously create a tree that used a <code>cp = 0</code> and saved it to <code>fit</code>. We can prune it like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pruned_fit &lt;-<span class="st"> </span><span class="kw">prune</span>(fit, <span class="dt">cp =</span> <span class="fl">0.01</span>)</code></pre></div>
</div>
<div id="classification-decision-trees" class="section level3">
<h3><span class="header-section-number">31.5.4</span> Classification (decision) trees</h3>
<p>Classification, or decision trees, are used in classification problems where the outcome is categorical. We use the same partitioning principle with some differences to account for the fact that we are now working with a categorical outcome.</p>
<p>The first difference is that rather than taking the average in each partition (we can’t take the average of categories), we form predictions by calculating which class is the most common among the training set observations within the partition.</p>
<p>The second is that we can no longer use RSS to decide on the partition. While we could use the naive approach of looking for partitions that minimize training error, better performing approaches use more sophisticated metrics. Two of the more popular ones are the <em>Gini Index</em> and <em>Entropy</em>.</p>
<p>In a perfect scenario, the outcomes in each of our partitions are all of the same category since this will permit perfect accuracy. The <em>Gini Index</em> is going to be 0 in this scenario, and become larger the more we deviate from this scenario. To define the Gini Index, we define <span class="math inline">\(\hat{p}_{j,k}\)</span> as the proportion of observations in partition <span class="math inline">\(j\)</span> that are of class <span class="math inline">\(k\)</span>. The Gini Index is defined as</p>
<p><span class="math display">\[
\mbox{Gini}(j) = \sum_{k=1}^K \hat{p}_{j,k}(1-\hat{p}_{j,k})
\]</span></p>
<p>If you study the formula carefully you will see that it is in fact 0 in the perfect scenario described above.</p>
<p><em>Entropy</em> is a very similar quantity, defined as</p>
<p><span class="math display">\[
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
\]</span></p>
<p>Let us look at how a classification tree performs on the digits example we examined before:</p>
<p>We can use this code to run the algorithm and plot the resulting tree:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_rpart &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,
                     <span class="dt">method =</span> <span class="st">&quot;rpart&quot;</span>,
                     <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">cp =</span> <span class="kw">seq</span>(<span class="fl">0.0</span>, <span class="fl">0.1</span>, <span class="dt">len =</span> <span class="dv">25</span>)),
                     <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)
<span class="kw">plot</span>(train_rpart)</code></pre></div>
<p><img src="book_files/figure-html/mnist-27-tree-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The accuracy achieves by this approach is better than regression’s, but is not as good as what we achieved with kernel methods:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rpart, mnist_<span class="dv">27</span><span class="op">$</span>test), mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.82</span></code></pre></div>
<p>The plot of the estimate conditional probability shows us the limitations of classification trees:</p>
<p><img src="book_files/figure-html/rf-cond-prob-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Note that with with decision trees, it is difficult to make the boundaries smooth since each partition creates a discontinuity.</p>
<p>Classification trees have certain advantages that make them very useful. They are highly interpretable, even more so than linear models. They are easy to visualize (if small enough). Finally, they can model a human decision processes and don’t require that dummy predictors for categorical variables be used. On the other hand, the approach via recursive partitioning can easily over-train and is therefore a bit harder to train than, for example, linear regression or kNN. Furthermore, in terms of accuracy, it is rarely the best performing method since it is not very flexible and is highly unstable to changes in training data. Random Forests, explained next, improve on several of these shortcomings.</p>
</div>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">31.6</span> Random Forests</h2>
<p>Random Forests are a <strong>very popular</strong> machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by <em>averaging</em> multiple decision trees (a forest of trees constructed with randomness). It has two features that help accomplish this.</p>
<p>The first step is <em>bootstrap aggregation</em> or <em>bagging</em>. The general idea is to generate many predictors, each using regression or classification trees, and then forming a final prediction based on the average prediction of all these trees. To assure that the trees are the same we use the bootstrap to induce randomness. These two combined explain the name: the bootstrap makes the individual trees <strong>randomly</strong> different, and the combination of trees is the <strong>forest</strong>. The specific steps are as follows.</p>
<ol style="list-style-type: decimal">
<li><p>Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1, T_2, \dots, T_B\)</span>. We later explain how we assure they are different.</p></li>
<li><p>For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span>.</p></li>
<li><p>For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} = \frac{1}{B} \sum_{j=1}^B \hat{y}_j\)</span>. For categorical data classification, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, \dots, \hat{y}_T\)</span>).</p></li>
</ol>
<p>So how do we get different decision trees from a single training set?</p>
<p>For this, we use the randomness in two ways which we explain in the steps below. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create <span class="math inline">\(T_j, \, j=1,\ldots,B\)</span> from the training set we do the following:</p>
<ol style="list-style-type: decimal">
<li><p>Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set <strong>with replacement</strong>. This is the first way in induce randomness.</p></li>
<li><p>A large number of features is typical in a machine learning challenges. Often, many features can be informative but including them all in the model may result in overfitting. The second way Random Forests induce randomness is by randomly selecting features to be included in the building of each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest, thereby improving prediction accuracy.</p></li>
</ol>
<p>To illustrate how the first steps can result in smoother estimates we will demonstrate by fitting a Random Forest to the 2008 polls data. We will use the <code>randomForest</code> function in the <strong>randomForest</strong> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
fit &lt;-<span class="st"> </span><span class="kw">randomForest</span>(margin<span class="op">~</span>., <span class="dt">data =</span> polls_<span class="dv">2008</span>) </code></pre></div>
<p>Note that if you apply the function <code>plot</code> to the resulting object, stored in <code>fit</code>, we see the how the error rate of our algorithm changes as we add trees.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit)</code></pre></div>
<p><img src="book_files/figure-html/more-trees-better-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can see that in this case, the accuracy improves as we add more trees until about 300 t where accuracy stabilizes.</p>
<p>The resulting estimate for this random forest can be seen like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">polls_<span class="dv">2008</span> <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y_hat =</span> <span class="kw">predict</span>(fit, <span class="dt">newdata =</span> polls_<span class="dv">2008</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(day, margin)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(day, y_hat), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="book_files/figure-html/polls-2008-rf-fit-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Notice that the random forest estimate is much smoother than what we achieved with the regression tree in the previous section. This is possible because the average of many step function can be smooth. We can see this by visually examining how the estimate changes as we add more trees. In the following figure you see each of bootstrap samples for several values if <span class="math inline">\(b\)</span> and for each one we see the tree that is fitted in grey, the previous trees that were fitted in lighter grey, and the result of averaging all the trees estimated up to that point.</p>
<p><img src="ml/img/rf.gif" width="100%" style="display: block; margin: auto;" /></p>
<p>Here is the Random Forest fit for our digits example based on two predictors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
train_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>mnist_<span class="dv">27</span><span class="op">$</span>train)

<span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf, mnist_<span class="dv">27</span><span class="op">$</span>test), mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;     0.79</span></code></pre></div>
<p>Here is what the conditional probabilities look like:</p>
<p><img src="book_files/figure-html/cond-prob-rf-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Visualizing the estimate shows that, although we obtain high accuracy, it appears that there is room for improvement by making the estimate smoother. This could be achieved by changing the parameter that controls the minimum number of data points in the nodes of the tree. The larger this minimum the smoother the final estimate will be. We can train the parameters of the Random Forest. Below, we use the <strong>caret</strong> package to optimize over the minimum node size. To do this with the <code>caret</code> package we need to use another implementation random that permits us to optimize for this parameter. The function <code>Rborist</code> of the <strong>Rborist</strong> permits this. We can run cross validation to chose this parameter using this code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_rf_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">train</span>(y <span class="op">~</span><span class="st"> </span>.,
      <span class="dt">method =</span> <span class="st">&quot;Rborist&quot;</span>,
      <span class="dt">tuneGrid =</span> <span class="kw">data.frame</span>(<span class="dt">predFixed =</span> <span class="dv">2</span>, <span class="dt">minNode =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">50</span>)),
      <span class="dt">data =</span> mnist_<span class="dv">27</span><span class="op">$</span>train)
<span class="kw">confusionMatrix</span>(<span class="kw">predict</span>(train_rf_<span class="dv">2</span>, mnist_<span class="dv">27</span><span class="op">$</span>test), mnist_<span class="dv">27</span><span class="op">$</span>test<span class="op">$</span>y)<span class="op">$</span>overall[<span class="st">&quot;Accuracy&quot;</span>]
<span class="co">#&gt; Accuracy </span>
<span class="co">#&gt;    0.795</span></code></pre></div>
<p>The selected model improves accuracy and provides a smoother estimate.</p>
<p><img src="book_files/figure-html/cond-prob-final-rf-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Random Forrest perform better in all the examples we have considered. However, a disadvantage of Random Forests is that we lose interpretability. An approach that helps with interpretability is to examine <em>variable importance</em>. To define <em>variable importance</em> we counts how often a predictor is used in the individual trees. You can learn more about <em>variable importance</em> in an advanced <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">machine learning book</a>. The <strong>caret</strong> package includes the function <code>varImp</code> that extracts variable importance from any model in which the calculation is implemented. We give an example on how we use variable importance in the next section.</p>
<div id="exercises-49" class="section level3">
<h3><span class="header-section-number">31.6.1</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li><p>Create a simple dataset where the outcomes grows 0.75 units on average for every increase in a predictor.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
sigma &lt;-<span class="st"> </span><span class="fl">0.25</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)
y &lt;-<span class="st"> </span><span class="fl">0.75</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dv">0</span>, sigma)
dat &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)</code></pre></div>
<p>Use <code>rpart</code> to fit a regression tree and save the result to <code>fit</code>.</p></li>
<li><p>Plot the final tree so that you can see where the partitions occurred.</p></li>
<li><p>Make a scatter plot of <code>y</code> versus <code>x</code> along with the predicted values based on the fit.</p></li>
<li><p>Now run Random Forests instead of a regression tree using <code>randomForest</code> from the <strong>randomForest</strong> package, and remake the scatterplot with the prediction line.</p></li>
<li><p>Use the function <code>plot</code> to see if the Random Forest has converged or if we need more trees.</p></li>
<li><p>It seems that the default values for the Random Forest result in an estimate that is too flexible (not smooth). Re-run the Random Forest but this time with <code>nodesize</code> set at 50 and <code>maxnodes</code> set at 25. Remake the plot.</p></li>
<li><p>We see that this yields smoother results. Let’s use the train function to help us pick these values. From the <a href="https://topepo.github.io/caret/available-models.html">caret description</a> of methods we see that we can’t tune the <code>maxnodes</code> parameter or the <code>nodesize</code> argument with <code>randomForests</code>. So we will use the <strong>Rborist</strong> package and tune the <code>minNode</code> argument. Use the <code>train</code> function to try values <code>minNode &lt;- seq(5, 250, 25)</code>. See which value minimizes the estimated RMSE.</p></li>
<li><p>Make a scatterplot along with the prediction from the best fitted model.</p></li>
<li><p>Use the <code>rpart</code> function to fit a classification tree to the <code>tissue_gene_expression</code> dataset. Use the <code>train</code> function to estimate the accuracy. Try out <code>cp</code> values of <code>seq(0, 0.05, 0.01)</code>. Plot the accuracy to report the results of the best model.</p></li>
<li><p>Study the confusion matrix for the best fitting classification tree. What do you observe happening for placenta?</p></li>
<li><p>Notice that placentas are called endometriums more often than placenta. Note also that the number of placentas is just six, and that, by default, <code>rpart</code> requires 20 observations before splitting a node. So it is difficult to have a node in which placentas are the majority. Rerun the above analysis but this time permit <code>rpart</code> to split any node by using the argument <code>control = rpart.control(minsplit = 0)</code>. Does the accuracy increase? Look at the confusion matrix again.</p></li>
<li><p>Plot the tree from the best fitting model obtained in exercise 11.</p></li>
<li><p>We can see that with just six genes, we are able to predict the tissue type. Now let’s see if we can do even better with a Random Forest. Use the <code>train</code> function and the <code>rf</code> method to train a Random Forest. Try out values of <code>mtry</code> ranging from, at least, <code>seq(50, 200, 25)</code>. What <code>mtry</code> value maximizes accuracy? To permit small <code>nodesize</code> to grow as we did with the classification trees, use the following argument: <code>nodesize = 1</code>. This will take several seconds to run. If you want to test it out, try using smaller values with <code>ntree</code>. Set the seed to 1990.</p></li>
<li><p>Use the function <code>varImp</code> on the output of <code>train</code> and save it to an object called <code>imp</code>.</p></li>
<li><p>The <code>rpart</code> model we ran above produced a tree that used just six predictors. Extracting the predictor names is not straightforward, but can be done. If the output of the call to train was <code>fit_rpart</code>, we can extract the names like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ind &lt;-<span class="st"> </span><span class="op">!</span>(fit_rpart<span class="op">$</span>finalModel<span class="op">$</span>frame<span class="op">$</span>var <span class="op">==</span><span class="st"> &quot;&lt;leaf&gt;&quot;</span>)
tree_terms &lt;-<span class="st"> </span>
<span class="st">  </span>fit_rpart<span class="op">$</span>finalModel<span class="op">$</span>frame<span class="op">$</span>var[ind] <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">unique</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as.character</span>()
tree_terms</code></pre></div>
<p>What is the variable importance in the Random Forest call for these predictors? Where do they rank?</p></li>
<li><p>Advanced: Extract the top 50 predictors based on importance, take a subset of <code>x</code> with just these predictors and apply the function <code>heatmap</code> to see how these genes behave across the tissues. We will introduce the <code>heatmap</code> function in the Section <a href="clustering.html#clustering">36</a>.</p></li>
<li><p>In this chapter, we illustrated a couple of machine learning algorithms on a subset of the MNIST dataset. Try fitting a model to the entire dataset.</p></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="smoothing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cross-validation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rafalab/dsbook/edit/master/ml/regression.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
