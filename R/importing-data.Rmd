# Importing data

In this chapter, we used a data set already stored in an R object. A data scientist will rarely have such luck and will have to import data into R from either a file, a database, or other source. We cover this in more detail in the Data Wrangling part of the book. But because it is so common to read data from a file, we will briefly describe the key approach and function, in case you want to use your new knowledge on one of your own datasets.


Small datasets such as the one used in this chapter are commonly stored as Excel files. Although there
are R packages designed to read Excel (xls) format, you generally want
to avoid this format and save files as comma delimited (Comma-Separated
Value/CSV) or tab delimited (Tab-Separated Value/TSV/TXT) files.
These plain-text formats make it easier to share data since commercial software is not required for working with the data.


## Paths and the working directory 

The first step is to find the file containing your data and know its *path*.
When you are working in R, it is useful to know your _working directory_. This is the folder in which R will save or look for files by default. You can see your working directory by typing:

```{r, eval=FALSE}
getwd()
```

You can also change your working directory using the function `setwd` or you can change it through RStudio by clicking on "Session".

The functions that read and write files (there are several in R) assume you mean to look for files or write files in the working directory. Our recommended approach for beginners will have you reading and writing to the working directory. However, you can also type the [full path](http://www.computerhope.com/jargon/a/absopath.htm), which will work independently of the working directory.

We have included the US murders data in CSV file as part of the __dslabs__ package. We recommend placing your data in your working directory. Because knowing where packages store files is rather advanced, we provide the following code that finds the directory and copies the file:

```{r}
dir <- system.file(package="dslabs") #extracts the location of package
filename <- file.path(dir,"extdata/murders.csv")
file.copy(filename, "murders.csv")
```

If a file is copied succesfully, the `file.copy` function returns `TRUE`.

You should be able to see the file in your working directory and can check by using:

```{r, eval=FALSE}
list.files()
```

## `read.csv`

We are ready to read in the file. There are several functions for reading in tables. Here we introduce one included in base R:

```{r}
dat <- read.csv("murders.csv")
head(dat)
```

We can see that we have read in the file.

Warning: `read.csv` automatically converts characters to factors. Note, for example, that: 

```{r}
class(dat$state)
```

You can avoid this using:
```{r}
dat <- read.csv("murders.csv", stringsAsFactors = FALSE)
class(dat$state)
```

With this call, the region variable is no longer a factor, but we can easily change this with:

```{r}
dat <- mutate(dat, region =  as.factor(region))
```

Now that we are done with this example, we remove the `murders.csv` file from our working directory

```{r}
file.remove("murders.csv")
```

If the file is removed successfully `file.remove` returns `TRUE`.


# Data import

# Importing data

In this chapter, we used a data set already stored in an R object. A data scientist will rarely have such luck and will have to import data into R from either a file, a database, or other source. We cover this in more detail in the Data Wrangling part of the book. But because it is so common to read data from a file, we will briefly describe the key approach and function, in case you want to use your new knowledge on one of your own datasets.


Small datasets such as the one used in this chapter are commonly stored as Excel files. Although there
are R packages designed to read Excel (xls) format, you generally want
to avoid this format and save files as comma delimited (Comma-Separated
Value/CSV) or tab delimited (Tab-Separated Value/TSV/TXT) files.
These plain-text formats make it easier to share data since commercial software is not required for working with the data.


## Paths and the working directory 

The first step is to find the file containing your data and know its *path*.
When you are working in R, it is useful to know your _working directory_. This is the folder in which R will save or look for files by default. You can see your working directory by typing:

```{r, eval=FALSE}
getwd()
```

You can also change your working directory using the function `setwd` or you can change it through RStudio by clicking on "Session".

The functions that read and write files (there are several in R) assume you mean to look for files or write files in the working directory. Our recommended approach for beginners will have you reading and writing to the working directory. However, you can also type the [full path](http://www.computerhope.com/jargon/a/absopath.htm), which will work independently of the working directory.

We have included the US murders data in CSV file as part of the __dslabs__ package. We recommend placing your data in your working directory. Because knowing where packages store files is rather advanced, we provide the following code that finds the directory and copies the file:

```{r}
dir <- system.file(package="dslabs") #extracts the location of package
filename <- file.path(dir,"extdata/murders.csv")
file.copy(filename, "murders.csv")
```

If a file is copied succesfully, the `file.copy` function returns `TRUE`.

You should be able to see the file in your working directory and can check by using:

```{r, eval=FALSE}
list.files()
```

## `read.csv`

We are ready to read in the file. There are several functions for reading in tables. Here we introduce one included in base R:

```{r}
dat <- read.csv("murders.csv")
head(dat)
```

We can see that we have read in the file.

Warning: `read.csv` automatically converts characters to factors. Note, for example, that: 

```{r}
class(dat$state)
```

You can avoid this using:
```{r}
dat <- read.csv("murders.csv", stringsAsFactors = FALSE)
class(dat$state)
```

With this call, the region variable is no longer a factor, but we can easily change this with:

```{r}
dat <- mutate(dat, region =  as.factor(region))
```

Now that we are done with this example, we remove the `murders.csv` file from our working directory

```{r}
file.remove("murders.csv")
```

If the file is removed successfully `file.remove` returns `TRUE`.


## Text versus binary files

For data science purposes, files can generally be classified into two categories: text files (also known as ASCII files) and binary files. You have already worked with text files. All your R scripts are text files and so are the R markdown files used to create this book. The csv tables you have read are also text files. One big advantage of these files is that we can easily "look" at them without having to purchase any kind of special software or follow complicated instructions. Any text editor can be used to examine a text file, including freely available editors such as RStudio, Notepad, textEdit, vi, emacs, nano, and pico. To see this, try opening a csv file using the "Open file" RStudio tool. You should be able to "see" the content right on your editor. However, if you try to open, say, an excel xls file, jpg or png file, you will not be able to see anything immediately useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined. 

Although R includes tools for reading widely used binary files, such as xls files, in general you will want to find data sets stored in text files. Similarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). 

FIX AFTER WE MOVE
Extracting data from a spreadsheet stored as a text file is perhaps the easiest way to bring data from a file to an R session. Unfortunately, spreadsheets are not always available and the fact that you can look at text files does not necessarily imply that extracting data from them will be straightforward. Part of what we learn in this chapter is to extract data from more complex text files such as html files.

## Unicode versus ASCII

A challenge in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file: a Unicode text file. 

To understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s. ASCII is an _encoding_ that maps characters to numbers. ASCII uses 7 bits (0s and 1s) which results in $2^7 = 128$ unique items, enough to encode all the characters on an English language keyboard. However, other languages use characters not included in this encoding. For example, the é in México is not encoded by ASCII. For this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16 and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding. 

Although we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see "weird looking" characters you were not expecting. [Here](https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell) is an example.

## Importing spreadsheets

```{r, message=FALSE}
library(tidyverse)
```

FIX WHEN MOVED
In the R chapter, we covered some of the basics of importing data. We described functions available in the default R installation. Here, we present a more general discussion and introduce the __tidyverse__ packages __readr__ and __readxl__.

Currently, one of the most common ways of storing and sharing data for analysis is through electronic spreadsheets. A spreadsheet stores data in rows and columns. It is basically a file version of a data frame. When saving such a table to a computer file, one needs a way to define when a new row or column ends and the other begins. This in turn defines the cells in which single values are stored. 

When creating spreadsheets with text files, like the ones created with a simple text editor, a new row is defined with return and columns are separated with some predefined special character. The most common characters are comma (`,`), semicolon (`;`), white space (`\  `) and tab (`\ \ \ \ `). Here is an example of what a comma separated file looks like if we open it with a basic text editor:


```{r, echo=FALSE}
knitr::include_graphics("wrangling/img/csv-file.png")
```

The first row contains column names rather than data. We call this a _header_ and when we read-in data from a spreadsheet it is important to know if the file has a header or not. Most reading functions assume there is a header. To know if the file has a header, it helps to look at the file before trying to read it. This can be done with a text editor or with RStudio. In RStudio, we can do this by either opening the file in the editor or navigating to the file location, double clicking on the file and hitting _View File_.

However, not all spreadsheet files are in a text format. Google Sheets, which are rendered on a browser, are an example. Another example is the proprietary format used by Microsoft Excel. These can't be viewed with a text editor. Given the widespread use of Microsoft Excel software, this format is widely used. Although there are R packages designed to read this format, if you are choosing a file format to save your own data, you generally want to avoid Microsoft Excel. We recommend Google Sheets as a free software tool for organizing data.  We provide more recommendations in the section Data Organization with Spreadsheets.

## Paths and the Working Directory


#CHANGE AFTER MOVE.. MAYBE BRING STUFF FROM TOOLS
We start by demonstrating how to read-in a file that is already saved on your computer. There are several ways to do this and we will discuss three of them. However, you only need to learn one to follow along.

The first step is to find the file containing your data and know its location in your file system.

When you are working in R, it is important to know your _working directory_. This is the directory in which R will save or look for files by default. You can see your working directory by typing:

```{r, eval=FALSE}
getwd()
```

You can change your working directory using the function `setwd`. If you are using RStudio, you can change it by clicking on _Session_ then _Set Working Directory_.

One thing that file reading functions  have in common is that, unless a full path is provided, they search for files in the working directory.  For this reason, our recommended approach for beginners is that you create a directory for each analysis and keep the raw data files in that directory. To keep raw data files organized, we recommend creating a `data` directory especially when the project involves more than one data file. We provide more advice on how to keep files organized in the Productivity Tools chapter.

Because you may not have a data file handy yet, we provide example data files in the __dslabs__ package. Once you download and install the __dslabs__ package, files will be in the external data ('extdata`) directory:

```{r, eval=FALSE}
system.file("extdata", package="dslabs")
```

Note that the output of this function call will change depending on your operating system, how you installed R and the version of R. We therefore do not show the output of the call. But it will be consistent within your system and you will be able to see the files included in this directory using the function `list.files`:

```{r}
path <- system.file("extdata", package="dslabs")
list.files(path)
```

Now that we know the location of these files, we are ready to import them into R. To make the code simpler and following along easier, you can move this file to your working directory. You can do this through the file system directly, but you can also do it within R itself using the `file.copy` function. To do this, it helps to define a variable with the full path using the function `file.path`. Using `paste` to construct a filepath string is not recommended since Microsoft Windows and Macs/Linux/Unix use different slashes when representing paths. The function `file.path` is aware of your system and chooses the correct slashes. Here is an example of how to define a full path for a file:

```{r}
path <- system.file("extdata", package = "dslabs")
filename <- "murders.csv"
fullpath <- file.path(path, filename)
```

We don't show the result of this function call because it will be different on your computer. Run the command and notice all the folder names related to the location of this file in your system.

You can now copy the file over to the working directory like this:

```{r}
file.copy(fullpath, getwd())
```

You can check if the file is now in your working directory using the `file.exists` function:

```{r}
file.exists(filename)
```

Because we have copied it here, in the next section we assume that `murders.csv` is in the working directory.

## The readr and readxl packages

Now we are ready to read-in the file. __readr__ is the __tidyverse__ library that includes functions for reading data stored in text file spreadsheets into R. The following functions are available to read-in spreadsheets:

| Function  | Format                                           | Typical suffix |
|-----------|--------------------------------------------------|----------------| 
| read_table| white space separated values | txt |
| read_csv | comma separated values |  csv |
| read_csv2 | semicolon separated values | csv |
| read_tsv | tab delimited separated values | tsv |
| read_delim | general text file format, must define delimiter | txt |

The readxl package provides functions to read-in Microsoft Excel formats:

| Function  | Format                                           | Typical suffix |
|-----------|--------------------------------------------------|----------------| 
| read_excel | auto detect the format | xls, xlsx|
| read_xls | original format |  xls |
| read_xlsx | new format | xlsx |

The Microsoft Excel formats permits you to have more than one spreadsheet in one file. These are referred to as _sheets_. The functions listed above read the first sheet by default, but we can also read the others. The `excel_sheets` function gives us the names of all the sheets in an excel file. These names can then be passed to the `sheet` argument in the three functions above to read sheets other than the first.

Although the suffix usually tells us what type of file it is, there is no guarantee that these always match. We can open the file to take a look or use function `read_lines` to look at a few lines:

```{r}
read_lines("murders.csv", n_max = 3)
```

This also shows that there is a header. Now we are ready to read-in the data into R. From the suffix and the peek at the file, we know to use `read_csv`:

```{r, message}
dat <- read_csv(filename)
```
Note that we receive a message letting us know what data types were used for each column. 
Also note that `dat` is a `tibble` not just a data frame. This is because `read_csv` is a __tidyverse__ parser. We can see that the data has in fact been read-in with the content in the file:

```{r}
head(dat)
```
Finally, note that we can also use the full path for the file:

```{r, eval=FALSE}
dat <- read_csv(fullpath)
```

## R-base functions

R-base also provides import functions. These have similar names to those in the __tidyverse__, for example `read.table`, `read.csv` and `read.delim`. However, there are a couple of important differences. To show this we read-in the data with an R-base function:

```{r}
dat2 <- read.csv(filename)
```

One difference is that now we have a data frame not a tibble:

```{r}
class(dat2)
```

The other difference is that the characters are converted to factors:

```{r}
class(dat2$abb)
class(dat2$region)
```

This can be avoided by setting the argument `stringsAsFactors` to `FALSE`. In our experience this can be a cause for confusion since a variable that was saved as characters in file is converted to factors regardless of what the variable represents. In fact, we **highly** recommend setting `stringsAsFactors=FALSE` to be your default approach when using the R-base parsers.

### Downloading files

Another common place for data to reside is on the internet. When these are data files, we can download them and then import them or even read them directly from the web. For example, we note that because our __dslabs__ package is on GitHub, the file we downloaded with the package has a url:

```{r}
url <- "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
```

The `read_csv` file can read these files directly:

```{r, message = FALSE}
dat <- read_csv(url)
```

If you want to have a local copy of the file, you can use the `download.file` function: 

```{r, eval=TRUE}
download.file(url, "murders.csv")
```

This will download the file and save it on your system with the name `murders.csv`. You can use any name here, not necessarily `murders.csv`. Note that when using `donwload.file` you should be careful as it will overwrite existing files without warning.

Two functions that are sometimes useful when downloading data from the internet are `tempdir` and `tempfile`. The first creates a directory with a random name that is very likely to be unique. Similarly, `tempfile` creates a character string, not a file, that is likely to be a unique filename:

```{r}
tempfile()
```

So you can run a command like this which erases the temporary file once it imports the data:

```{r, eval=FALSE}
tmp_filename <- tempfile()
download.file(url, tmp_filename)
dat <- read_csv(tmp_filename)
file.remove(tmp_filename)
```


## Nuances

When reading in spreadsheets many things can go wrong. The file might have a multiline header, be missing cells or it might use an unexpected [encoding]( https://en.wikipedia.org/wiki/Character_encoding). We recommend you read this [post](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/). 

With experience you will learn how to deal with different challenges. Carefully reading the help files for the functions discussed here will be useful. Two other functions that are helpful are `scan` and `readLines`. With scan you can read-in each cell of a file. Here is an example:

```{r}
path <- system.file("extdata", package = "dslabs")
filename <- "murders.csv"
x <- scan(file.path(path, filename), sep=",", what = "c")
x[1:10]
```



```{r, echo=FALSE, message=FALSE}
tmp <- file.remove(filename)
```


## Exercises {-}


1. Use the `read_csv` function to read each of the files that the following code saves in the `files` object: 

    ```{r, eval=FALSE}
    path <- system.file("extdata", package = "dslabs")
    files <- list.files(path)
    files
    ```

  
2. Note that the last one, the `olive` file, gives us a warning. This is because the first line of the file is missing the header for the first column. 

    Read the help file for `read.csv` to figure out how to read in the file without reading this header. If you skip the header, you should not get this warning. Save the result to an object called `dat`

  
3. A problem with the previous approach is that we don't know what the columns represent. Type:

    ```{r, eval=FALSE}
    names(dat)
    ```

    to see that the names are not informative.

    Use the `readLines` function to read in just the first line (we later learn how to extract values from the output). 

  

# Organizing Data with Spreadsheets

This book focuses on data analysis. Yet often a data scientist needs to collect data or work with others collecting data. Filling out a spreadsheet by hand is a practice we highly discourage and instead recommend that the process be automatized as much as possible. But sometimes you just have to do it.
In this section, we provide recommendations on how to store data in a spreadsheet. We summarize [a paper](https://www.tandfonline.com/doi/abs/10.1080/00031305.2017.1375989) by Karl Broman and Kara Woo. Below are their general recommendations. Please read the paper for important details.

1. __Be Consistent__ - Before you commence entering data, have a plan. Once you have a plan, be consistent and stick to it. 
2. __Choose Good Names for Things__ - You want the names you pick for objects, files and directories to be memorable, easy to spell, and descriptive. This is actually a hard balance to achieve and it does require time and thought. One important rule to follow is **do not use spaces**, use underscores `_` or dashes instead `-`. Also, avoid symbols, stick to letters and numbers.
3. __Write Dates as YYYY-MM-DD__ -  To avoid confusion, we strongly recommend using this global ISO 8601 standard.
4. __No Empty Cells__ - Fill in all cells and use some common code for missing data. 
5. __Put Just One Thing in a Cell__ - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.
6. __Make it a Rectangle__ - The spreadsheet should be a rectangle. 
7. __Create a Data Dictionary__ - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file.
8. __No Calculations in the Raw Data Files__ - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.
9. __Do Not Use Font Color or Highlighting as Data__ - Most import functions are not able to import this information. Encode this information as a variable instead.
10. __Make Backups__ - Make regular backups of your data. 
11. __Use Data Validation to Avoid Errors__ - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. 
12. __Save the Data as Text Files__ - Save files for sharing as comma or tab delimiters.
    
## Exercises {-}


1. Pick a measurement you can take on a regular basis. For example, your daily weight or how long it takes you to run 5 miles. Keep a spreadsheet that includes the date, the hour, the measurement, and any other informative variable you think is worth keeping. Do this for 2 weeks. Then make a plot.

