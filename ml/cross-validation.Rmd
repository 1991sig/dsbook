# Cross validation

In a previous chapter we described that a common goal of machine learning is to find an algorithm that produces predictors $\hat{Y}$ for an outcome $Y$ that minimizes the MSE:

$$
\mbox{MSE} = \mbox{E}\left\{ \frac{1}{N}\sum_{i=1}^N (\hat{Y}_i - Y_i)^2 \right\}
$$
When all we have to our disposal is one dataset, we can estimate the MSE with the observed MSE like this:

$$
\hat{\mbox{MSE}} = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2
$$
These two are often referred to as the _true error_ and _apparent error_ respectively.

There are two important characteristics of the apparent error we should always keep in mind:

1. Because our data is random,  the aparent error is a random variable: for example the dataset we have may be a random sample from a larger population. So an algorithm having a lower apparent error than another algorithm, may be due to luck.

2. If we train an algorithm on the same dataset that we use to compute the apparent error we might be overtraining. In general, when we do this, the apparent error will be an underestimate of the true error. We saw an extreme example of this with k nearest neighbor.

Cross validation is a technique that permits us to alleviate both these problems. 
To understand cross validation it helps to think of the true error, a theoretical quantity, as the average of many many apparent errors obtained by applying the algorithm to $B$ new random samples of the data, none of them used to train the algorithm. As shown in a previous chapter we think of the true error as :


$$
\frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b - y_i^b\right)^2 
$$
with $B$ a large number that can be though off as practically infinite. As we mentioned this is theoretical quantity because one only get one set of outcomes $y_1, \dots, y_n$. The idea is to imitate the theoretical setup as best we can with the data we have. To do this we have to generate a series of different random samples. There are several approaches to doing this. But the general idea for all of them is to randomly generate smaller datasets that are not used for training, and instead used to estimate the true error.

## K-fold cross validation

The first one we describe is _K-fold cross validation_. 
Generally speaking a machine learning challenge starts with a dataset (blue in the image below). We need to build an algorithm using this dataset that will eventually be used in completely independent datasets (yellow)

```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-1.png")
```

But we don't get to see these independent dataset. 

```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-2.png")
```

So to imitate this situation, we carve out a piece of our dataset and pretend it is an independent dataset: we divide the dataset into a _training set_ (blue) and a _test set_ (red). We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.

We usually try to select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large so that we obtain stable estimate of the loss. Typical choices are to use 10%-20% of the data for testing. 

```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-3.png")
```

Let's reiterate that it indispensable that we do not use the test set at all: not for filtering out rows, not for selecting features, nothing! 

Now this presents a new problem because for most machine learning algorithms we need to select parameters, for example the number of neighbors $k$ in k-nearest neighbors. Here we will refer to the set of parameters as $\lambda$. We need to optimize algorithm parameters without using our test set and we know that if we optimize and evaluate on the same dataset we will overtrain.  This is where cross validation is most useful.

So for each set of algorithm parameters being considered, we we want an estimate of the MSE and then we will chose the parameters with the smallest MSE. Cross validation provides this estimate.

First, it is important that, before we start the cross validation procedure, we fix all the algorithm parameters. Although we will train the algorithm on set of training sets, the parameters $\lambda$ will be the same across all training sets. We will use $\hat{y}_i(\lambda)$ to denote the predictors obtained when we use parameters $\lambda$.

So, if we are going to imitate this definition 


$$
\mbox{MSE}(\lambda) = \frac{1}{B} \sum_{b=1}^B \frac{1}{N}\sum_{i=1}^N \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
$$

we want to consider datasets that can be thought of as an independent random sample and we want to do this several times. With K-fold cross validation we do it $K$ times. In the cartoons we are showing an example that uses $K=5$. 

We will eventually end up with $K$ samples but let's start by describing how to construct the first:  we simply pick $M=N/K$ (we round if $M$ is not a round number) observations at random and think of these as a random sample $y_1^b, \dots, y_M^b$, with $b=1$. We call this the validation set:


```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-4.png")
```

Now we can fit the model in the training set then compute the apparent error on the independent set:

$$
\hat{\mbox{MSE}}_b(\lambda) = \frac{1}{M}\sum_{i=1}^M \left(\hat{y}_i^b(\lambda) - y_i^b\right)^2 
$$

Note that this is just one sample and will therefore return a noisy estimate of the true error. This is why we take $K$ samples, not just one. In K-cross validation we randomly split the observations into $K$ non-overlapping sets:


```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-5.png")
```


Now we we repeat the calculation above for each of these sets $b=1,\dots,K$ and obtain $\hat\mbox{MSE}}_1(\lambda),\dots, \hat{\mbox{MSE}}_K(\lambda)$. Our final estimate we compute the average:

$$
\hat{\mbox{MSE}}(\lambda) = \frac{1}{B} \sum_{b=1}^K \hat{\mbox{MSE}}_b(\lambda)
$$

and obtain an estimate of our loss. A final step would be to select the $\lambda$ that minimizes the MSE.

So we have described how to use cross validation to optimize parameters. However, now we have to take into account the fact that the optimization occurred on the training data so we need to an estimate of our final algorithm based on data that was not used to optimize the choice. Here is where we use the test set we separated early on:


```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-6.png")
```

We can do cross validation again:

```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-7.png")
```

And obtain an final estimate of our expected loss. However, note that this involves 
that our entire compute time gets multiplied by $K$. You will soon learn that performing this task take time because we are performing many complex computations and are always looking for ways to reduce this. For the final evaluation, we often just use the one test set.

Once we are satisfied with this model and want to make it available to others, we could refit the model on the entire dataset, without changing the parameters.


```{r, echo=FALSE,  out.width = "500px", out.extra='style="display: block; margin-left: auto; margin-right: auto; background-color: #000; padding:3px;"'}
knitr::include_graphics("ml/img/cv-7.png")
```


Now how do we pick the cross validation $K$. Large values of $K$ are preferable because the training data  better imitate the original dataset. However, larger values of $K$ will have much slower computation time: for example 100-fold cross validation will be 10 times slower than 10-fold cross validation. For this reason the choices of $K=5$ and $K=10$ are popular.

One way we can improve the variance of our final estimate is to take more samples. To do this, we would no longer require the training set to be partitioned into non-overlapping sets. Instead we would just pick $K$ sets of some size at random.

One popular version of this technique, at each fold, picks observations at random with replacement (which means the same observation can appear twice). This approach 
has some advantages (not discussed here) is generally referred to as the _Bootstrap_. In fact, this is the default approach in the `caret` package.  Below we include a explanation of how it works in genera.


# Bootstrap

Suppose the income distribution of your population is as follows:

```{r, echo = FALSE}
n <- 10^6
income <- 10^(rnorm(n, 4.656786, 0.4394738))
```

```{r income-distribution}
hist(log10(income))
```

The population median is 

```{r}
m <- median(income)
m
```

Suppose we don't have access to the entire population but want to estimate the median $m$. We take a sample of 250 and estimate the population median $m$ with the sample median $M$:

```{r}
set.seed(1)
N <- 250
X <- sample(income, N)
M <- median(X)
M
```

Can we construct a confidence interval? What is the distribution of $M$ ?

From a Monte Carlo simulation we see that the distribution of $M$ is approximately normal with the following expected value and standard error:

```{r median-is-normal}
B <- 10^5
Ms <- replicate(B, {
  X <- sample(income, N)
  M <- median(X)
})
par(mfrow=c(1,2))
hist(Ms)
qqnorm(Ms)
qqline(Ms)
mean(Ms)
sd(Ms)
```

The problem here is that, as we have described before, in practice we do not have access to the distribution. In the past we have used the central limit theorem. But the CLT we studies applies to averages and here we are interested in the median. 

The Bootstrap permits us to approximate a Monte Carlo simulation without access to the entire distribution. The general idea is relatively simple. We act as if the sample is the distribution and sample (with replacement) datasets of the same size. Then we compute the summary statistic, in this case median, on this _bootstrap sample_. 

There is theory telling us that, in many situations, the distribution of the statistics obtained with bootstrap samples approximate the distribution of our actual statistic. This is how we construct bootstrap samples and an approximate distribution:


```{r}
B <- 10^5
M_stars <- replicate(B, {
  X_star <- sample(X, N, replace = TRUE)
  M_star <- median(X_star)
})
```

Now we can check how close it is to the actual distribution
```{r boostram-versus-monte-carlo}
qqplot(Ms, M_stars)
abline(0,1)  
```

We see it is not perfect but it provides a decent approximation:

```{r}
quantile(Ms, c(0.05, 0.95))
quantile(M_stars, c(0.05, 0.95))
```

This is much better than what we get if we mindlessly use the CLT:
```{r}
median(X) + 1.96 * sd(X)/sqrt(N) * c(-1,1)
```


If we know the distribution is normal, we can use the bootstrap to estimate the mean:
```{r}
mean(Ms) + 1.96*sd(Ms)*c(-1,1)
mean(M_stars) + 1.96*sd(M_stars)*c(-1,1)
```

**Note that we can use the Bootstrap ideas in cross validaion: instead of dividing the data into equal paritions, we simply Boostrap many times.**