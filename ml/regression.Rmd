# Linear regression for prediction

Linear regression can be considered a machine learning algorithm. As we will see, it is too rigid to be useful in general, but for some challenges it works rather well. It also serves as a baseline approach: if you can't beat it with a more complex approach, you probably want to stick to linear regression. To quickly make the connection between regression and machine learning, we will reformulate Galton's study with heights: a continuous outcome.

```{r}
library(HistData)

galton_heights <- GaltonFamilies %>%
  filter(childNum == 1 & gender == "male") %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
```

Suppose you are tasked with building a machine learning algorithm that predicts the son's height $Y$ using the father's height $X$. Let's generate testing and training sets:

```{r}
library(caret)
y <- galton_heights$son
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

train_set <- galton_heights %>% slice(-test_index)
test_set <- galton_heights %>% slice(test_index)
```

In this case, if we were just ignoring the father's height and guessing the son's height we would guess the average height of sons.

```{r}
avg <- mean(train_set$son)
avg
```

Our squared loss is: 

```{r}
mean((avg - test_set$son)^2)
```

Can we do better? In the regression chapter we learned that if the pair $(X,Y)$ follow a bivariate normal distribution, the conditional expectation (what we want to estimate) is equivalent to the regression line:

$$
f(x) = \mbox{E}( Y  \mid  X= x ) = \beta_0 + \beta_1 x
$$

We also introduced least squares as a method for estimating the slope $\beta_0$ and intercept $\beta_1$: 

```{r}
fit <- lm(son ~ father, data = train_set)
fit$coef
```

This gives us an estimate of the conditional expectation:

$$ \hat{f}(x) = 38 + 0.47 x $$


We can see that this does indeed provide an improvement over our guessing approach. 

```{r}
y_hat <- fit$coef[1] + fit$coef[2]*test_set$father
mean((y_hat - test_set$son)^2)
```


## The `predict` function

The `predict` function is very useful for machine learning applications. This function takes a fitted object from function such as `lm` or `glm` (we learn about `glm` soon) and a data frame with the new predictors for which to predict. So in our current example we would use predict like this:

```{r}
y_hat <- predict(fit, test_set)
```

Using `predict` we can get the same results as we did previously:

```{r}
y_hat <- predict(fit, test_set)
mean((y_hat - test_set$son)^2)
```

Predict does not always return objects of the same types; it depends on what type of object is sent to it. To learn about the specifics, you need to look at the help file specific for the type of fit object that is being used. The `predict` is a actually a special type of function in R (called a _generic function_) that calls other functions depending on what kind of object it receives. So if `predict` receives an object coming out of the `lm` function, it will call `predict.lm`. If it receives an object coming out of `glm`, it calls `predict.glm`. These two functions are similar but different. You can learn more about the differences by reading the help files:

```{r, eval=FALSE}
?predict.lm
```

and

```{r, eval=FALSE}
?predict.glm
```

There are many other versions of `predict` with many machine learning algorithms having one.

## Regression for a categorical outcome

The regression approach can also be applied to categorical data. To illustrate this, we will apply it to our previous predicting sex example:

```{r, echo=FALSE}
library(dslabs)
data("heights")

y <- heights$height
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
train_set <- heights %>% slice(-test_index)
test_set <- heights %>% slice(test_index)
```

If we define the outcome $Y$ as 1 for females and 0 for males, and $X$ as the height, in this case we are interested in the conditional probability:

$$
\mbox{Pr}( Y = 1 \mid X = x)
$$

As an example, let's provide a prediction for a student that is 66 inches tall.  What is the conditional probability of being female if you are 66 inches tall? In our dataset we can estimate this by rounding to the nearest inch and computing:

```{r}
train_set %>% 
  filter(round(height)==66) %>%
  summarize(mean(sex=="Female"))
```

We will define $Y=1$ for females and $Y=0$ for males. To construct a prediction algorithm, we want to estimate the proportion of the population that is female for any given height $X=x$, which we write as the conditional probability described above: $\mbox{Pr}( Y = 1 | X=x)$. Let's see what this looks like for several values of $x$ (we will remove values of $x$ with few data points):

```{r height-and-sex-conditional-probabilities}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point()
```

Since the results from the plot above look close to linear, and it is the only approach we currently know, we will try regression. We assume that:

$$p(x) = \mbox{Pr}( Y = 1 | X=x)  = \beta_0 + \beta_1 x$$

Note: because $p_0(x) = 1 - p_1(x)$, we will only estimate $p_1(x)$ and drop the index.

If we convert the factors to 0s and 1s, we can we can estimate $\beta_0$ and $\beta_1$ with least squares. 

```{r}
lm_fit <- mutate(train_set, y = as.numeric(sex == "Female")) %>%
                lm(y ~ height, data = .)
```


Once we have estimates $\hat{\beta}_0$ and $\hat{\beta}_1$, we can obtain an actual prediction. Our estimate of the conditional probability $p(x)$ is:

$$
\hat{p}(x) = \hat{\beta}_0+ \hat{\beta}_1 x
$$

To form a prediction we define a _decision rule_:  predict female if $\hat{p}(x) > 0.5$. We can compare our predictions to the outcomes using:

```{r}
p_hat <- predict(lm_fit, test_set)
y_hat <- ifelse(p_hat > 0.5, "Female", "Male") %>% factor()
confusionMatrix(y_hat, test_set$sex)
```

We see this method does substantially better than guessing.

# Logistic Regression

The function $\beta_0 + \beta_1 x$ can take any value including negatives and values larger than 1. In fact, the estimate $\hat{p}(x)$ computed in the linear regression section does indeed become negative at around 76 inches.

```{r regression-prediction}
heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) %>%
  ggplot(aes(x, prop)) +
  geom_point() + 
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
```

The range is:

```{r}
range(p_hat)
```

But we are estimating a probability: $\mbox{Pr}( Y = 1 \mid X = x)$ which is constrained between 0 and 1. 

Logistic regression is an extension of linear regression that assures that the estimate of  $\mbox{Pr}( Y = 1 \mid X = x)$ is between 0 and 1. This approach makes use of the  _logistics_ transformation introduced in the data visualization chapter.  

$$ g(p) = \log \frac{p}{1-p}$$

This logistic transformation converts probability to log odds. As discussed in the data visualization lecture, the odds tell us how much more likely something will happen compared to not happening. So $p=0.5$ means the odds are 1 to 1, thus the odds are 1. If $p=0.75$, the odds are 3 to 1. A nice characteristic of this transformation is that it transforms probabilities to be symmetric around 0. Here is a plot of $g(p)$ versus $p$:

```{r p-versus-logistic-of-p, echo=FALSE}
p <- seq(0.01,.99,len=100)
qplot(p, log( p/(1-p) ), geom="line")
```

With _logistic regression_ we model the conditional probability directly with:

$$ 
g\left\{ \mbox{Pr}(Y = 1 \mid X=x) \right\} = \beta_0 + \beta_1 x
$$


With this model, we can no longer use least squares. Instead we compute the _maximum likelihood estimate_ (MLE). You can learn more about this concept in a [statistical theory text](http://www.amazon.com/Mathematical-Statistics-Analysis-Available-Enhanced/dp/0534399428). 

In R we can fit the logistic regression model with the function `glm`: generalized linear models. This function is more general than logistic regression so we need to specify the model we want through the `family` parameter:

```{r}
glm_fit <- train_set %>% 
  mutate(y = as.numeric(sex == "Female")) %>%
  glm(y ~ height, data=., family = "binomial")
```

We can obtain prediction using the predict function:

```{r}
p_hat_logit <- predict(glm_fit, newdata = test_set, type = "response")
```

When using `predict` with a `glm` object, we have to specify that we want `type="response"` if we want the conditional probabilities since the default is to return the logistic transformed values.

This model fits the data slightly better than the line:

```{r conditional-prob-glm-fit, echo=FALSE }
tmp <- heights %>% 
  mutate(x = round(height)) %>%
  group_by(x) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female")) 
logistic_curve <- data.frame(x = seq(min(tmp$x), max(tmp$x))) %>%
  mutate(p_hat = plogis(glm_fit$coef[1] + glm_fit$coef[2]*x))
tmp %>% 
  ggplot(aes(x, prop)) +
  geom_point() +
  geom_line(data = logistic_curve,
             mapping = aes(x, p_hat), lty = 2)
```

Because we have an estimate $\hat{p}(x)$, we can obtain predictions:

```{r}
y_hat_logit <- ifelse(p_hat_logit > 0.5, "Female", "Male") %>% factor
confusionMatrix(y_hat_logit, test_set$sex)
```

The resulting predictions are similar. This is because the two estimates of $p(x)$ are larger than 1/2 in about the same region of x:

```{r glm-prediction}
data.frame(x = seq(min(tmp$x), max(tmp$x))) %>%
  mutate(logistic = plogis(glm_fit$coef[1] + glm_fit$coef[2]*x),
         regression = lm_fit$coef[1] + lm_fit$coef[2]*x) %>%
  gather(method, p_x, -x) %>%
  ggplot(aes(x, p_x, color = method)) + 
  geom_line() +
  geom_hline(yintercept = 0.5, lty = 5)
```

Both linear and logistic regressions provide an estimate for the conditional expectation:

$$
\mbox{E}(Y \mid X=x)
$$
which in the case of binary data is equivalent to the conditional probability:

$$
\mbox{Pr}(Y = 1 \mid X = x)
$$

Once we move on to more complex examples, we will see that linear regression and generalized linear regression are limited and not flexible enough to be useful. The techniques we learn are essentially approaches to estimating the conditional probability in a way that is more flexible.


